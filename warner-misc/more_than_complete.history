commit 0e2fb6db39c6d3d54d350572dc626450a80cc319
Author: Brian Warner <warner@lothar.com>
Date:   Mon Jun 28 09:25:29 2010 -0700

    fix Makefile for misc/coding_tools/*
---
 Makefile |    4 ++--
 1 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/Makefile b/Makefile
index 4b3e0e1..723d656 100644
--- a/Makefile
+++ b/Makefile
@@ -125,7 +125,7 @@ quicktest:
 # quicktest-coverage" to do a unit test run with coverage-gathering enabled,
 # then use "make coverate-output-text" for a brief report, or "make
 # coverage-output" for a pretty HTML report. Also see "make .coverage.el" and
-# misc/coding_helpers/coverage.el for emacs integration.
+# misc/coding_tools/coverage.el for emacs integration.
 
 quicktest-coverage:
 	rm -f .coverage
@@ -154,7 +154,7 @@ coverage-output:
 .PHONY: repl test-darcs-boringfile test-clean clean find-trailing-spaces
 
 .coverage.el: .coverage
-	$(PYTHON) misc/coding_helpers/coverage2el.py
+	$(PYTHON) misc/coding_tools/coverage2el.py
 
 # 'upload-coverage' is meant to be run with an UPLOAD_TARGET=host:/dir setting
 ifdef UPLOAD_TARGET

commit a798a588d2444e946adb6b9cc2b544e4f6fd5f32
Author: Brian Warner <warner@lothar.com>
Date:   Mon Jun 28 09:25:11 2010 -0700

    move check-umids.py into new location
---
 Makefile                         |    2 +-
 misc/check-umids.py              |   30 ------------------------------
 misc/coding_tools/check-umids.py |   30 ++++++++++++++++++++++++++++++
 3 files changed, 31 insertions(+), 31 deletions(-)

diff --git a/Makefile b/Makefile
index efb91df..4b3e0e1 100644
--- a/Makefile
+++ b/Makefile
@@ -179,7 +179,7 @@ endif
 pyflakes:
 	$(PYTHON) -OOu `which pyflakes` src/allmydata |sort |uniq
 check-umids:
-	$(PYTHON) misc/check-umids.py `find src/allmydata -name '*.py'`
+	$(PYTHON) misc/coding_tools/check-umids.py `find src/allmydata -name '*.py'`
 
 count-lines:
 	@echo -n "files: "
diff --git a/misc/check-umids.py b/misc/check-umids.py
deleted file mode 100755
index 05e8825..0000000
--- a/misc/check-umids.py
+++ /dev/null
@@ -1,30 +0,0 @@
-#! /usr/bin/python
-
-# ./rumid.py foo.py
-
-import sys, re, os
-
-ok = True
-umids = {}
-
-for fn in sys.argv[1:]:
-    fn = os.path.abspath(fn)
-    for lineno,line in enumerate(open(fn, "r").readlines()):
-        lineno = lineno+1
-        if "umid" not in line:
-            continue
-        mo = re.search("umid=[\"\']([^\"\']+)[\"\']", line)
-        if mo:
-            umid = mo.group(1)
-            if umid in umids:
-                oldfn, oldlineno = umids[umid]
-                print "%s:%d: duplicate umid '%s'" % (fn, lineno, umid)
-                print "%s:%d: first used here" % (oldfn, oldlineno)
-                ok = False
-            umids[umid] = (fn,lineno)
-
-if ok:
-    print "all umids are unique"
-else:
-    print "some umids were duplicates"
-    sys.exit(1)
diff --git a/misc/coding_tools/check-umids.py b/misc/coding_tools/check-umids.py
new file mode 100755
index 0000000..05e8825
--- /dev/null
+++ b/misc/coding_tools/check-umids.py
@@ -0,0 +1,30 @@
+#! /usr/bin/python
+
+# ./rumid.py foo.py
+
+import sys, re, os
+
+ok = True
+umids = {}
+
+for fn in sys.argv[1:]:
+    fn = os.path.abspath(fn)
+    for lineno,line in enumerate(open(fn, "r").readlines()):
+        lineno = lineno+1
+        if "umid" not in line:
+            continue
+        mo = re.search("umid=[\"\']([^\"\']+)[\"\']", line)
+        if mo:
+            umid = mo.group(1)
+            if umid in umids:
+                oldfn, oldlineno = umids[umid]
+                print "%s:%d: duplicate umid '%s'" % (fn, lineno, umid)
+                print "%s:%d: first used here" % (oldfn, oldlineno)
+                ok = False
+            umids[umid] = (fn,lineno)
+
+if ok:
+    print "all umids are unique"
+else:
+    print "some umids were duplicates"
+    sys.exit(1)

commit 0c104f2bbb199da54663bb0ae7208e7cf53ce1c5
Author: Brian Warner <warner@lothar.com>
Date:   Mon Jun 28 08:25:25 2010 -0700

    test_upload: add some TODO notes
---
 src/allmydata/test/test_upload.py |    8 ++++++++
 1 files changed, 8 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/test/test_upload.py b/src/allmydata/test/test_upload.py
index 52e5aed..df88f28 100644
--- a/src/allmydata/test/test_upload.py
+++ b/src/allmydata/test/test_upload.py
@@ -1816,3 +1816,11 @@ class EncodingParameters(GridTestMixin, unittest.TestCase, SetDEPMixin,
 #  upload with exactly 75 peers (shares_of_happiness)
 #  have a download fail
 #  cancel a download (need to implement more cancel stuff)
+
+# from test_encode:
+# NoNetworkGrid, upload part of ciphertext, kill server, continue upload
+# check with Kevan, they want to live in test_upload, existing tests might cover
+#     def test_lost_one_shareholder(self): # these are upload-side tests
+#     def test_lost_one_shareholder_early(self):
+#     def test_lost_many_shareholders(self):
+#     def test_lost_all_shareholders(self):

commit 24a57ae55aebd59b0336bc46280654811bf6eb28
Author: Brian Warner <warner@lothar.com>
Date:   Wed Jun 23 17:55:20 2010 -0700

    Share: add note about defending against empty sharehash response
---
 src/allmydata/immutable/downloader/share.py |    8 ++++++++
 1 files changed, 8 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/downloader/share.py b/src/allmydata/immutable/downloader/share.py
index c4dbd73..50caa60 100644
--- a/src/allmydata/immutable/downloader/share.py
+++ b/src/allmydata/immutable/downloader/share.py
@@ -423,6 +423,14 @@ class Share:
             (hashnum,) = struct.unpack(">H", hashdata[i:i+2])
             hashvalue = hashdata[i+2:i+2+HASH_SIZE]
             share_hashes[hashnum] = hashvalue
+        # TODO: if they give us an empty set of hashes,
+        # process_share_hashes() won't fail. We must ensure that this
+        # situation doesn't allow unverified shares through. Manual testing
+        # shows that set_block_hash_root() throws an assert because an
+        # internal node is None instead of an actual hash, but we want
+        # something better. It's probably best to add a method to
+        # IncompleteHashTree which takes a leaf number and raises an
+        # exception unless that leaf is present and fully validated.
         try:
             self._node.process_share_hashes(share_hashes)
             # adds to self._node.share_hash_tree

commit 73a051ffc84d676a7f7db937d4840ec0d261c9b9
Author: Brian Warner <warner@lothar.com>
Date:   Wed Jun 23 17:54:03 2010 -0700

    test_download: exercise failover from server disconnect during a download
---
 src/allmydata/test/test_download.py |   65 +++++++++++++++++++++++++++++++++++
 1 files changed, 65 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index 234665d..520eaf2 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -267,6 +267,60 @@ class DownloadTest(_Base, unittest.TestCase):
         d.addCallback(_clobber_all_shares)
         return d
 
+    def test_lost_servers(self):
+        # while downloading a file (after seg[0], before seg[1]), lose the
+        # three servers that we were using. The download should switch over
+        # to other servers.
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # upload a file with multiple segments, so we can catch the download
+        # in the middle.
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 70 # 5 segs
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            self.uri = ur.uri
+            self.n = self.c0.create_node_from_uri(self.uri)
+            return download_to_data(self.n)
+        d.addCallback(_uploaded)
+        def _got_data(data):
+            self.failUnlessEqual(data, plaintext)
+        d.addCallback(_got_data)
+        def _kill_some_servers():
+            # find the three shares that were used, and delete them. Then
+            # download again, forcing the downloader to fail over to other
+            # shares
+            servers = []
+            shares = sorted([s._shnum for s in self.n._cnode._node._shares])
+            self.failUnlessEqual(shares, [0,1,2])
+            # break the RIBucketReader references
+            for s in self.n._cnode._node._shares:
+                s._rref.broken = True
+                for servernum in immutable_shares:
+                    for shnum in immutable_shares[servernum]:
+                        if s._shnum == shnum:
+                            ss = self.g.servers_by_number[servernum]
+                            servers.append(ss)
+            # and, for good measure, break the RIStorageServer references
+            # too, just in case the downloader gets more aggressive in the
+            # future and tries to re-fetch the same share.
+            for ss in servers:
+                wrapper = self.g.servers_by_id[ss.my_nodeid]
+                wrapper.broken = True
+        def _download_again(ign):
+            c = StallingConsumer(_kill_some_servers)
+            return self.n.read(c)
+        d.addCallback(_download_again)
+        def _check_failover(c):
+            self.failUnlessEqual("".join(c.chunks), plaintext)
+            shares = sorted([s._shnum for s in self.n._cnode._node._shares])
+            # we should now be using more shares than we were before
+            self.failIfEqual(shares, [0,1,2])
+        d.addCallback(_check_failover)
+        return d
+
     def test_badguess(self):
         self.basedir = self.mktemp()
         self.set_up_grid()
@@ -700,6 +754,17 @@ class StoppingConsumer(PausingConsumer):
     def write(self, data):
         self.producer.stopProducing()
 
+class StallingConsumer(MemoryConsumer):
+    def __init__(self, halfway_cb):
+        MemoryConsumer.__init__(self)
+        self.halfway_cb = halfway_cb
+        self.writes = 0
+    def write(self, data):
+        self.writes += 1
+        if self.writes == 1:
+            self.halfway_cb()
+        return MemoryConsumer.write(self, data)
+
 class Corruption(_Base, unittest.TestCase):
 
     def _corrupt_flip(self, ign, imm_uri, which):

commit b75c8f0b18f7d2278cc79aa9c1fe4fb4c2e4d917
Author: Brian Warner <warner@lothar.com>
Date:   Wed Jun 23 17:21:18 2010 -0700

    test_encode: remove obsolete tests
---
 src/allmydata/test/test_encode.py |  611 ++++---------------------------------
 1 files changed, 52 insertions(+), 559 deletions(-)

diff --git a/src/allmydata/test/test_encode.py b/src/allmydata/test/test_encode.py
index c6ece2a..c06fbbd 100644
--- a/src/allmydata/test/test_encode.py
+++ b/src/allmydata/test/test_encode.py
@@ -1,19 +1,15 @@
 from zope.interface import implements
 from twisted.trial import unittest
-from twisted.internet import defer, reactor
+from twisted.internet import defer
 from twisted.python.failure import Failure
-from foolscap.api import eventually, fireEventually
-from allmydata import hashtree, uri
-from allmydata.immutable import encode, upload, checker, filenode
-from allmydata.immutable.downloader.share import Share
-#from allmydata.immutable.downloader.util import Observer2
+from foolscap.api import fireEventually
+from allmydata import uri
+from allmydata.immutable import encode, upload, checker
 from allmydata.util import hashutil
 from allmydata.util.assertutil import _assert
-from allmydata.util.consumer import MemoryConsumer
-from allmydata.interfaces import IStorageBucketWriter, IStorageBucketReader, \
-     NotEnoughSharesError, IStorageBroker, UploadUnhappinessError
-from allmydata.monitor import Monitor
-import allmydata.test.common_util as testutil
+from allmydata.util.consumer import download_to_data
+from allmydata.interfaces import IStorageBucketWriter, IStorageBucketReader
+from allmydata.test.no_network import GridTestMixin
 
 class LostPeerError(Exception):
     pass
@@ -21,38 +17,6 @@ class LostPeerError(Exception):
 def flip_bit(good): # flips the last bit
     return good[:-1] + chr(ord(good[-1]) ^ 0x01)
 
-class FakeStorageBroker:
-    implements(IStorageBroker)
-    def get_servers_for_index(self, si):
-        return []
-
-class FakeShareFinder:
-    def __init__(self, node, shares):
-        self.node = node
-        self.shares = shares
-    def stop(self):
-        pass
-    def hungry(self):
-        def _deliver():
-            self.node.got_shares(set([self.shares.pop(0)]))
-        eventually(_deliver)
-class FakeShare(Share):
-    def __init__(self, data, mode, rref, server_version, verifycap,
-                 commonshare, node, download_status, peerid, shnum, logparent):
-        self._shnum = shnum
-        self._peerid = "peerid"
-        self._peerid_s = "peerid"
-        self.mode = mode
-        Share.__init__(self, stuff..)
-    def _send_request(self, start, length):
-        return fireEventually(self.data[start:start+length])
-
-    def OFFget_block(self, segnum):
-        NotImplementedError
-        o = Observer2()
-        o.notify(state=COMPLETE, block=block)
-        return o
-
 class FakeBucketReaderWriterProxy:
     implements(IStorageBucketWriter, IStorageBucketReader)
     # these are used for both reading and writing
@@ -90,13 +54,6 @@ class FakeBucketReaderWriterProxy:
             self.blocks[segmentnum] = data
         return defer.maybeDeferred(_try)
 
-    def put_plaintext_hashes(self, hashes):
-        def _try():
-            assert not self.closed
-            assert not self.plaintext_hashes
-            self.plaintext_hashes = hashes
-        return defer.maybeDeferred(_try)
-
     def put_crypttext_hashes(self, hashes):
         def _try():
             assert not self.closed
@@ -364,30 +321,6 @@ class Encode(unittest.TestCase):
 
         return d
 
-    # a series of 3*3 tests to check out edge conditions. One axis is how the
-    # plaintext is divided into segments: kn+(-1,0,1). Another way to express
-    # that is that n%k == -1 or 0 or 1. For example, for 25-byte segments, we
-    # might test 74 bytes, 75 bytes, and 76 bytes.
-
-    # on the other axis is how many leaves in the block hash tree we wind up
-    # with, relative to a power of 2, so 2^a+(-1,0,1). Each segment turns
-    # into a single leaf. So we'd like to check out, e.g., 3 segments, 4
-    # segments, and 5 segments.
-
-    # that results in the following series of data lengths:
-    #  3 segs: 74, 75, 51
-    #  4 segs: 99, 100, 76
-    #  5 segs: 124, 125, 101
-
-    # all tests encode to 100 shares, which means the share hash tree will
-    # have 128 leaves, which means that buckets will be given an 8-long share
-    # hash chain
-
-    # all 3-segment files will have a 4-leaf blockhashtree, and thus expect
-    # to get 7 blockhashes. 4-segment files will also get 4-leaf block hash
-    # trees and 7 blockhashes. 5-segment files will get 8-leaf block hash
-    # trees, which get 15 blockhashes.
-
     def test_send_74(self):
         # 3 segments (25, 25, 24)
         return self.do_encode(25, 74, 100, 3, 7, 8)
@@ -418,502 +351,62 @@ class Encode(unittest.TestCase):
         # 5 segments: 25, 25, 25, 25, 1
         return self.do_encode(25, 101, 100, 5, 15, 8)
 
-class PausingConsumer(MemoryConsumer):
-    def __init__(self):
-        MemoryConsumer.__init__(self)
-        self.size = 0
-        self.writes = 0
-    def write(self, data):
-        self.size += len(data)
-        self.writes += 1
-        if self.writes <= 2:
-            # we happen to use 4 segments, and want to avoid pausing on the
-            # last one (since then the _unpause timer will still be running)
-            self.producer.pauseProducing()
-            reactor.callLater(0.1, self._unpause)
-        return MemoryConsumer.write(self, data)
-    def _unpause(self):
-        self.producer.resumeProducing()
-
-class PausingAndStoppingConsumer(PausingConsumer):
-    def write(self, data):
-        self.producer.pauseProducing()
-        reactor.callLater(0.5, self._stop)
-    def _stop(self):
-        self.producer.stopProducing()
-
-class StoppingConsumer(PausingConsumer):
-    def write(self, data):
-        self.producer.stopProducing()
-
-class Roundtrip_OFF(unittest.TestCase, testutil.ShouldFailMixin):
-    timeout = 2400 # It takes longer than 240 seconds on Zandr's ARM box.
-    def send_and_recover(self, k_and_happy_and_n=(25,75,100),
-                         AVAILABLE_SHARES=None,
-                         datalen=76,
-                         max_segment_size=25,
-                         bucket_modes={},
-                         recover_mode="recover",
-                         consumer=None,
-                         ):
-        if AVAILABLE_SHARES is None:
-            AVAILABLE_SHARES = k_and_happy_and_n[2]
-        data = make_data(datalen)
-        d = self.send(k_and_happy_and_n, AVAILABLE_SHARES,
-                      max_segment_size, bucket_modes, data)
-        # that fires with (uri_extension_hash, e, shareholders)
-        d.addCallback(self.recover, AVAILABLE_SHARES,
-                      bucket_modes, recover_mode,
-                      consumer=consumer)
-        # that fires with newdata
-        def _downloaded(newdata):
-            self.failUnless(newdata == data, str((len(newdata), len(data))))
-            return None
-        d.addCallback(_downloaded)
-        return d
-
-    def send(self, k_and_happy_and_n, AVAILABLE_SHARES, max_segment_size,
-             bucket_modes, data):
-        k, happy, n = k_and_happy_and_n
-        NUM_SHARES = k_and_happy_and_n[2]
-        if AVAILABLE_SHARES is None:
-            AVAILABLE_SHARES = NUM_SHARES
-        e = encode.Encoder()
-        u = upload.Data(data, convergence="some convergence string")
-        # force use of multiple segments by using a low max_segment_size
-        u.max_segment_size = max_segment_size
-        u.encoding_param_k = k
-        u.encoding_param_happy = happy
-        u.encoding_param_n = n
-        eu = upload.EncryptAnUploadable(u)
-        d = e.set_encrypted_uploadable(eu)
-
-        shareholders = {}
-        def _ready(res):
-            k,happy,n = e.get_param("share_counts")
-            assert n == NUM_SHARES # else we'll be completely confused
-            servermap = {}
-            for shnum in range(NUM_SHARES):
-                mode = bucket_modes.get(shnum, "good")
-                peer = FakeBucketReaderWriterProxy(mode, "peer%d" % shnum)
-                shareholders[shnum] = peer
-                servermap.setdefault(shnum, set()).add(peer.get_peerid())
-            e.set_shareholders(shareholders, servermap)
-            return e.start()
-        d.addCallback(_ready)
-        def _sent(res):
-            d1 = u.get_encryption_key()
-            d1.addCallback(lambda key: (res, key, shareholders))
-            return d1
-        d.addCallback(_sent)
-        return d
-
-    def recover(self, (res, key, shareholders), AVAILABLE_SHARES,
-                bucket_modes, recover_mode, consumer=None):
-        verifycap = res
-
-        if "corrupt_key" in recover_mode:
-            # we corrupt the key, so that the decrypted data is corrupted and
-            # will fail the plaintext hash check. Since we're manually
-            # attaching shareholders, the fact that the storage index is also
-            # corrupted doesn't matter.
-            key = flip_bit(key)
-
-        u = uri.CHKFileURI(key=key,
-                           uri_extension_hash=verifycap.uri_extension_hash,
-                           needed_shares=verifycap.needed_shares,
-                           total_shares=verifycap.total_shares,
-                           size=verifycap.size)
-
-        # we use a special StorageBroker to get more control over the
-        # download process: use a subset of the real servers, skip
-        # permuted-peerlist (to control which server is used first?), ..
-        sb = FakeStorageBroker()
-        if not consumer:
-            consumer = MemoryConsumer()
-        secret_holder = None; terminator = None; history = None
-        fn = filenode.ImmutableFileNode(u, sb, secret_holder,
-                                        terminator, history)
-
-        # populate our storagebroker with .. something?
-        #for shnum, bucket in shareholders.items():
-        #    if shnum < AVAILABLE_SHARES and bucket.closed:
-        #        fd.add_share_bucket(shnum, bucket)
-        #fd._got_all_shareholders(None)
-
-        # need to create replacement for FakeBucketReaderWriterProxy, since
-        # new-downloader doesn't use readerproxies
-        f = fn._cnode._node._sharefinder
-        # fille the ShareFinder with our doctored shares, so it will never
-        # ask the StorageBroker for servers
-        for shnum in range(AVAILABLE_SHARES):
-            bucket = 234
-            best_numsegs = 1 # good enough
-            server_version = NativeStorageClientDescriptor.VERSION_DEFAULTS
-            ...
-            f._create_share(best_numsegs, shnum, bucket, server_version,
-                            "peerid")
-            s = FakeShare(shnum, bucket_modes.get(shnum, "good"))
-            f.undelivered_shares.append(s)
-
-        # Make it possible to obtain uri_extension from the shareholders.
-        # Arrange for shareholders[0] to be the first, so we can selectively
-        # corrupt the data it returns.
-        #uri_extension_sources = shareholders.values()
-        #uri_extension_sources.remove(shareholders[0])
-        #uri_extension_sources.insert(0, shareholders[0])
-
-        d = fn.read(consumer)
-
-        if 0:
-            # have the CiphertextDownloader retrieve a copy of uri_extension itself
-            d.addCallback(fd._obtain_uri_extension)
-
-            if "corrupt_crypttext_hashes" in recover_mode:
-                # replace everybody's crypttext hash trees with a different one
-                # (computed over a different file), then modify our uri_extension
-                # to reflect the new crypttext hash tree root
-                def _corrupt_crypttext_hashes(unused):
-                    assert isinstance(fd._vup, checker.ValidatedExtendedURIProxy), fd._vup
-                    assert fd._vup.crypttext_root_hash, fd._vup
-                    badhash = hashutil.tagged_hash("bogus", "data")
-                    bad_crypttext_hashes = [badhash] * fd._vup.num_segments
-                    badtree = hashtree.HashTree(bad_crypttext_hashes)
-                    for bucket in shareholders.values():
-                        bucket.crypttext_hashes = list(badtree)
-                    fd._crypttext_hash_tree = hashtree.IncompleteHashTree(fd._vup.num_segments)
-                    fd._crypttext_hash_tree.set_hashes({0: badtree[0]})
-                    return fd._vup
-                d.addCallback(_corrupt_crypttext_hashes)
-
-            # also have the CiphertextDownloader ask for hash trees
-            d.addCallback(fd._get_crypttext_hash_tree)
-
-            d.addCallback(fd._download_all_segments)
-            d.addCallback(fd._done)
-        def _done(t):
-            newdata = "".join(consumer.chunks)
-            return newdata
-        d.addCallback(_done)
-        return d
 
-    def test_not_enough_shares(self):
-        d = self.send_and_recover((4,8,10), AVAILABLE_SHARES=2)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(NotEnoughSharesError))
-        d.addBoth(_done)
-        return d
-
-    def test_one_share_per_peer(self):
-        return self.send_and_recover()
-
-    def test_74(self):
-        return self.send_and_recover(datalen=74)
-    def test_75(self):
-        return self.send_and_recover(datalen=75)
-    def test_51(self):
-        return self.send_and_recover(datalen=51)
-
-    def test_99(self):
-        return self.send_and_recover(datalen=99)
-    def test_100(self):
-        return self.send_and_recover(datalen=100)
-    def test_76(self):
-        return self.send_and_recover(datalen=76)
-
-    def test_124(self):
-        return self.send_and_recover(datalen=124)
-    def test_125(self):
-        return self.send_and_recover(datalen=125)
-    def test_101(self):
-        return self.send_and_recover(datalen=101)
-
-    def test_pause(self):
-        # use a download target that does pauseProducing/resumeProducing a
-        # few times, then finishes
-        c = PausingConsumer()
-        d = self.send_and_recover(consumer=c)
-        return d
-
-    def test_pause_then_stop(self):
-        # use a download target that pauses, then stops.
-        c = PausingAndStoppingConsumer()
-        d = self.shouldFail(DownloadStopped, "test_pause_then_stop",
-                            "our Consumer called stopProducing()",
-                            self.send_and_recover, consumer=c)
-        return d
-
-    def test_stop(self):
-        # use a download targetthat does an immediate stop (ticket #473)
-        c = StoppingConsumer()
-        d = self.shouldFail(DownloadStopped, "test_stop",
-                            "our Consumer called stopProducing()",
-                            self.send_and_recover, consumer=c)
-        return d
-
-    # the following tests all use 4-out-of-10 encoding
-
-    def test_bad_blocks(self):
-        # the first 6 servers have bad blocks, which will be caught by the
-        # blockhashes
-        modemap = dict([(i, "bad block")
-                        for i in range(6)]
-                       + [(i, "good")
-                          for i in range(6, 10)])
-        return self.send_and_recover((4,8,10), bucket_modes=modemap)
-
-    def test_bad_blocks_failure(self):
-        # the first 7 servers have bad blocks, which will be caught by the
-        # blockhashes, and the download will fail
-        modemap = dict([(i, "bad block")
-                        for i in range(7)]
-                       + [(i, "good")
-                          for i in range(7, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure), res)
-            self.failUnless(res.check(NotEnoughSharesError), res)
-        d.addBoth(_done)
-        return d
-
-    def test_bad_blockhashes(self):
-        # the first 6 servers have bad block hashes, so the blockhash tree
-        # will not validate
-        modemap = dict([(i, "bad blockhash")
-                        for i in range(6)]
-                       + [(i, "good")
-                          for i in range(6, 10)])
-        return self.send_and_recover((4,8,10), bucket_modes=modemap)
-
-    def test_bad_blockhashes_failure(self):
-        # the first 7 servers have bad block hashes, so the blockhash tree
-        # will not validate, and the download will fail
-        modemap = dict([(i, "bad blockhash")
-                        for i in range(7)]
-                       + [(i, "good")
-                          for i in range(7, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(NotEnoughSharesError), res)
-        d.addBoth(_done)
-        return d
-
-    def test_bad_sharehashes(self):
-        # the first 6 servers have bad block hashes, so the sharehash tree
-        # will not validate
-        modemap = dict([(i, "bad sharehash")
-                        for i in range(6)]
-                       + [(i, "good")
-                          for i in range(6, 10)])
-        return self.send_and_recover((4,8,10), bucket_modes=modemap)
-
-    def assertFetchFailureIn(self, fd, where):
-        expected = {"uri_extension": 0,
-                    "crypttext_hash_tree": 0,
-                    }
-        if where is not None:
-            expected[where] += 1
-        self.failUnlessEqual(fd._fetch_failures, expected)
-
-    def test_good(self):
-        # just to make sure the test harness works when we aren't
-        # intentionally causing failures
-        modemap = dict([(i, "good") for i in range(0, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        d.addCallback(self.assertFetchFailureIn, None)
-        return d
-
-    def test_bad_uri_extension(self):
-        # the first server has a bad uri_extension block, so we will fail
-        # over to a different server.
-        modemap = dict([(i, "bad uri_extension") for i in range(1)] +
-                       [(i, "good") for i in range(1, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        d.addCallback(self.assertFetchFailureIn, "uri_extension")
-        return d
-
-    def test_bad_crypttext_hashroot(self):
-        # the first server has a bad crypttext hashroot, so we will fail
-        # over to a different server.
-        modemap = dict([(i, "bad crypttext hashroot") for i in range(1)] +
-                       [(i, "good") for i in range(1, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        d.addCallback(self.assertFetchFailureIn, "crypttext_hash_tree")
-        return d
-
-    def test_bad_crypttext_hashes(self):
-        # the first server has a bad crypttext hash block, so we will fail
-        # over to a different server.
-        modemap = dict([(i, "bad crypttext hash") for i in range(1)] +
-                       [(i, "good") for i in range(1, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        d.addCallback(self.assertFetchFailureIn, "crypttext_hash_tree")
-        return d
-
-    def test_bad_crypttext_hashes_failure(self):
-        # to test that the crypttext merkle tree is really being applied, we
-        # sneak into the download process and corrupt two things: we replace
-        # everybody's crypttext hashtree with a bad version (computed over
-        # bogus data), and we modify the supposedly-validated uri_extension
-        # block to match the new crypttext hashtree root. The download
-        # process should notice that the crypttext coming out of FEC doesn't
-        # match the tree, and fail.
-
-        modemap = dict([(i, "good") for i in range(0, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap,
-                                  recover_mode=("corrupt_crypttext_hashes"))
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(hashtree.BadHashError), res)
-        d.addBoth(_done)
-        return d
+class Roundtrip(GridTestMixin, unittest.TestCase):
 
-    def OFF_test_bad_plaintext(self):
-        # faking a decryption failure is easier: just corrupt the key
-        modemap = dict([(i, "good") for i in range(0, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap,
-                                  recover_mode=("corrupt_key"))
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(hashtree.BadHashError), res)
-        d.addBoth(_done)
-        return d
+    # a series of 3*3 tests to check out edge conditions. One axis is how the
+    # plaintext is divided into segments: kn+(-1,0,1). Another way to express
+    # this is n%k == -1 or 0 or 1. For example, for 25-byte segments, we
+    # might test 74 bytes, 75 bytes, and 76 bytes.
 
-    def test_bad_sharehashes_failure(self):
-        # all ten servers have bad share hashes, so the sharehash tree
-        # will not validate, and the download will fail
-        modemap = dict([(i, "bad sharehash")
-                        for i in range(10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(NotEnoughSharesError))
-        d.addBoth(_done)
-        return d
+    # on the other axis is how many leaves in the block hash tree we wind up
+    # with, relative to a power of 2, so 2^a+(-1,0,1). Each segment turns
+    # into a single leaf. So we'd like to check out, e.g., 3 segments, 4
+    # segments, and 5 segments.
 
-    def test_missing_sharehashes(self):
-        # the first 6 servers are missing their sharehashes, so the
-        # sharehash tree will not validate
-        modemap = dict([(i, "missing sharehash")
-                        for i in range(6)]
-                       + [(i, "good")
-                          for i in range(6, 10)])
-        return self.send_and_recover((4,8,10), bucket_modes=modemap)
-
-    def test_missing_sharehashes_failure(self):
-        # all servers are missing their sharehashes, so the sharehash tree will not validate,
-        # and the download will fail
-        modemap = dict([(i, "missing sharehash")
-                        for i in range(10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure), res)
-            self.failUnless(res.check(NotEnoughSharesError), res)
-        d.addBoth(_done)
-        return d
+    # that results in the following series of data lengths:
+    #  3 segs: 74, 75, 51
+    #  4 segs: 99, 100, 76
+    #  5 segs: 124, 125, 101
 
-    def test_lost_one_shareholder(self):
-        # we have enough shareholders when we start, but one segment in we
-        # lose one of them. The upload should still succeed, as long as we
-        # still have 'servers_of_happiness' peers left.
-        modemap = dict([(i, "good") for i in range(9)] +
-                       [(i, "lost") for i in range(9, 10)])
-        return self.send_and_recover((4,8,10), bucket_modes=modemap)
-
-    def test_lost_one_shareholder_early(self):
-        # we have enough shareholders when we choose peers, but just before
-        # we send the 'start' message, we lose one of them. The upload should
-        # still succeed, as long as we still have 'servers_of_happiness' peers
-        # left.
-        modemap = dict([(i, "good") for i in range(9)] +
-                       [(i, "lost-early") for i in range(9, 10)])
-        return self.send_and_recover((4,8,10), bucket_modes=modemap)
-
-    def test_lost_many_shareholders(self):
-        # we have enough shareholders when we start, but one segment in we
-        # lose all but one of them. The upload should fail.
-        modemap = dict([(i, "good") for i in range(1)] +
-                       [(i, "lost") for i in range(1, 10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(UploadUnhappinessError), res)
-        d.addBoth(_done)
-        return d
+    # all tests encode to 100 shares, which means the share hash tree will
+    # have 128 leaves, which means that buckets will be given an 8-long share
+    # hash chain
 
-    def test_lost_all_shareholders(self):
-        # we have enough shareholders when we start, but one segment in we
-        # lose all of them. The upload should fail.
-        modemap = dict([(i, "lost") for i in range(10)])
-        d = self.send_and_recover((4,8,10), bucket_modes=modemap)
-        def _done(res):
-            self.failUnless(isinstance(res, Failure))
-            self.failUnless(res.check(UploadUnhappinessError))
-        d.addBoth(_done)
+    # all 3-segment files will have a 4-leaf blockhashtree, and thus expect
+    # to get 7 blockhashes. 4-segment files will also get 4-leaf block hash
+    # trees and 7 blockhashes. 5-segment files will get 8-leaf block hash
+    # trees, which gets 15 blockhashes.
+
+    def test_74(self): return self.do_test_size(74)
+    def test_75(self): return self.do_test_size(75)
+    def test_51(self): return self.do_test_size(51)
+    def test_99(self): return self.do_test_size(99)
+    def test_100(self): return self.do_test_size(100)
+    def test_76(self): return self.do_test_size(76)
+    def test_124(self): return self.do_test_size(124)
+    def test_125(self): return self.do_test_size(125)
+    def test_101(self): return self.do_test_size(101)
+
+    def upload(self, data):
+        u = upload.Data(data, None)
+        u.max_segment_size = 25
+        u.encoding_param_k = 25
+        u.encoding_param_happy = 1
+        u.encoding_param_n = 100
+        d = self.c0.upload(u)
+        d.addCallback(lambda ur: self.c0.create_node_from_uri(ur.uri))
+        # returns a FileNode
         return d
 
-class Roundtrip(GridTestMixin, unittest.TestCase):
-    def test_74(self):
+    def do_test_size(self, size):
         self.basedir = self.mktemp()
         self.set_up_grid()
         self.c0 = self.g.clients[0]
-        u = upload.Data("p"*74, None))
-        u.max_segment_size = 25
-        u.???.encoding = (25,75,100)
-        d = self.c0.upload(u)
-        def _uploaded(ur):
-            n = self.c0.create_node_from_uri(ur.uri)
-            return download_to_data(n)
-        d.addCallback(_uploaded)
+        DATA = "p"*size
+        d = self.upload(DATA)
+        d.addCallback(lambda n: download_to_data(n))
         def _downloaded(newdata):
-            self.failUnlessEqual(newdata, "p"*74)
+            self.failUnlessEqual(newdata, DATA)
         d.addCallback(_downloaded)
         return d
-
-
-"""
-NoNetworkGrid:
-    def test_good(self):
-    def test_one_share_per_peer(self):
-    def test_74(self):
-    def test_75(self):
-    def test_51(self):
-    def test_99(self):
-    def test_100(self):
-    def test_76(self):
-    def test_124(self):
-    def test_125(self):
-    def test_101(self):
-    def test_pause(self):
-    def test_pause_then_stop(self):
-    def test_stop(self):
-
-NoNetworkGrid and delete shares
-    def test_not_enough_shares(self):
-NoNetworkGrid and corrupt shares
- (need to sequence retrievals?)
- covered by test_download.Corruption.test_each_byte
-    def test_bad_blocks(self):
-    def test_bad_blocks_failure(self):
-    def test_bad_blockhashes(self):
-    def test_bad_blockhashes_failure(self):
-    def test_bad_sharehashes(self):
-    def test_bad_uri_extension(self):
-    def test_bad_crypttext_hashroot(self):
-    def test_bad_crypttext_hashes(self):
-    def test_bad_crypttext_hashes_failure(self):
-    def test_bad_sharehashes_failure(self):
-    def test_missing_sharehashes(self):
-    def test_missing_sharehashes_failure(self):
-
-NoNetworkGrid, upload part of ciphertext, kill server, continue upload
-    def test_lost_one_shareholder(self): # these are upload-side tests
-    def test_lost_one_shareholder_early(self):
-    def test_lost_many_shareholders(self):
-    def test_lost_all_shareholders(self):
-
-need to add to test_downloader
- [read(seg0), kill server, read(seg1)]
-"""

commit c0a499aacccf59b0dee21f8c27dae94cef8e9737
Author: Brian Warner <warner@lothar.com>
Date:   Wed Jun 23 17:20:36 2010 -0700

    segmentation: errback with DownloadStopped if consumer calls stopProducing()
    test_download: add pause/stop tests, add corruption-causes-failure tests
---
 src/allmydata/immutable/downloader/common.py       |    2 +
 src/allmydata/immutable/downloader/segmentation.py |    5 +-
 src/allmydata/test/no_network.py                   |   16 ++
 src/allmydata/test/test_download.py                |  165 ++++++++++++++++----
 4 files changed, 154 insertions(+), 34 deletions(-)

diff --git a/src/allmydata/immutable/downloader/common.py b/src/allmydata/immutable/downloader/common.py
index 7364b8d..e9dd271 100644
--- a/src/allmydata/immutable/downloader/common.py
+++ b/src/allmydata/immutable/downloader/common.py
@@ -9,3 +9,5 @@ class WrongSegmentError(Exception):
 class BadCiphertextHashError(Exception):
     pass
 
+class DownloadStopped(Exception):
+    pass
diff --git a/src/allmydata/immutable/downloader/segmentation.py b/src/allmydata/immutable/downloader/segmentation.py
index adc138e..4890195 100644
--- a/src/allmydata/immutable/downloader/segmentation.py
+++ b/src/allmydata/immutable/downloader/segmentation.py
@@ -8,7 +8,7 @@ from foolscap.api import eventually
 from allmydata.util import log
 from allmydata.util.spans import overlap
 
-from common import BadSegmentNumberError, WrongSegmentError
+from common import BadSegmentNumberError, WrongSegmentError, DownloadStopped
 
 class Segmentation:
     """I am responsible for a single offset+size read of the file. I handle
@@ -145,6 +145,9 @@ class Segmentation:
         if self._cancel_segment_request:
             self._cancel_segment_request.cancel()
             self._cancel_segment_request = None
+        e = DownloadStopped("our Consumer called stopProducing()")
+        self._deferred.errback(e)
+
     def pauseProducing(self):
         self._hungry = False
         self._start_pause = now()
diff --git a/src/allmydata/test/no_network.py b/src/allmydata/test/no_network.py
index 801714e..4f58dea 100644
--- a/src/allmydata/test/no_network.py
+++ b/src/allmydata/test/no_network.py
@@ -319,6 +319,16 @@ class GridTestMixin:
                     pass
         return sorted(shares)
 
+    def copy_shares(self, uri):
+        shares = {}
+        for (shnum, serverid, sharefile) in self.find_shares(uri):
+            shares[sharefile] = open(sharefile, "rb").read()
+        return shares
+
+    def restore_all_shares(self, shares):
+        for sharefile, data in shares.items():
+            open(sharefile, "wb").write(data)
+
     def delete_share(self, (shnum, serverid, sharefile)):
         os.unlink(sharefile)
 
@@ -339,6 +349,12 @@ class GridTestMixin:
                 corruptdata = corruptor(sharedata, debug=debug)
                 open(i_sharefile, "wb").write(corruptdata)
 
+    def corrupt_all_shares(self, uri, corruptor, debug=False):
+        for (i_shnum, i_serverid, i_sharefile) in self.find_shares(uri):
+            sharedata = open(i_sharefile, "rb").read()
+            corruptdata = corruptor(sharedata, debug=debug)
+            open(i_sharefile, "wb").write(corruptdata)
+
     def GET(self, urlpath, followRedirect=False, return_response=False,
             method="GET", clientnum=0, **kwargs):
         # if return_response=True, this fires with (data, statuscode,
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index 95fae2f..234665d 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -5,7 +5,7 @@
 
 import os
 from twisted.trial import unittest
-from twisted.internet import defer
+from twisted.internet import defer, reactor
 from allmydata import uri
 from allmydata.storage.server import storage_index_to_dir
 from allmydata.util import base32, fileutil, spans, log
@@ -15,7 +15,7 @@ from allmydata.test.no_network import GridTestMixin
 from allmydata.test.common import ShouldFailMixin
 from allmydata.interfaces import NotEnoughSharesError, NoSharesError
 from allmydata.immutable.downloader.common import BadSegmentNumberError, \
-     BadCiphertextHashError
+     BadCiphertextHashError, DownloadStopped
 from allmydata.codec import CRSDecoder
 from foolscap.eventual import fireEventually, flushEventualQueue
 
@@ -498,18 +498,44 @@ class DownloadTest(_Base, unittest.TestCase):
         d.addCallback(_check)
         return d
 
-    def test_stop_producing(self):
+    def test_pause(self):
         self.basedir = self.mktemp()
         self.set_up_grid()
         self.c0 = self.g.clients[0]
         self.load_shares()
         n = self.c0.create_node_from_uri(immutable_uri)
+        c = PausingConsumer()
+        d = n.read(c)
+        def _downloaded(mc):
+            newdata = "".join(mc.chunks)
+            self.failUnlessEqual(newdata, plaintext)
+        d.addCallback(_downloaded)
+        return d
 
-        con = MemoryConsumer()
-        d = n.read(con)
-        con.producer.stopProducing()
-        # d should never fire
-        del d
+    def test_pause_then_stop(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+        c = PausingAndStoppingConsumer()
+        d = self.shouldFail(DownloadStopped, "test_pause_then_stop",
+                            "our Consumer called stopProducing()",
+                            n.read, c)
+        return d
+
+    def test_stop(self):
+        # use a download targetthat does an immediate stop (ticket #473)
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+        c = StoppingConsumer()
+        d = self.shouldFail(DownloadStopped, "test_stop",
+                            "our Consumer called stopProducing()",
+                            n.read, c)
+        return d
 
     def test_download_segment_bad_ciphertext_hash(self):
         # The crypttext_hash_tree asserts the integrity of the decoded
@@ -645,8 +671,49 @@ class BrokenDecoder(CRSDecoder):
         d.addCallback(_decoded)
         return d
 
+
+class PausingConsumer(MemoryConsumer):
+    def __init__(self):
+        MemoryConsumer.__init__(self)
+        self.size = 0
+        self.writes = 0
+    def write(self, data):
+        self.size += len(data)
+        self.writes += 1
+        if self.writes <= 2:
+            # we happen to use 4 segments, and want to avoid pausing on the
+            # last one (since then the _unpause timer will still be running)
+            self.producer.pauseProducing()
+            reactor.callLater(0.1, self._unpause)
+        return MemoryConsumer.write(self, data)
+    def _unpause(self):
+        self.producer.resumeProducing()
+
+class PausingAndStoppingConsumer(PausingConsumer):
+    def write(self, data):
+        self.producer.pauseProducing()
+        reactor.callLater(0.5, self._stop)
+    def _stop(self):
+        self.producer.stopProducing()
+
+class StoppingConsumer(PausingConsumer):
+    def write(self, data):
+        self.producer.stopProducing()
+
 class Corruption(_Base, unittest.TestCase):
 
+    def _corrupt_flip(self, ign, imm_uri, which):
+        log.msg("corrupt %d" % which)
+        def _corruptor(s, debug=False):
+            return s[:which] + chr(ord(s[which])^0x01) + s[which+1:]
+        self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
+
+    def _corrupt_set(self, ign, imm_uri, which, newvalue):
+        log.msg("corrupt %d" % which)
+        def _corruptor(s, debug=False):
+            return s[:which] + chr(newvalue) + s[which+1:]
+        self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
+
     def test_each_byte(self):
         # Setting catalog_detection=True performs an exhaustive test of the
         # Downloader's response to corruption in the lsb of each byte of the
@@ -668,22 +735,6 @@ class Corruption(_Base, unittest.TestCase):
         u = upload.Data(plaintext, None)
         u.max_segment_size = 120 # 3 segs, 4-wide hashtree
 
-        def _fix_sh0(res):
-            f = open(self.sh0_file, "wb")
-            f.write(self.sh0_orig)
-            f.close()
-        def _corrupt_flip(ign, imm_uri, which):
-            log.msg("corrupt %d" % which)
-            def _corruptor(s, debug=False):
-                return s[:which] + chr(ord(s[which])^0x01) + s[which+1:]
-            self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
-
-        def _corrupt_set(ign, imm_uri, which, newvalue):
-            log.msg("corrupt %d" % which)
-            def _corruptor(s, debug=False):
-                return s[:which] + chr(newvalue) + s[which+1:]
-            self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
-
         if self.catalog_detection:
             undetected = spans.Spans()
 
@@ -723,11 +774,7 @@ class Corruption(_Base, unittest.TestCase):
         d = self.c0.upload(u)
         def _uploaded(ur):
             imm_uri = ur.uri
-            self.sh0_file = [sharefile
-                             for (shnum, serverid, sharefile)
-                             in self.find_shares(imm_uri)
-                             if shnum == 0][0]
-            self.sh0_orig = open(self.sh0_file, "rb").read()
+            self.shares = self.copy_shares(imm_uri)
             d = defer.succeed(None)
             # 'victims' is a list of corruption tests to run. Each one flips
             # the low-order bit of the specified offset in the share file (so
@@ -773,17 +820,19 @@ class Corruption(_Base, unittest.TestCase):
             if self.catalog_detection:
                 corrupt_me = [(i, "") for i in range(len(self.sh0_orig))]
             for i,expected in corrupt_me:
-                d.addCallback(_corrupt_flip, imm_uri, i)
+                # All these tests result in a successful download. What we're
+                # measuring is how many shares the downloader had to use.
+                d.addCallback(self._corrupt_flip, imm_uri, i)
                 d.addCallback(_download, imm_uri, i, expected)
-                d.addCallback(_fix_sh0)
+                d.addCallback(lambda ign: self.restore_all_shares(self.shares))
                 d.addCallback(fireEventually)
             corrupt_values = [(3, 2, "no-sh0"),
                               (15, 2, "need-4th"), # share looks v2
                               ]
             for i,newvalue,expected in corrupt_values:
-                d.addCallback(_corrupt_set, imm_uri, i, newvalue)
+                d.addCallback(self._corrupt_set, imm_uri, i, newvalue)
                 d.addCallback(_download, imm_uri, i, expected)
-                d.addCallback(_fix_sh0)
+                d.addCallback(lambda ign: self.restore_all_shares(self.shares))
                 d.addCallback(fireEventually)
             return d
         d.addCallback(_uploaded)
@@ -805,6 +854,56 @@ class Corruption(_Base, unittest.TestCase):
             #  [1309-]: reserved+unused UEB space
         return d
 
+    def test_failure(self):
+        # this test corrupts all shares in the same way, and asserts that the
+        # download fails.
+
+        self.basedir = "download/Corruption/failure"
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # to exercise the block-hash-tree code properly, we need to have
+        # multiple segments. We don't tell the downloader about the different
+        # segsize, so it guesses wrong and must do extra roundtrips.
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 120 # 3 segs, 4-wide hashtree
+
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            imm_uri = ur.uri
+            self.shares = self.copy_shares(imm_uri)
+
+            corrupt_me = [(48, "block data", "Last failure: None"),
+                          (600+2*32, "block_hashes[2]", "BadHashError"),
+                          (376+2*32, "crypttext_hash_tree[2]", "BadHashError"),
+                          (824, "share_hashes", "BadHashError"),
+                          ]
+            def _download(imm_uri):
+                n = self.c0.create_node_from_uri(imm_uri)
+                # for this test to work, we need to have a new Node each time.
+                # Make sure the NodeMaker's weakcache hasn't interfered.
+                assert not n._cnode._node._shares
+                return download_to_data(n)
+
+            d = defer.succeed(None)
+            for i,which,substring in corrupt_me:
+                # All these tests result in a failed download.
+                d.addCallback(self._corrupt_flip_all, imm_uri, i)
+                d.addCallback(lambda ign:
+                              self.shouldFail(NotEnoughSharesError, which,
+                                              substring,
+                                              _download, imm_uri))
+                d.addCallback(lambda ign: self.restore_all_shares(self.shares))
+                d.addCallback(fireEventually)
+            return d
+        d.addCallback(_uploaded)
+
+        return d
+
+    def _corrupt_flip_all(self, ign, imm_uri, which):
+        def _corruptor(s, debug=False):
+            return s[:which] + chr(ord(s[which])^0x01) + s[which+1:]
+        self.corrupt_all_shares(imm_uri, _corruptor)
 
 class DownloadV2(_Base, unittest.TestCase):
     # tests which exercise v2-share code. They first upload a file with

commit 4b449e209b6fb90448d89002f5c9683fbb6a639f
Author: Brian Warner <warner@lothar.com>
Date:   Wed Jun 16 09:25:15 2010 -0700

    WIP removing download.py (rewriting test_encode, refactoring ShareFinder),
    still horribly broken
---
 src/allmydata/immutable/checker.py           |  450 +++++++++-
 src/allmydata/immutable/download.py          | 1321 --------------------------
 src/allmydata/immutable/downloader/finder.py |   51 +-
 src/allmydata/test/test_encode.py            |  223 ++++--
 src/allmydata/test/test_web.py               |    5 +-
 5 files changed, 637 insertions(+), 1413 deletions(-)

diff --git a/src/allmydata/immutable/checker.py b/src/allmydata/immutable/checker.py
index 31c70e3..cd5c556 100644
--- a/src/allmydata/immutable/checker.py
+++ b/src/allmydata/immutable/checker.py
@@ -1,16 +1,444 @@
+from zope.interface import implements
+from twisted.internet import defer
 from foolscap.api import DeadReferenceError, RemoteException
+from allmydata import hashtree, codec, uri
+from allmydata.interfaces import IValidatedThingProxy, IVerifierURI
 from allmydata.hashtree import IncompleteHashTree
 from allmydata.check_results import CheckResults
-from allmydata.immutable import download
 from allmydata.uri import CHKFileVerifierURI
 from allmydata.util.assertutil import precondition
-from allmydata.util import base32, idlib, deferredutil, dictutil, log
+from allmydata.util import base32, idlib, deferredutil, dictutil, log, mathutil
 from allmydata.util.hashutil import file_renewal_secret_hash, \
      file_cancel_secret_hash, bucket_renewal_secret_hash, \
-     bucket_cancel_secret_hash
+     bucket_cancel_secret_hash, uri_extension_hash, CRYPTO_VAL_SIZE, \
+     block_hash
 
 from allmydata.immutable import layout
 
+class IntegrityCheckReject(Exception):
+    pass
+class BadURIExtension(IntegrityCheckReject):
+    pass
+class BadURIExtensionHashValue(IntegrityCheckReject):
+    pass
+class BadOrMissingHash(IntegrityCheckReject):
+    pass
+class UnsupportedErasureCodec(BadURIExtension):
+    pass
+
+class ValidatedExtendedURIProxy:
+    implements(IValidatedThingProxy)
+    """ I am a front-end for a remote UEB (using a local ReadBucketProxy),
+    responsible for retrieving and validating the elements from the UEB."""
+
+    def __init__(self, readbucketproxy, verifycap, fetch_failures=None):
+        # fetch_failures is for debugging -- see test_encode.py
+        self._fetch_failures = fetch_failures
+        self._readbucketproxy = readbucketproxy
+        precondition(IVerifierURI.providedBy(verifycap), verifycap)
+        self._verifycap = verifycap
+
+        # required
+        self.segment_size = None
+        self.crypttext_root_hash = None
+        self.share_root_hash = None
+
+        # computed
+        self.block_size = None
+        self.share_size = None
+        self.num_segments = None
+        self.tail_data_size = None
+        self.tail_segment_size = None
+
+        # optional
+        self.crypttext_hash = None
+
+    def __str__(self):
+        return "<%s %s>" % (self.__class__.__name__, self._verifycap.to_string())
+
+    def _check_integrity(self, data):
+        h = uri_extension_hash(data)
+        if h != self._verifycap.uri_extension_hash:
+            msg = ("The copy of uri_extension we received from %s was bad: wanted %s, got %s" %
+                   (self._readbucketproxy,
+                    base32.b2a(self._verifycap.uri_extension_hash),
+                    base32.b2a(h)))
+            if self._fetch_failures is not None:
+                self._fetch_failures["uri_extension"] += 1
+            raise BadURIExtensionHashValue(msg)
+        else:
+            return data
+
+    def _parse_and_validate(self, data):
+        self.share_size = mathutil.div_ceil(self._verifycap.size,
+                                            self._verifycap.needed_shares)
+
+        d = uri.unpack_extension(data)
+
+        # There are several kinds of things that can be found in a UEB.
+        # First, things that we really need to learn from the UEB in order to
+        # do this download. Next: things which are optional but not redundant
+        # -- if they are present in the UEB they will get used. Next, things
+        # that are optional and redundant. These things are required to be
+        # consistent: they don't have to be in the UEB, but if they are in
+        # the UEB then they will be checked for consistency with the
+        # already-known facts, and if they are inconsistent then an exception
+        # will be raised. These things aren't actually used -- they are just
+        # tested for consistency and ignored. Finally: things which are
+        # deprecated -- they ought not be in the UEB at all, and if they are
+        # present then a warning will be logged but they are otherwise
+        # ignored.
+
+        # First, things that we really need to learn from the UEB:
+        # segment_size, crypttext_root_hash, and share_root_hash.
+        self.segment_size = d['segment_size']
+
+        self.block_size = mathutil.div_ceil(self.segment_size,
+                                            self._verifycap.needed_shares)
+        self.num_segments = mathutil.div_ceil(self._verifycap.size,
+                                              self.segment_size)
+
+        self.tail_data_size = self._verifycap.size % self.segment_size
+        if not self.tail_data_size:
+            self.tail_data_size = self.segment_size
+        # padding for erasure code
+        self.tail_segment_size = mathutil.next_multiple(self.tail_data_size,
+                                                        self._verifycap.needed_shares)
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B.
+        self.crypttext_root_hash = d['crypttext_root_hash']
+
+        self.share_root_hash = d['share_root_hash']
+
+
+        # Next: things that are optional and not redundant: crypttext_hash
+        if d.has_key('crypttext_hash'):
+            self.crypttext_hash = d['crypttext_hash']
+            if len(self.crypttext_hash) != CRYPTO_VAL_SIZE:
+                raise BadURIExtension('crypttext_hash is required to be hashutil.CRYPTO_VAL_SIZE bytes, not %s bytes' % (len(self.crypttext_hash),))
+
+
+        # Next: things that are optional, redundant, and required to be
+        # consistent: codec_name, codec_params, tail_codec_params,
+        # num_segments, size, needed_shares, total_shares
+        if d.has_key('codec_name'):
+            if d['codec_name'] != "crs":
+                raise UnsupportedErasureCodec(d['codec_name'])
+
+        if d.has_key('codec_params'):
+            ucpss, ucpns, ucpts = codec.parse_params(d['codec_params'])
+            if ucpss != self.segment_size:
+                raise BadURIExtension("inconsistent erasure code params: "
+                                      "ucpss: %s != self.segment_size: %s" %
+                                      (ucpss, self.segment_size))
+            if ucpns != self._verifycap.needed_shares:
+                raise BadURIExtension("inconsistent erasure code params: ucpns: %s != "
+                                      "self._verifycap.needed_shares: %s" %
+                                      (ucpns, self._verifycap.needed_shares))
+            if ucpts != self._verifycap.total_shares:
+                raise BadURIExtension("inconsistent erasure code params: ucpts: %s != "
+                                      "self._verifycap.total_shares: %s" %
+                                      (ucpts, self._verifycap.total_shares))
+
+        if d.has_key('tail_codec_params'):
+            utcpss, utcpns, utcpts = codec.parse_params(d['tail_codec_params'])
+            if utcpss != self.tail_segment_size:
+                raise BadURIExtension("inconsistent erasure code params: utcpss: %s != "
+                                      "self.tail_segment_size: %s, self._verifycap.size: %s, "
+                                      "self.segment_size: %s, self._verifycap.needed_shares: %s"
+                                      % (utcpss, self.tail_segment_size, self._verifycap.size,
+                                         self.segment_size, self._verifycap.needed_shares))
+            if utcpns != self._verifycap.needed_shares:
+                raise BadURIExtension("inconsistent erasure code params: utcpns: %s != "
+                                      "self._verifycap.needed_shares: %s" % (utcpns,
+                                                                             self._verifycap.needed_shares))
+            if utcpts != self._verifycap.total_shares:
+                raise BadURIExtension("inconsistent erasure code params: utcpts: %s != "
+                                      "self._verifycap.total_shares: %s" % (utcpts,
+                                                                            self._verifycap.total_shares))
+
+        if d.has_key('num_segments'):
+            if d['num_segments'] != self.num_segments:
+                raise BadURIExtension("inconsistent num_segments: size: %s, "
+                                      "segment_size: %s, computed_num_segments: %s, "
+                                      "ueb_num_segments: %s" % (self._verifycap.size,
+                                                                self.segment_size,
+                                                                self.num_segments, d['num_segments']))
+
+        if d.has_key('size'):
+            if d['size'] != self._verifycap.size:
+                raise BadURIExtension("inconsistent size: URI size: %s, UEB size: %s" %
+                                      (self._verifycap.size, d['size']))
+
+        if d.has_key('needed_shares'):
+            if d['needed_shares'] != self._verifycap.needed_shares:
+                raise BadURIExtension("inconsistent needed shares: URI needed shares: %s, UEB "
+                                      "needed shares: %s" % (self._verifycap.total_shares,
+                                                             d['needed_shares']))
+
+        if d.has_key('total_shares'):
+            if d['total_shares'] != self._verifycap.total_shares:
+                raise BadURIExtension("inconsistent total shares: URI total shares: %s, UEB "
+                                      "total shares: %s" % (self._verifycap.total_shares,
+                                                            d['total_shares']))
+
+        # Finally, things that are deprecated and ignored: plaintext_hash,
+        # plaintext_root_hash
+        if d.get('plaintext_hash'):
+            log.msg("Found plaintext_hash in UEB. This field is deprecated for security reasons "
+                    "and is no longer used.  Ignoring.  %s" % (self,))
+        if d.get('plaintext_root_hash'):
+            log.msg("Found plaintext_root_hash in UEB. This field is deprecated for security "
+                    "reasons and is no longer used.  Ignoring.  %s" % (self,))
+
+        return self
+
+    def start(self):
+        """Fetch the UEB from bucket, compare its hash to the hash from
+        verifycap, then parse it. Returns a deferred which is called back
+        with self once the fetch is successful, or is erred back if it
+        fails."""
+        d = self._readbucketproxy.get_uri_extension()
+        d.addCallback(self._check_integrity)
+        d.addCallback(self._parse_and_validate)
+        return d
+
+class ValidatedReadBucketProxy(log.PrefixingLogMixin):
+    """I am a front-end for a remote storage bucket, responsible for
+    retrieving and validating data from that bucket.
+
+    My get_block() method is used by BlockDownloaders.
+    """
+
+    def __init__(self, sharenum, bucket, share_hash_tree, num_blocks,
+                 block_size, share_size):
+        """ share_hash_tree is required to have already been initialized with
+        the root hash (the number-0 hash), using the share_root_hash from the
+        UEB"""
+        precondition(share_hash_tree[0] is not None, share_hash_tree)
+        prefix = "%d-%s-%s" % (sharenum, bucket,
+                               base32.b2a_l(share_hash_tree[0][:8], 60))
+        log.PrefixingLogMixin.__init__(self,
+                                       facility="tahoe.immutable.download",
+                                       prefix=prefix)
+        self.sharenum = sharenum
+        self.bucket = bucket
+        self.share_hash_tree = share_hash_tree
+        self.num_blocks = num_blocks
+        self.block_size = block_size
+        self.share_size = share_size
+        self.block_hash_tree = hashtree.IncompleteHashTree(self.num_blocks)
+
+    def get_all_sharehashes(self):
+        """Retrieve and validate all the share-hash-tree nodes that are
+        included in this share, regardless of whether we need them to
+        validate the share or not. Each share contains a minimal Merkle tree
+        chain, but there is lots of overlap, so usually we'll be using hashes
+        from other shares and not reading every single hash from this share.
+        The Verifier uses this function to read and validate every single
+        hash from this share.
+
+        Call this (and wait for the Deferred it returns to fire) before
+        calling get_block() for the first time: this lets us check that the
+        share share contains enough hashes to validate its own data, and
+        avoids downloading any share hash twice.
+
+        I return a Deferred which errbacks upon failure, probably with
+        BadOrMissingHash."""
+
+        d = self.bucket.get_share_hashes()
+        def _got_share_hashes(sh):
+            sharehashes = dict(sh)
+            try:
+                self.share_hash_tree.set_hashes(sharehashes)
+            except IndexError, le:
+                raise BadOrMissingHash(le)
+            except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
+                raise BadOrMissingHash(le)
+        d.addCallback(_got_share_hashes)
+        return d
+
+    def get_all_blockhashes(self):
+        """Retrieve and validate all the block-hash-tree nodes that are
+        included in this share. Each share contains a full Merkle tree, but
+        we usually only fetch the minimal subset necessary for any particular
+        block. This function fetches everything at once. The Verifier uses
+        this function to validate the block hash tree.
+
+        Call this (and wait for the Deferred it returns to fire) after
+        calling get_all_sharehashes() and before calling get_block() for the
+        first time: this lets us check that the share contains all block
+        hashes and avoids downloading them multiple times.
+
+        I return a Deferred which errbacks upon failure, probably with
+        BadOrMissingHash.
+        """
+
+        # get_block_hashes(anything) currently always returns everything
+        needed = list(range(len(self.block_hash_tree)))
+        d = self.bucket.get_block_hashes(needed)
+        def _got_block_hashes(blockhashes):
+            if len(blockhashes) < len(self.block_hash_tree):
+                raise BadOrMissingHash()
+            bh = dict(enumerate(blockhashes))
+
+            try:
+                self.block_hash_tree.set_hashes(bh)
+            except IndexError, le:
+                raise BadOrMissingHash(le)
+            except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
+                raise BadOrMissingHash(le)
+        d.addCallback(_got_block_hashes)
+        return d
+
+    def get_all_crypttext_hashes(self, crypttext_hash_tree):
+        """Retrieve and validate all the crypttext-hash-tree nodes that are
+        in this share. Normally we don't look at these at all: the download
+        process fetches them incrementally as needed to validate each segment
+        of ciphertext. But this is a convenient place to give the Verifier a
+        function to validate all of these at once.
+
+        Call this with a new hashtree object for each share, initialized with
+        the crypttext hash tree root. I return a Deferred which errbacks upon
+        failure, probably with BadOrMissingHash.
+        """
+
+        # get_crypttext_hashes() always returns everything
+        d = self.bucket.get_crypttext_hashes()
+        def _got_crypttext_hashes(hashes):
+            if len(hashes) < len(crypttext_hash_tree):
+                raise BadOrMissingHash()
+            ct_hashes = dict(enumerate(hashes))
+            try:
+                crypttext_hash_tree.set_hashes(ct_hashes)
+            except IndexError, le:
+                raise BadOrMissingHash(le)
+            except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
+                raise BadOrMissingHash(le)
+        d.addCallback(_got_crypttext_hashes)
+        return d
+
+    def get_block(self, blocknum):
+        # the first time we use this bucket, we need to fetch enough elements
+        # of the share hash tree to validate it from our share hash up to the
+        # hashroot.
+        if self.share_hash_tree.needed_hashes(self.sharenum):
+            d1 = self.bucket.get_share_hashes()
+        else:
+            d1 = defer.succeed([])
+
+        # We might need to grab some elements of our block hash tree, to
+        # validate the requested block up to the share hash.
+        blockhashesneeded = self.block_hash_tree.needed_hashes(blocknum, include_leaf=True)
+        # We don't need the root of the block hash tree, as that comes in the
+        # share tree.
+        blockhashesneeded.discard(0)
+        d2 = self.bucket.get_block_hashes(blockhashesneeded)
+
+        if blocknum < self.num_blocks-1:
+            thisblocksize = self.block_size
+        else:
+            thisblocksize = self.share_size % self.block_size
+            if thisblocksize == 0:
+                thisblocksize = self.block_size
+        d3 = self.bucket.get_block_data(blocknum,
+                                        self.block_size, thisblocksize)
+
+        dl = deferredutil.gatherResults([d1, d2, d3])
+        dl.addCallback(self._got_data, blocknum)
+        return dl
+
+    def _got_data(self, results, blocknum):
+        precondition(blocknum < self.num_blocks,
+                     self, blocknum, self.num_blocks)
+        sharehashes, blockhashes, blockdata = results
+        try:
+            sharehashes = dict(sharehashes)
+        except ValueError, le:
+            le.args = tuple(le.args + (sharehashes,))
+            raise
+        blockhashes = dict(enumerate(blockhashes))
+
+        candidate_share_hash = None # in case we log it in the except block below
+        blockhash = None # in case we log it in the except block below
+
+        try:
+            if self.share_hash_tree.needed_hashes(self.sharenum):
+                # This will raise exception if the values being passed do not
+                # match the root node of self.share_hash_tree.
+                try:
+                    self.share_hash_tree.set_hashes(sharehashes)
+                except IndexError, le:
+                    # Weird -- sharehashes contained index numbers outside of
+                    # the range that fit into this hash tree.
+                    raise BadOrMissingHash(le)
+
+            # To validate a block we need the root of the block hash tree,
+            # which is also one of the leafs of the share hash tree, and is
+            # called "the share hash".
+            if not self.block_hash_tree[0]: # empty -- no root node yet
+                # Get the share hash from the share hash tree.
+                share_hash = self.share_hash_tree.get_leaf(self.sharenum)
+                if not share_hash:
+                    # No root node in block_hash_tree and also the share hash
+                    # wasn't sent by the server.
+                    raise hashtree.NotEnoughHashesError
+                self.block_hash_tree.set_hashes({0: share_hash})
+
+            if self.block_hash_tree.needed_hashes(blocknum):
+                self.block_hash_tree.set_hashes(blockhashes)
+
+            blockhash = block_hash(blockdata)
+            self.block_hash_tree.set_hashes(leaves={blocknum: blockhash})
+            #self.log("checking block_hash(shareid=%d, blocknum=%d) len=%d "
+            #        "%r .. %r: %s" %
+            #        (self.sharenum, blocknum, len(blockdata),
+            #         blockdata[:50], blockdata[-50:], base32.b2a(blockhash)))
+
+        except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
+            # log.WEIRD: indicates undetected disk/network error, or more
+            # likely a programming error
+            self.log("hash failure in block=%d, shnum=%d on %s" %
+                    (blocknum, self.sharenum, self.bucket))
+            if self.block_hash_tree.needed_hashes(blocknum):
+                self.log(""" failure occurred when checking the block_hash_tree.
+                This suggests that either the block data was bad, or that the
+                block hashes we received along with it were bad.""")
+            else:
+                self.log(""" the failure probably occurred when checking the
+                share_hash_tree, which suggests that the share hashes we
+                received from the remote peer were bad.""")
+            self.log(" have candidate_share_hash: %s" % bool(candidate_share_hash))
+            self.log(" block length: %d" % len(blockdata))
+            self.log(" block hash: %s" % base32.b2a_or_none(blockhash))
+            if len(blockdata) < 100:
+                self.log(" block data: %r" % (blockdata,))
+            else:
+                self.log(" block data start/end: %r .. %r" %
+                        (blockdata[:50], blockdata[-50:]))
+            self.log(" share hash tree:\n" + self.share_hash_tree.dump())
+            self.log(" block hash tree:\n" + self.block_hash_tree.dump())
+            lines = []
+            for i,h in sorted(sharehashes.items()):
+                lines.append("%3d: %s" % (i, base32.b2a_or_none(h)))
+            self.log(" sharehashes:\n" + "\n".join(lines) + "\n")
+            lines = []
+            for i,h in blockhashes.items():
+                lines.append("%3d: %s" % (i, base32.b2a_or_none(h)))
+            log.msg(" blockhashes:\n" + "\n".join(lines) + "\n")
+            raise BadOrMissingHash(le)
+
+        # If we made it here, the block is good. If the hash trees didn't
+        # like what they saw, they would have raised a BadHashError, causing
+        # our caller to see a Failure and thus ignore this block (as well as
+        # dropping this bucket).
+        return blockdata
+
+
 class Checker(log.PrefixingLogMixin):
     """I query all servers to see if M uniquely-numbered shares are
     available.
@@ -148,18 +576,18 @@ class Checker(log.PrefixingLogMixin):
 
         vcap = self._verifycap
         b = layout.ReadBucketProxy(bucket, serverid, vcap.get_storage_index())
-        veup = download.ValidatedExtendedURIProxy(b, vcap)
+        veup = ValidatedExtendedURIProxy(b, vcap)
         d = veup.start()
 
         def _got_ueb(vup):
             share_hash_tree = IncompleteHashTree(vcap.total_shares)
             share_hash_tree.set_hashes({0: vup.share_root_hash})
 
-            vrbp = download.ValidatedReadBucketProxy(sharenum, b,
-                                                     share_hash_tree,
-                                                     vup.num_segments,
-                                                     vup.block_size,
-                                                     vup.share_size)
+            vrbp = ValidatedReadBucketProxy(sharenum, b,
+                                            share_hash_tree,
+                                            vup.num_segments,
+                                            vup.block_size,
+                                            vup.share_size)
 
             # note: normal download doesn't use get_all_sharehashes(),
             # because it gets more data than necessary. We've discussed the
@@ -218,8 +646,8 @@ class Checker(log.PrefixingLogMixin):
                 return (False, sharenum, 'incompatible')
             elif f.check(layout.LayoutInvalid,
                          layout.RidiculouslyLargeURIExtensionBlock,
-                         download.BadOrMissingHash,
-                         download.BadURIExtensionHashValue):
+                         BadOrMissingHash,
+                         BadURIExtensionHashValue):
                 return (False, sharenum, 'corrupt')
 
             # if it wasn't one of those reasons, re-raise the error
diff --git a/src/allmydata/immutable/download.py b/src/allmydata/immutable/download.py
deleted file mode 100644
index eb02c6a..0000000
--- a/src/allmydata/immutable/download.py
+++ /dev/null
@@ -1,1321 +0,0 @@
-import random, weakref, itertools, time
-from zope.interface import implements
-from twisted.internet import defer, reactor
-from twisted.internet.interfaces import IPushProducer, IConsumer
-from foolscap.api import DeadReferenceError, RemoteException, eventually
-
-from allmydata.util import base32, deferredutil, hashutil, log, mathutil, idlib
-from allmydata.util.assertutil import _assert, precondition
-from allmydata import codec, hashtree, uri
-from allmydata.interfaces import IDownloadTarget, IDownloader, IVerifierURI, \
-     IDownloadStatus, IDownloadResults, IValidatedThingProxy, \
-     IStorageBroker, NotEnoughSharesError, NoSharesError, NoServersError, \
-     UnableToFetchCriticalDownloadDataError
-from allmydata.immutable import layout
-from allmydata.monitor import Monitor
-from pycryptopp.cipher.aes import AES
-
-class IntegrityCheckReject(Exception):
-    pass
-
-class BadURIExtensionHashValue(IntegrityCheckReject):
-    pass
-class BadURIExtension(IntegrityCheckReject):
-    pass
-class UnsupportedErasureCodec(BadURIExtension):
-    pass
-class BadCrypttextHashValue(IntegrityCheckReject):
-    pass
-class BadOrMissingHash(IntegrityCheckReject):
-    pass
-
-class DownloadStopped(Exception):
-    pass
-
-class DownloadResults:
-    implements(IDownloadResults)
-
-    def __init__(self):
-        self.servers_used = set()
-        self.server_problems = {}
-        self.servermap = {}
-        self.timings = {}
-        self.file_size = None
-
-class DecryptingTarget(log.PrefixingLogMixin):
-    implements(IDownloadTarget, IConsumer)
-    def __init__(self, target, key, _log_msg_id=None):
-        precondition(IDownloadTarget.providedBy(target), target)
-        self.target = target
-        self._decryptor = AES(key)
-        prefix = str(target)
-        log.PrefixingLogMixin.__init__(self, "allmydata.immutable.download", _log_msg_id, prefix=prefix)
-    # methods to satisfy the IConsumer interface
-    def registerProducer(self, producer, streaming):
-        if IConsumer.providedBy(self.target):
-            self.target.registerProducer(producer, streaming)
-    def unregisterProducer(self):
-        if IConsumer.providedBy(self.target):
-            self.target.unregisterProducer()
-    def write(self, ciphertext):
-        plaintext = self._decryptor.process(ciphertext)
-        self.target.write(plaintext)
-    def open(self, size):
-        self.target.open(size)
-    def close(self):
-        self.target.close()
-    def finish(self):
-        return self.target.finish()
-    # The following methods is just to pass through to the next target, and
-    # just because that target might be a repairer.DownUpConnector, and just
-    # because the current CHKUpload object expects to find the storage index
-    # in its Uploadable.
-    def set_storageindex(self, storageindex):
-        self.target.set_storageindex(storageindex)
-    def set_encodingparams(self, encodingparams):
-        self.target.set_encodingparams(encodingparams)
-
-class ValidatedThingObtainer:
-    def __init__(self, validatedthingproxies, debugname, log_id):
-        self._validatedthingproxies = validatedthingproxies
-        self._debugname = debugname
-        self._log_id = log_id
-
-    def _bad(self, f, validatedthingproxy):
-        f.trap(RemoteException, DeadReferenceError,
-               IntegrityCheckReject, layout.LayoutInvalid,
-               layout.ShareVersionIncompatible)
-        level = log.WEIRD
-        if f.check(DeadReferenceError):
-            level = log.UNUSUAL
-        elif f.check(RemoteException):
-            level = log.WEIRD
-        else:
-            level = log.SCARY
-        log.msg(parent=self._log_id, facility="tahoe.immutable.download",
-                format="operation %(op)s from validatedthingproxy %(validatedthingproxy)s failed",
-                op=self._debugname, validatedthingproxy=str(validatedthingproxy),
-                failure=f, level=level, umid="JGXxBA")
-        if not self._validatedthingproxies:
-            raise UnableToFetchCriticalDownloadDataError("ran out of peers, last error was %s" % (f,))
-        # try again with a different one
-        d = self._try_the_next_one()
-        return d
-
-    def _try_the_next_one(self):
-        vtp = self._validatedthingproxies.pop(0)
-        # start() obtains, validates, and callsback-with the thing or else
-        # errbacks
-        d = vtp.start()
-        d.addErrback(self._bad, vtp)
-        return d
-
-    def start(self):
-        return self._try_the_next_one()
-
-class ValidatedCrypttextHashTreeProxy:
-    implements(IValidatedThingProxy)
-    """ I am a front-end for a remote crypttext hash tree using a local
-    ReadBucketProxy -- I use its get_crypttext_hashes() method and offer the
-    Validated Thing protocol (i.e., I have a start() method that fires with
-    self once I get a valid one)."""
-    def __init__(self, readbucketproxy, crypttext_hash_tree, num_segments,
-                 fetch_failures=None):
-        # fetch_failures is for debugging -- see test_encode.py
-        self._readbucketproxy = readbucketproxy
-        self._num_segments = num_segments
-        self._fetch_failures = fetch_failures
-        self._crypttext_hash_tree = crypttext_hash_tree
-
-    def _validate(self, proposal):
-        ct_hashes = dict(list(enumerate(proposal)))
-        try:
-            self._crypttext_hash_tree.set_hashes(ct_hashes)
-        except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
-            if self._fetch_failures is not None:
-                self._fetch_failures["crypttext_hash_tree"] += 1
-            raise BadOrMissingHash(le)
-        # If we now have enough of the crypttext hash tree to integrity-check
-        # *any* segment of ciphertext, then we are done. TODO: It would have
-        # better alacrity if we downloaded only part of the crypttext hash
-        # tree at a time.
-        for segnum in range(self._num_segments):
-            if self._crypttext_hash_tree.needed_hashes(segnum):
-                raise BadOrMissingHash("not enough hashes to validate segment number %d" % (segnum,))
-        return self
-
-    def start(self):
-        d = self._readbucketproxy.get_crypttext_hashes()
-        d.addCallback(self._validate)
-        return d
-
-class ValidatedExtendedURIProxy:
-    implements(IValidatedThingProxy)
-    """ I am a front-end for a remote UEB (using a local ReadBucketProxy),
-    responsible for retrieving and validating the elements from the UEB."""
-
-    def __init__(self, readbucketproxy, verifycap, fetch_failures=None):
-        # fetch_failures is for debugging -- see test_encode.py
-        self._fetch_failures = fetch_failures
-        self._readbucketproxy = readbucketproxy
-        precondition(IVerifierURI.providedBy(verifycap), verifycap)
-        self._verifycap = verifycap
-
-        # required
-        self.segment_size = None
-        self.crypttext_root_hash = None
-        self.share_root_hash = None
-
-        # computed
-        self.block_size = None
-        self.share_size = None
-        self.num_segments = None
-        self.tail_data_size = None
-        self.tail_segment_size = None
-
-        # optional
-        self.crypttext_hash = None
-
-    def __str__(self):
-        return "<%s %s>" % (self.__class__.__name__, self._verifycap.to_string())
-
-    def _check_integrity(self, data):
-        h = hashutil.uri_extension_hash(data)
-        if h != self._verifycap.uri_extension_hash:
-            msg = ("The copy of uri_extension we received from %s was bad: wanted %s, got %s" %
-                   (self._readbucketproxy,
-                    base32.b2a(self._verifycap.uri_extension_hash),
-                    base32.b2a(h)))
-            if self._fetch_failures is not None:
-                self._fetch_failures["uri_extension"] += 1
-            raise BadURIExtensionHashValue(msg)
-        else:
-            return data
-
-    def _parse_and_validate(self, data):
-        self.share_size = mathutil.div_ceil(self._verifycap.size,
-                                            self._verifycap.needed_shares)
-
-        d = uri.unpack_extension(data)
-
-        # There are several kinds of things that can be found in a UEB.
-        # First, things that we really need to learn from the UEB in order to
-        # do this download. Next: things which are optional but not redundant
-        # -- if they are present in the UEB they will get used. Next, things
-        # that are optional and redundant. These things are required to be
-        # consistent: they don't have to be in the UEB, but if they are in
-        # the UEB then they will be checked for consistency with the
-        # already-known facts, and if they are inconsistent then an exception
-        # will be raised. These things aren't actually used -- they are just
-        # tested for consistency and ignored. Finally: things which are
-        # deprecated -- they ought not be in the UEB at all, and if they are
-        # present then a warning will be logged but they are otherwise
-        # ignored.
-
-        # First, things that we really need to learn from the UEB:
-        # segment_size, crypttext_root_hash, and share_root_hash.
-        self.segment_size = d['segment_size']
-
-        self.block_size = mathutil.div_ceil(self.segment_size,
-                                            self._verifycap.needed_shares)
-        self.num_segments = mathutil.div_ceil(self._verifycap.size,
-                                              self.segment_size)
-
-        self.tail_data_size = self._verifycap.size % self.segment_size
-        if not self.tail_data_size:
-            self.tail_data_size = self.segment_size
-        # padding for erasure code
-        self.tail_segment_size = mathutil.next_multiple(self.tail_data_size,
-                                                        self._verifycap.needed_shares)
-
-        # Ciphertext hash tree root is mandatory, so that there is at most
-        # one ciphertext that matches this read-cap or verify-cap. The
-        # integrity check on the shares is not sufficient to prevent the
-        # original encoder from creating some shares of file A and other
-        # shares of file B.
-        self.crypttext_root_hash = d['crypttext_root_hash']
-
-        self.share_root_hash = d['share_root_hash']
-
-
-        # Next: things that are optional and not redundant: crypttext_hash
-        if d.has_key('crypttext_hash'):
-            self.crypttext_hash = d['crypttext_hash']
-            if len(self.crypttext_hash) != hashutil.CRYPTO_VAL_SIZE:
-                raise BadURIExtension('crypttext_hash is required to be hashutil.CRYPTO_VAL_SIZE bytes, not %s bytes' % (len(self.crypttext_hash),))
-
-
-        # Next: things that are optional, redundant, and required to be
-        # consistent: codec_name, codec_params, tail_codec_params,
-        # num_segments, size, needed_shares, total_shares
-        if d.has_key('codec_name'):
-            if d['codec_name'] != "crs":
-                raise UnsupportedErasureCodec(d['codec_name'])
-
-        if d.has_key('codec_params'):
-            ucpss, ucpns, ucpts = codec.parse_params(d['codec_params'])
-            if ucpss != self.segment_size:
-                raise BadURIExtension("inconsistent erasure code params: "
-                                      "ucpss: %s != self.segment_size: %s" %
-                                      (ucpss, self.segment_size))
-            if ucpns != self._verifycap.needed_shares:
-                raise BadURIExtension("inconsistent erasure code params: ucpns: %s != "
-                                      "self._verifycap.needed_shares: %s" %
-                                      (ucpns, self._verifycap.needed_shares))
-            if ucpts != self._verifycap.total_shares:
-                raise BadURIExtension("inconsistent erasure code params: ucpts: %s != "
-                                      "self._verifycap.total_shares: %s" %
-                                      (ucpts, self._verifycap.total_shares))
-
-        if d.has_key('tail_codec_params'):
-            utcpss, utcpns, utcpts = codec.parse_params(d['tail_codec_params'])
-            if utcpss != self.tail_segment_size:
-                raise BadURIExtension("inconsistent erasure code params: utcpss: %s != "
-                                      "self.tail_segment_size: %s, self._verifycap.size: %s, "
-                                      "self.segment_size: %s, self._verifycap.needed_shares: %s"
-                                      % (utcpss, self.tail_segment_size, self._verifycap.size,
-                                         self.segment_size, self._verifycap.needed_shares))
-            if utcpns != self._verifycap.needed_shares:
-                raise BadURIExtension("inconsistent erasure code params: utcpns: %s != "
-                                      "self._verifycap.needed_shares: %s" % (utcpns,
-                                                                             self._verifycap.needed_shares))
-            if utcpts != self._verifycap.total_shares:
-                raise BadURIExtension("inconsistent erasure code params: utcpts: %s != "
-                                      "self._verifycap.total_shares: %s" % (utcpts,
-                                                                            self._verifycap.total_shares))
-
-        if d.has_key('num_segments'):
-            if d['num_segments'] != self.num_segments:
-                raise BadURIExtension("inconsistent num_segments: size: %s, "
-                                      "segment_size: %s, computed_num_segments: %s, "
-                                      "ueb_num_segments: %s" % (self._verifycap.size,
-                                                                self.segment_size,
-                                                                self.num_segments, d['num_segments']))
-
-        if d.has_key('size'):
-            if d['size'] != self._verifycap.size:
-                raise BadURIExtension("inconsistent size: URI size: %s, UEB size: %s" %
-                                      (self._verifycap.size, d['size']))
-
-        if d.has_key('needed_shares'):
-            if d['needed_shares'] != self._verifycap.needed_shares:
-                raise BadURIExtension("inconsistent needed shares: URI needed shares: %s, UEB "
-                                      "needed shares: %s" % (self._verifycap.total_shares,
-                                                             d['needed_shares']))
-
-        if d.has_key('total_shares'):
-            if d['total_shares'] != self._verifycap.total_shares:
-                raise BadURIExtension("inconsistent total shares: URI total shares: %s, UEB "
-                                      "total shares: %s" % (self._verifycap.total_shares,
-                                                            d['total_shares']))
-
-        # Finally, things that are deprecated and ignored: plaintext_hash,
-        # plaintext_root_hash
-        if d.get('plaintext_hash'):
-            log.msg("Found plaintext_hash in UEB. This field is deprecated for security reasons "
-                    "and is no longer used.  Ignoring.  %s" % (self,))
-        if d.get('plaintext_root_hash'):
-            log.msg("Found plaintext_root_hash in UEB. This field is deprecated for security "
-                    "reasons and is no longer used.  Ignoring.  %s" % (self,))
-
-        return self
-
-    def start(self):
-        """Fetch the UEB from bucket, compare its hash to the hash from
-        verifycap, then parse it. Returns a deferred which is called back
-        with self once the fetch is successful, or is erred back if it
-        fails."""
-        d = self._readbucketproxy.get_uri_extension()
-        d.addCallback(self._check_integrity)
-        d.addCallback(self._parse_and_validate)
-        return d
-
-class ValidatedReadBucketProxy(log.PrefixingLogMixin):
-    """I am a front-end for a remote storage bucket, responsible for
-    retrieving and validating data from that bucket.
-
-    My get_block() method is used by BlockDownloaders.
-    """
-
-    def __init__(self, sharenum, bucket, share_hash_tree, num_blocks,
-                 block_size, share_size):
-        """ share_hash_tree is required to have already been initialized with
-        the root hash (the number-0 hash), using the share_root_hash from the
-        UEB"""
-        precondition(share_hash_tree[0] is not None, share_hash_tree)
-        prefix = "%d-%s-%s" % (sharenum, bucket,
-                               base32.b2a_l(share_hash_tree[0][:8], 60))
-        log.PrefixingLogMixin.__init__(self,
-                                       facility="tahoe.immutable.download",
-                                       prefix=prefix)
-        self.sharenum = sharenum
-        self.bucket = bucket
-        self.share_hash_tree = share_hash_tree
-        self.num_blocks = num_blocks
-        self.block_size = block_size
-        self.share_size = share_size
-        self.block_hash_tree = hashtree.IncompleteHashTree(self.num_blocks)
-
-    def get_all_sharehashes(self):
-        """Retrieve and validate all the share-hash-tree nodes that are
-        included in this share, regardless of whether we need them to
-        validate the share or not. Each share contains a minimal Merkle tree
-        chain, but there is lots of overlap, so usually we'll be using hashes
-        from other shares and not reading every single hash from this share.
-        The Verifier uses this function to read and validate every single
-        hash from this share.
-
-        Call this (and wait for the Deferred it returns to fire) before
-        calling get_block() for the first time: this lets us check that the
-        share share contains enough hashes to validate its own data, and
-        avoids downloading any share hash twice.
-
-        I return a Deferred which errbacks upon failure, probably with
-        BadOrMissingHash."""
-
-        d = self.bucket.get_share_hashes()
-        def _got_share_hashes(sh):
-            sharehashes = dict(sh)
-            try:
-                self.share_hash_tree.set_hashes(sharehashes)
-            except IndexError, le:
-                raise BadOrMissingHash(le)
-            except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
-                raise BadOrMissingHash(le)
-        d.addCallback(_got_share_hashes)
-        return d
-
-    def get_all_blockhashes(self):
-        """Retrieve and validate all the block-hash-tree nodes that are
-        included in this share. Each share contains a full Merkle tree, but
-        we usually only fetch the minimal subset necessary for any particular
-        block. This function fetches everything at once. The Verifier uses
-        this function to validate the block hash tree.
-
-        Call this (and wait for the Deferred it returns to fire) after
-        calling get_all_sharehashes() and before calling get_block() for the
-        first time: this lets us check that the share contains all block
-        hashes and avoids downloading them multiple times.
-
-        I return a Deferred which errbacks upon failure, probably with
-        BadOrMissingHash.
-        """
-
-        # get_block_hashes(anything) currently always returns everything
-        needed = list(range(len(self.block_hash_tree)))
-        d = self.bucket.get_block_hashes(needed)
-        def _got_block_hashes(blockhashes):
-            if len(blockhashes) < len(self.block_hash_tree):
-                raise BadOrMissingHash()
-            bh = dict(enumerate(blockhashes))
-
-            try:
-                self.block_hash_tree.set_hashes(bh)
-            except IndexError, le:
-                raise BadOrMissingHash(le)
-            except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
-                raise BadOrMissingHash(le)
-        d.addCallback(_got_block_hashes)
-        return d
-
-    def get_all_crypttext_hashes(self, crypttext_hash_tree):
-        """Retrieve and validate all the crypttext-hash-tree nodes that are
-        in this share. Normally we don't look at these at all: the download
-        process fetches them incrementally as needed to validate each segment
-        of ciphertext. But this is a convenient place to give the Verifier a
-        function to validate all of these at once.
-
-        Call this with a new hashtree object for each share, initialized with
-        the crypttext hash tree root. I return a Deferred which errbacks upon
-        failure, probably with BadOrMissingHash.
-        """
-
-        # get_crypttext_hashes() always returns everything
-        d = self.bucket.get_crypttext_hashes()
-        def _got_crypttext_hashes(hashes):
-            if len(hashes) < len(crypttext_hash_tree):
-                raise BadOrMissingHash()
-            ct_hashes = dict(enumerate(hashes))
-            try:
-                crypttext_hash_tree.set_hashes(ct_hashes)
-            except IndexError, le:
-                raise BadOrMissingHash(le)
-            except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
-                raise BadOrMissingHash(le)
-        d.addCallback(_got_crypttext_hashes)
-        return d
-
-    def get_block(self, blocknum):
-        # the first time we use this bucket, we need to fetch enough elements
-        # of the share hash tree to validate it from our share hash up to the
-        # hashroot.
-        if self.share_hash_tree.needed_hashes(self.sharenum):
-            d1 = self.bucket.get_share_hashes()
-        else:
-            d1 = defer.succeed([])
-
-        # We might need to grab some elements of our block hash tree, to
-        # validate the requested block up to the share hash.
-        blockhashesneeded = self.block_hash_tree.needed_hashes(blocknum, include_leaf=True)
-        # We don't need the root of the block hash tree, as that comes in the
-        # share tree.
-        blockhashesneeded.discard(0)
-        d2 = self.bucket.get_block_hashes(blockhashesneeded)
-
-        if blocknum < self.num_blocks-1:
-            thisblocksize = self.block_size
-        else:
-            thisblocksize = self.share_size % self.block_size
-            if thisblocksize == 0:
-                thisblocksize = self.block_size
-        d3 = self.bucket.get_block_data(blocknum,
-                                        self.block_size, thisblocksize)
-
-        dl = deferredutil.gatherResults([d1, d2, d3])
-        dl.addCallback(self._got_data, blocknum)
-        return dl
-
-    def _got_data(self, results, blocknum):
-        precondition(blocknum < self.num_blocks,
-                     self, blocknum, self.num_blocks)
-        sharehashes, blockhashes, blockdata = results
-        try:
-            sharehashes = dict(sharehashes)
-        except ValueError, le:
-            le.args = tuple(le.args + (sharehashes,))
-            raise
-        blockhashes = dict(enumerate(blockhashes))
-
-        candidate_share_hash = None # in case we log it in the except block below
-        blockhash = None # in case we log it in the except block below
-
-        try:
-            if self.share_hash_tree.needed_hashes(self.sharenum):
-                # This will raise exception if the values being passed do not
-                # match the root node of self.share_hash_tree.
-                try:
-                    self.share_hash_tree.set_hashes(sharehashes)
-                except IndexError, le:
-                    # Weird -- sharehashes contained index numbers outside of
-                    # the range that fit into this hash tree.
-                    raise BadOrMissingHash(le)
-
-            # To validate a block we need the root of the block hash tree,
-            # which is also one of the leafs of the share hash tree, and is
-            # called "the share hash".
-            if not self.block_hash_tree[0]: # empty -- no root node yet
-                # Get the share hash from the share hash tree.
-                share_hash = self.share_hash_tree.get_leaf(self.sharenum)
-                if not share_hash:
-                    # No root node in block_hash_tree and also the share hash
-                    # wasn't sent by the server.
-                    raise hashtree.NotEnoughHashesError
-                self.block_hash_tree.set_hashes({0: share_hash})
-
-            if self.block_hash_tree.needed_hashes(blocknum):
-                self.block_hash_tree.set_hashes(blockhashes)
-
-            blockhash = hashutil.block_hash(blockdata)
-            self.block_hash_tree.set_hashes(leaves={blocknum: blockhash})
-            #self.log("checking block_hash(shareid=%d, blocknum=%d) len=%d "
-            #        "%r .. %r: %s" %
-            #        (self.sharenum, blocknum, len(blockdata),
-            #         blockdata[:50], blockdata[-50:], base32.b2a(blockhash)))
-
-        except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
-            # log.WEIRD: indicates undetected disk/network error, or more
-            # likely a programming error
-            self.log("hash failure in block=%d, shnum=%d on %s" %
-                    (blocknum, self.sharenum, self.bucket))
-            if self.block_hash_tree.needed_hashes(blocknum):
-                self.log(""" failure occurred when checking the block_hash_tree.
-                This suggests that either the block data was bad, or that the
-                block hashes we received along with it were bad.""")
-            else:
-                self.log(""" the failure probably occurred when checking the
-                share_hash_tree, which suggests that the share hashes we
-                received from the remote peer were bad.""")
-            self.log(" have candidate_share_hash: %s" % bool(candidate_share_hash))
-            self.log(" block length: %d" % len(blockdata))
-            self.log(" block hash: %s" % base32.b2a_or_none(blockhash))
-            if len(blockdata) < 100:
-                self.log(" block data: %r" % (blockdata,))
-            else:
-                self.log(" block data start/end: %r .. %r" %
-                        (blockdata[:50], blockdata[-50:]))
-            self.log(" share hash tree:\n" + self.share_hash_tree.dump())
-            self.log(" block hash tree:\n" + self.block_hash_tree.dump())
-            lines = []
-            for i,h in sorted(sharehashes.items()):
-                lines.append("%3d: %s" % (i, base32.b2a_or_none(h)))
-            self.log(" sharehashes:\n" + "\n".join(lines) + "\n")
-            lines = []
-            for i,h in blockhashes.items():
-                lines.append("%3d: %s" % (i, base32.b2a_or_none(h)))
-            log.msg(" blockhashes:\n" + "\n".join(lines) + "\n")
-            raise BadOrMissingHash(le)
-
-        # If we made it here, the block is good. If the hash trees didn't
-        # like what they saw, they would have raised a BadHashError, causing
-        # our caller to see a Failure and thus ignore this block (as well as
-        # dropping this bucket).
-        return blockdata
-
-
-
-class BlockDownloader(log.PrefixingLogMixin):
-    """I am responsible for downloading a single block (from a single bucket)
-    for a single segment.
-
-    I am a child of the SegmentDownloader.
-    """
-
-    def __init__(self, vbucket, blocknum, parent, results):
-        precondition(isinstance(vbucket, ValidatedReadBucketProxy), vbucket)
-        prefix = "%s-%d" % (vbucket, blocknum)
-        log.PrefixingLogMixin.__init__(self, facility="tahoe.immutable.download", prefix=prefix)
-        self.vbucket = vbucket
-        self.blocknum = blocknum
-        self.parent = parent
-        self.results = results
-
-    def start(self, segnum):
-        self.log("get_block(segnum=%d)" % segnum)
-        started = time.time()
-        d = self.vbucket.get_block(segnum)
-        d.addCallbacks(self._hold_block, self._got_block_error,
-                       callbackArgs=(started,))
-        return d
-
-    def _hold_block(self, data, started):
-        if self.results:
-            elapsed = time.time() - started
-            peerid = self.vbucket.bucket.get_peerid()
-            if peerid not in self.results.timings["fetch_per_server"]:
-                self.results.timings["fetch_per_server"][peerid] = []
-            self.results.timings["fetch_per_server"][peerid].append(elapsed)
-        self.log("got block")
-        self.parent.hold_block(self.blocknum, data)
-
-    def _got_block_error(self, f):
-        f.trap(RemoteException, DeadReferenceError,
-               IntegrityCheckReject, layout.LayoutInvalid,
-               layout.ShareVersionIncompatible)
-        if f.check(RemoteException, DeadReferenceError):
-            level = log.UNUSUAL
-        else:
-            level = log.WEIRD
-        self.log("failure to get block", level=level, umid="5Z4uHQ")
-        if self.results:
-            peerid = self.vbucket.bucket.get_peerid()
-            self.results.server_problems[peerid] = str(f)
-        self.parent.bucket_failed(self.vbucket)
-
-class SegmentDownloader:
-    """I am responsible for downloading all the blocks for a single segment
-    of data.
-
-    I am a child of the CiphertextDownloader.
-    """
-
-    def __init__(self, parent, segmentnumber, needed_shares, results):
-        self.parent = parent
-        self.segmentnumber = segmentnumber
-        self.needed_blocks = needed_shares
-        self.blocks = {} # k: blocknum, v: data
-        self.results = results
-        self._log_number = self.parent.log("starting segment %d" %
-                                           segmentnumber)
-
-    def log(self, *args, **kwargs):
-        if "parent" not in kwargs:
-            kwargs["parent"] = self._log_number
-        return self.parent.log(*args, **kwargs)
-
-    def start(self):
-        return self._download()
-
-    def _download(self):
-        d = self._try()
-        def _done(res):
-            if len(self.blocks) >= self.needed_blocks:
-                # we only need self.needed_blocks blocks
-                # we want to get the smallest blockids, because they are
-                # more likely to be fast "primary blocks"
-                blockids = sorted(self.blocks.keys())[:self.needed_blocks]
-                blocks = []
-                for blocknum in blockids:
-                    blocks.append(self.blocks[blocknum])
-                return (blocks, blockids)
-            else:
-                return self._download()
-        d.addCallback(_done)
-        return d
-
-    def _try(self):
-        # fill our set of active buckets, maybe raising NotEnoughSharesError
-        active_buckets = self.parent._activate_enough_buckets()
-        # Now we have enough buckets, in self.parent.active_buckets.
-
-        # in test cases, bd.start might mutate active_buckets right away, so
-        # we need to put off calling start() until we've iterated all the way
-        # through it.
-        downloaders = []
-        for blocknum, vbucket in active_buckets.iteritems():
-            assert isinstance(vbucket, ValidatedReadBucketProxy), vbucket
-            bd = BlockDownloader(vbucket, blocknum, self, self.results)
-            downloaders.append(bd)
-            if self.results:
-                self.results.servers_used.add(vbucket.bucket.get_peerid())
-        l = [bd.start(self.segmentnumber) for bd in downloaders]
-        return defer.DeferredList(l, fireOnOneErrback=True)
-
-    def hold_block(self, blocknum, data):
-        self.blocks[blocknum] = data
-
-    def bucket_failed(self, vbucket):
-        self.parent.bucket_failed(vbucket)
-
-class DownloadStatus:
-    implements(IDownloadStatus)
-    statusid_counter = itertools.count(0)
-
-    def __init__(self):
-        self.storage_index = None
-        self.size = None
-        self.helper = False
-        self.status = "Not started"
-        self.progress = 0.0
-        self.paused = False
-        self.stopped = False
-        self.active = True
-        self.results = None
-        self.counter = self.statusid_counter.next()
-        self.started = time.time()
-
-    def get_started(self):
-        return self.started
-    def get_storage_index(self):
-        return self.storage_index
-    def get_size(self):
-        return self.size
-    def using_helper(self):
-        return self.helper
-    def get_status(self):
-        status = self.status
-        if self.paused:
-            status += " (output paused)"
-        if self.stopped:
-            status += " (output stopped)"
-        return status
-    def get_progress(self):
-        return self.progress
-    def get_active(self):
-        return self.active
-    def get_results(self):
-        return self.results
-    def get_counter(self):
-        return self.counter
-
-    def set_storage_index(self, si):
-        self.storage_index = si
-    def set_size(self, size):
-        self.size = size
-    def set_helper(self, helper):
-        self.helper = helper
-    def set_status(self, status):
-        self.status = status
-    def set_paused(self, paused):
-        self.paused = paused
-    def set_stopped(self, stopped):
-        self.stopped = stopped
-    def set_progress(self, value):
-        self.progress = value
-    def set_active(self, value):
-        self.active = value
-    def set_results(self, value):
-        self.results = value
-
-class CiphertextDownloader(log.PrefixingLogMixin):
-    """ I download shares, check their integrity, then decode them, check the
-    integrity of the resulting ciphertext, then and write it to my target.
-    Before I send any new request to a server, I always ask the 'monitor'
-    object that was passed into my constructor whether this task has been
-    cancelled (by invoking its raise_if_cancelled() method)."""
-    implements(IPushProducer)
-    _status = None
-
-    def __init__(self, storage_broker, v, target, monitor):
-
-        precondition(IStorageBroker.providedBy(storage_broker), storage_broker)
-        precondition(IVerifierURI.providedBy(v), v)
-        precondition(IDownloadTarget.providedBy(target), target)
-
-        self._storage_broker = storage_broker
-        self._verifycap = v
-        self._storage_index = v.get_storage_index()
-        self._uri_extension_hash = v.uri_extension_hash
-
-        prefix=base32.b2a_l(self._storage_index[:8], 60)
-        log.PrefixingLogMixin.__init__(self, facility="tahoe.immutable.download", prefix=prefix)
-
-        self._started = time.time()
-        self._status = s = DownloadStatus()
-        s.set_status("Starting")
-        s.set_storage_index(self._storage_index)
-        s.set_size(self._verifycap.size)
-        s.set_helper(False)
-        s.set_active(True)
-
-        self._results = DownloadResults()
-        s.set_results(self._results)
-        self._results.file_size = self._verifycap.size
-        self._results.timings["servers_peer_selection"] = {}
-        self._results.timings["fetch_per_server"] = {}
-        self._results.timings["cumulative_fetch"] = 0.0
-        self._results.timings["cumulative_decode"] = 0.0
-        self._results.timings["cumulative_decrypt"] = 0.0
-        self._results.timings["paused"] = 0.0
-
-        self._paused = False
-        self._stopped = False
-        if IConsumer.providedBy(target):
-            target.registerProducer(self, True)
-        self._target = target
-        # Repairer (uploader) needs the storageindex.
-        self._target.set_storageindex(self._storage_index)
-        self._monitor = monitor
-        self._opened = False
-
-        self.active_buckets = {} # k: shnum, v: bucket
-        self._share_buckets = {} # k: sharenum, v: list of buckets
-
-        # _download_all_segments() will set this to:
-        # self._share_vbuckets = {} # k: shnum, v: set of ValidatedBuckets
-        self._share_vbuckets = None
-
-        self._fetch_failures = {"uri_extension": 0, "crypttext_hash_tree": 0, }
-
-        self._ciphertext_hasher = hashutil.crypttext_hasher()
-
-        self._bytes_done = 0
-        self._status.set_progress(float(self._bytes_done)/self._verifycap.size)
-
-        # _got_uri_extension() will create the following:
-        # self._crypttext_hash_tree
-        # self._share_hash_tree
-        # self._current_segnum = 0
-        # self._vup # ValidatedExtendedURIProxy
-
-        # _get_all_shareholders() will create the following:
-        # self._total_queries
-        # self._responses_received = 0
-        # self._queries_failed = 0
-
-        # This is solely for the use of unit tests. It will be triggered when
-        # we start downloading shares.
-        self._stage_4_d = defer.Deferred()
-
-    def pauseProducing(self):
-        if self._paused:
-            return
-        self._paused = defer.Deferred()
-        self._paused_at = time.time()
-        if self._status:
-            self._status.set_paused(True)
-
-    def resumeProducing(self):
-        if self._paused:
-            paused_for = time.time() - self._paused_at
-            self._results.timings['paused'] += paused_for
-            p = self._paused
-            self._paused = None
-            eventually(p.callback, None)
-            if self._status:
-                self._status.set_paused(False)
-
-    def stopProducing(self):
-        self.log("Download.stopProducing")
-        self._stopped = True
-        self.resumeProducing()
-        if self._status:
-            self._status.set_stopped(True)
-            self._status.set_active(False)
-
-    def start(self):
-        self.log("starting download")
-
-        # first step: who should we download from?
-        d = defer.maybeDeferred(self._get_all_shareholders)
-        d.addBoth(self._got_all_shareholders)
-        # now get the uri_extension block from somebody and integrity check
-        # it and parse and validate its contents
-        d.addCallback(self._obtain_uri_extension)
-        d.addCallback(self._get_crypttext_hash_tree)
-        # once we know that, we can download blocks from everybody
-        d.addCallback(self._download_all_segments)
-        def _finished(res):
-            if self._status:
-                self._status.set_status("Finished")
-                self._status.set_active(False)
-                self._status.set_paused(False)
-            if IConsumer.providedBy(self._target):
-                self._target.unregisterProducer()
-            return res
-        d.addBoth(_finished)
-        def _failed(why):
-            if self._status:
-                self._status.set_status("Failed")
-                self._status.set_active(False)
-            if why.check(DownloadStopped):
-                # DownloadStopped just means the consumer aborted the
-                # download; not so scary.
-                self.log("download stopped", level=log.UNUSUAL)
-            else:
-                # This is really unusual, and deserves maximum forensics.
-                self.log("download failed!", failure=why, level=log.SCARY,
-                         umid="lp1vaQ")
-            return why
-        d.addErrback(_failed)
-        d.addCallback(self._done)
-        return d
-
-    def _get_all_shareholders(self):
-        """ Once the number of buckets that I know about is >= K then I
-        callback the Deferred that I return.
-
-        If all of the get_buckets deferreds have fired (whether callback
-        or errback) and I still don't have enough buckets then I'll also
-        callback -- not errback -- the Deferred that I return.
-        """
-        wait_for_enough_buckets_d = defer.Deferred()
-        self._wait_for_enough_buckets_d = wait_for_enough_buckets_d
-
-        sb = self._storage_broker
-        servers = sb.get_servers_for_index(self._storage_index)
-        if not servers:
-            raise NoServersError("broker gave us no servers!")
-
-        self._total_queries = len(servers)
-        self._responses_received = 0
-        self._queries_failed = 0
-        for (peerid,ss) in servers:
-            self.log(format="sending DYHB to [%(peerid)s]",
-                     peerid=idlib.shortnodeid_b2a(peerid),
-                     level=log.NOISY, umid="rT03hg")
-            d = ss.callRemote("get_buckets", self._storage_index)
-            d.addCallbacks(self._got_response, self._got_error,
-                           callbackArgs=(peerid,))
-            d.addBoth(self._check_got_all_responses)
-
-        if self._status:
-            self._status.set_status("Locating Shares (%d/%d)" %
-                                    (self._responses_received,
-                                     self._total_queries))
-        return wait_for_enough_buckets_d
-
-    def _check_got_all_responses(self, ignored=None):
-        assert (self._responses_received+self._queries_failed) <= self._total_queries
-        if self._wait_for_enough_buckets_d and (self._responses_received+self._queries_failed) == self._total_queries:
-            reactor.callLater(0, self._wait_for_enough_buckets_d.callback, False)
-            self._wait_for_enough_buckets_d = None
-
-    def _got_response(self, buckets, peerid):
-        # Note that this can continue to receive responses after _wait_for_enough_buckets_d
-        # has fired.
-        self._responses_received += 1
-        self.log(format="got results from [%(peerid)s]: shnums %(shnums)s",
-                 peerid=idlib.shortnodeid_b2a(peerid),
-                 shnums=sorted(buckets.keys()),
-                 level=log.NOISY, umid="o4uwFg")
-        if self._results:
-            elapsed = time.time() - self._started
-            self._results.timings["servers_peer_selection"][peerid] = elapsed
-        if self._status:
-            self._status.set_status("Locating Shares (%d/%d)" %
-                                    (self._responses_received,
-                                     self._total_queries))
-        for sharenum, bucket in buckets.iteritems():
-            b = layout.ReadBucketProxy(bucket, peerid, self._storage_index)
-            self.add_share_bucket(sharenum, b)
-            # If we just got enough buckets for the first time, then fire the
-            # deferred. Then remove it from self so that we don't fire it
-            # again.
-            if self._wait_for_enough_buckets_d and len(self._share_buckets) >= self._verifycap.needed_shares:
-                reactor.callLater(0, self._wait_for_enough_buckets_d.callback, True)
-                self._wait_for_enough_buckets_d = None
-
-            if self._share_vbuckets is not None:
-                vbucket = ValidatedReadBucketProxy(sharenum, b, self._share_hash_tree, self._vup.num_segments, self._vup.block_size, self._vup.share_size)
-                self._share_vbuckets.setdefault(sharenum, set()).add(vbucket)
-
-            if self._results:
-                if peerid not in self._results.servermap:
-                    self._results.servermap[peerid] = set()
-                self._results.servermap[peerid].add(sharenum)
-
-    def add_share_bucket(self, sharenum, bucket):
-        # this is split out for the benefit of test_encode.py
-        self._share_buckets.setdefault(sharenum, []).append(bucket)
-
-    def _got_error(self, f):
-        self._queries_failed += 1
-        level = log.WEIRD
-        if f.check(DeadReferenceError):
-            level = log.UNUSUAL
-        self.log("Error during get_buckets", failure=f, level=level,
-                         umid="3uuBUQ")
-
-    def bucket_failed(self, vbucket):
-        shnum = vbucket.sharenum
-        del self.active_buckets[shnum]
-        s = self._share_vbuckets[shnum]
-        # s is a set of ValidatedReadBucketProxy instances
-        s.remove(vbucket)
-        # ... which might now be empty
-        if not s:
-            # there are no more buckets which can provide this share, so
-            # remove the key. This may prompt us to use a different share.
-            del self._share_vbuckets[shnum]
-
-    def _got_all_shareholders(self, res):
-        if self._results:
-            now = time.time()
-            self._results.timings["peer_selection"] = now - self._started
-
-        if len(self._share_buckets) < self._verifycap.needed_shares:
-            msg = "Failed to get enough shareholders: have %d, need %d" \
-                  % (len(self._share_buckets), self._verifycap.needed_shares)
-            if self._share_buckets:
-                raise NotEnoughSharesError(msg)
-            else:
-                raise NoSharesError(msg)
-
-        #for s in self._share_vbuckets.values():
-        #    for vb in s:
-        #        assert isinstance(vb, ValidatedReadBucketProxy), \
-        #               "vb is %s but should be a ValidatedReadBucketProxy" % (vb,)
-
-    def _obtain_uri_extension(self, ignored):
-        # all shareholders are supposed to have a copy of uri_extension, and
-        # all are supposed to be identical. We compute the hash of the data
-        # that comes back, and compare it against the version in our URI. If
-        # they don't match, ignore their data and try someone else.
-        if self._status:
-            self._status.set_status("Obtaining URI Extension")
-
-        uri_extension_fetch_started = time.time()
-
-        vups = []
-        for sharenum, buckets in self._share_buckets.iteritems():
-            for bucket in buckets:
-                vups.append(ValidatedExtendedURIProxy(bucket, self._verifycap, self._fetch_failures))
-        vto = ValidatedThingObtainer(vups, debugname="vups", log_id=self._parentmsgid)
-        d = vto.start()
-
-        def _got_uri_extension(vup):
-            precondition(isinstance(vup, ValidatedExtendedURIProxy), vup)
-            if self._results:
-                elapsed = time.time() - uri_extension_fetch_started
-                self._results.timings["uri_extension"] = elapsed
-
-            self._vup = vup
-            self._codec = codec.CRSDecoder()
-            self._codec.set_params(self._vup.segment_size, self._verifycap.needed_shares, self._verifycap.total_shares)
-            self._tail_codec = codec.CRSDecoder()
-            self._tail_codec.set_params(self._vup.tail_segment_size, self._verifycap.needed_shares, self._verifycap.total_shares)
-
-            self._current_segnum = 0
-
-            self._share_hash_tree = hashtree.IncompleteHashTree(self._verifycap.total_shares)
-            self._share_hash_tree.set_hashes({0: vup.share_root_hash})
-
-            self._crypttext_hash_tree = hashtree.IncompleteHashTree(self._vup.num_segments)
-            self._crypttext_hash_tree.set_hashes({0: self._vup.crypttext_root_hash})
-
-            # Repairer (uploader) needs the encodingparams.
-            self._target.set_encodingparams((
-                self._verifycap.needed_shares,
-                0, # see ticket #778 for why this is
-                self._verifycap.total_shares,
-                self._vup.segment_size
-                ))
-        d.addCallback(_got_uri_extension)
-        return d
-
-    def _get_crypttext_hash_tree(self, res):
-        vchtps = []
-        for sharenum, buckets in self._share_buckets.iteritems():
-            for bucket in buckets:
-                vchtp = ValidatedCrypttextHashTreeProxy(bucket, self._crypttext_hash_tree, self._vup.num_segments, self._fetch_failures)
-                vchtps.append(vchtp)
-
-        _get_crypttext_hash_tree_started = time.time()
-        if self._status:
-            self._status.set_status("Retrieving crypttext hash tree")
-
-        vto = ValidatedThingObtainer(vchtps, debugname="vchtps",
-                                     log_id=self._parentmsgid)
-        d = vto.start()
-
-        def _got_crypttext_hash_tree(res):
-            # Good -- the self._crypttext_hash_tree that we passed to vchtp
-            # is now populated with hashes.
-            if self._results:
-                elapsed = time.time() - _get_crypttext_hash_tree_started
-                self._results.timings["hashtrees"] = elapsed
-        d.addCallback(_got_crypttext_hash_tree)
-        return d
-
-    def _activate_enough_buckets(self):
-        """either return a mapping from shnum to a ValidatedReadBucketProxy
-        that can provide data for that share, or raise NotEnoughSharesError"""
-
-        while len(self.active_buckets) < self._verifycap.needed_shares:
-            # need some more
-            handled_shnums = set(self.active_buckets.keys())
-            available_shnums = set(self._share_vbuckets.keys())
-            potential_shnums = list(available_shnums - handled_shnums)
-            if len(potential_shnums) < (self._verifycap.needed_shares
-                                        - len(self.active_buckets)):
-                have = len(potential_shnums) + len(self.active_buckets)
-                msg = "Unable to activate enough shares: have %d, need %d" \
-                      % (have, self._verifycap.needed_shares)
-                if have:
-                    raise NotEnoughSharesError(msg)
-                else:
-                    raise NoSharesError(msg)
-            # For the next share, choose a primary share if available, else a
-            # randomly chosen secondary share.
-            potential_shnums.sort()
-            if potential_shnums[0] < self._verifycap.needed_shares:
-                shnum = potential_shnums[0]
-            else:
-                shnum = random.choice(potential_shnums)
-            # and a random bucket that will provide it
-            validated_bucket = random.choice(list(self._share_vbuckets[shnum]))
-            self.active_buckets[shnum] = validated_bucket
-        return self.active_buckets
-
-
-    def _download_all_segments(self, res):
-        # From now on if new buckets are received then I will notice that
-        # self._share_vbuckets is not None and generate a vbucket for that new
-        # bucket and add it in to _share_vbuckets. (We had to wait because we
-        # didn't have self._vup and self._share_hash_tree earlier. We didn't
-        # need validated buckets until now -- now that we are ready to download
-        # shares.)
-        self._share_vbuckets = {}
-        for sharenum, buckets in self._share_buckets.iteritems():
-            for bucket in buckets:
-                vbucket = ValidatedReadBucketProxy(sharenum, bucket, self._share_hash_tree, self._vup.num_segments, self._vup.block_size, self._vup.share_size)
-                self._share_vbuckets.setdefault(sharenum, set()).add(vbucket)
-
-        # after the above code, self._share_vbuckets contains enough
-        # buckets to complete the download, and some extra ones to
-        # tolerate some buckets dropping out or having
-        # errors. self._share_vbuckets is a dictionary that maps from
-        # shnum to a set of ValidatedBuckets, which themselves are
-        # wrappers around RIBucketReader references.
-        self.active_buckets = {} # k: shnum, v: ValidatedReadBucketProxy instance
-
-        self._started_fetching = time.time()
-
-        d = defer.succeed(None)
-        for segnum in range(self._vup.num_segments):
-            d.addCallback(self._download_segment, segnum)
-            # this pause, at the end of write, prevents pre-fetch from
-            # happening until the consumer is ready for more data.
-            d.addCallback(self._check_for_pause)
-
-        self._stage_4_d.callback(None)
-        return d
-
-    def _check_for_pause(self, res):
-        if self._paused:
-            d = defer.Deferred()
-            self._paused.addCallback(lambda ignored: d.callback(res))
-            return d
-        if self._stopped:
-            raise DownloadStopped("our Consumer called stopProducing()")
-        self._monitor.raise_if_cancelled()
-        return res
-
-    def _download_segment(self, res, segnum):
-        if self._status:
-            self._status.set_status("Downloading segment %d of %d" %
-                                    (segnum+1, self._vup.num_segments))
-        self.log("downloading seg#%d of %d (%d%%)"
-                 % (segnum, self._vup.num_segments,
-                    100.0 * segnum / self._vup.num_segments))
-        # memory footprint: when the SegmentDownloader finishes pulling down
-        # all shares, we have 1*segment_size of usage.
-        segmentdler = SegmentDownloader(self, segnum,
-                                        self._verifycap.needed_shares,
-                                        self._results)
-        started = time.time()
-        d = segmentdler.start()
-        def _finished_fetching(res):
-            elapsed = time.time() - started
-            self._results.timings["cumulative_fetch"] += elapsed
-            return res
-        if self._results:
-            d.addCallback(_finished_fetching)
-        # pause before using more memory
-        d.addCallback(self._check_for_pause)
-        # while the codec does its job, we hit 2*segment_size
-        def _started_decode(res):
-            self._started_decode = time.time()
-            return res
-        if self._results:
-            d.addCallback(_started_decode)
-        if segnum + 1 == self._vup.num_segments:
-            codec = self._tail_codec
-        else:
-            codec = self._codec
-        d.addCallback(lambda (shares, shareids): codec.decode(shares, shareids))
-        # once the codec is done, we drop back to 1*segment_size, because
-        # 'shares' goes out of scope. The memory usage is all in the
-        # plaintext now, spread out into a bunch of tiny buffers.
-        def _finished_decode(res):
-            elapsed = time.time() - self._started_decode
-            self._results.timings["cumulative_decode"] += elapsed
-            return res
-        if self._results:
-            d.addCallback(_finished_decode)
-
-        # pause/check-for-stop just before writing, to honor stopProducing
-        d.addCallback(self._check_for_pause)
-        d.addCallback(self._got_segment)
-        return d
-
-    def _got_segment(self, buffers):
-        precondition(self._crypttext_hash_tree)
-        started_decrypt = time.time()
-        self._status.set_progress(float(self._current_segnum)/self._verifycap.size)
-
-        if self._current_segnum + 1 == self._vup.num_segments:
-            # This is the last segment.
-            # Trim off any padding added by the upload side. We never send
-            # empty segments. If the data was an exact multiple of the
-            # segment size, the last segment will be full.
-            tail_buf_size = mathutil.div_ceil(self._vup.tail_segment_size, self._verifycap.needed_shares)
-            num_buffers_used = mathutil.div_ceil(self._vup.tail_data_size, tail_buf_size)
-            # Remove buffers which don't contain any part of the tail.
-            del buffers[num_buffers_used:]
-            # Remove the past-the-tail-part of the last buffer.
-            tail_in_last_buf = self._vup.tail_data_size % tail_buf_size
-            if tail_in_last_buf == 0:
-                tail_in_last_buf = tail_buf_size
-            buffers[-1] = buffers[-1][:tail_in_last_buf]
-
-        # First compute the hash of this segment and check that it fits.
-        ch = hashutil.crypttext_segment_hasher()
-        for buffer in buffers:
-            self._ciphertext_hasher.update(buffer)
-            ch.update(buffer)
-        self._crypttext_hash_tree.set_hashes(leaves={self._current_segnum: ch.digest()})
-
-        # Then write this segment to the target.
-        if not self._opened:
-            self._opened = True
-            self._target.open(self._verifycap.size)
-
-        for buffer in buffers:
-            self._target.write(buffer)
-            self._bytes_done += len(buffer)
-
-        self._status.set_progress(float(self._bytes_done)/self._verifycap.size)
-        self._current_segnum += 1
-
-        if self._results:
-            elapsed = time.time() - started_decrypt
-            self._results.timings["cumulative_decrypt"] += elapsed
-
-    def _done(self, res):
-        self.log("download done")
-        if self._results:
-            now = time.time()
-            self._results.timings["total"] = now - self._started
-            self._results.timings["segments"] = now - self._started_fetching
-        if self._vup.crypttext_hash:
-            _assert(self._vup.crypttext_hash == self._ciphertext_hasher.digest(),
-                    "bad crypttext_hash: computed=%s, expected=%s" %
-                    (base32.b2a(self._ciphertext_hasher.digest()),
-                     base32.b2a(self._vup.crypttext_hash)))
-        _assert(self._bytes_done == self._verifycap.size, self._bytes_done, self._verifycap.size)
-        self._status.set_progress(1)
-        self._target.close()
-        return self._target.finish()
-    def get_download_status(self):
-        return self._status
-
-
-class ConsumerAdapter:
-    implements(IDownloadTarget, IConsumer)
-    def __init__(self, consumer):
-        self._consumer = consumer
-
-    def registerProducer(self, producer, streaming):
-        self._consumer.registerProducer(producer, streaming)
-    def unregisterProducer(self):
-        self._consumer.unregisterProducer()
-
-    def open(self, size):
-        pass
-    def write(self, data):
-        self._consumer.write(data)
-    def close(self):
-        pass
-
-    def fail(self, why):
-        pass
-    def register_canceller(self, cb):
-        pass
-    def finish(self):
-        return self._consumer
-    # The following methods are just because the target might be a
-    # repairer.DownUpConnector, and just because the current CHKUpload object
-    # expects to find the storage index and encoding parameters in its
-    # Uploadable.
-    def set_storageindex(self, storageindex):
-        pass
-    def set_encodingparams(self, encodingparams):
-        pass
-
-
-class Downloader:
-    """I am a service that allows file downloading.
-    """
-    # TODO: in fact, this service only downloads immutable files (URI:CHK:).
-    # It is scheduled to go away, to be replaced by filenode.download()
-    implements(IDownloader)
-
-    def __init__(self, storage_broker, stats_provider):
-        self.storage_broker = storage_broker
-        self.stats_provider = stats_provider
-        self._all_downloads = weakref.WeakKeyDictionary() # for debugging
-
-    def download(self, u, t, _log_msg_id=None, monitor=None, history=None):
-        assert isinstance(u, uri.CHKFileURI)
-        t = IDownloadTarget(t)
-        assert t.write
-        assert t.close
-
-        if self.stats_provider:
-            # these counters are meant for network traffic, and don't
-            # include LIT files
-            self.stats_provider.count('downloader.files_downloaded', 1)
-            self.stats_provider.count('downloader.bytes_downloaded', u.get_size())
-
-        target = DecryptingTarget(t, u.key, _log_msg_id=_log_msg_id)
-        if not monitor:
-            monitor=Monitor()
-        dl = CiphertextDownloader(self.storage_broker,
-                                  u.get_verify_cap(), target,
-                                  monitor=monitor)
-        self._all_downloads[dl] = None
-        if history:
-            history.add_download(dl.get_download_status())
-        d = dl.start()
-        return d
diff --git a/src/allmydata/immutable/downloader/finder.py b/src/allmydata/immutable/downloader/finder.py
index 7cefefa..4be3db3 100644
--- a/src/allmydata/immutable/downloader/finder.py
+++ b/src/allmydata/immutable/downloader/finder.py
@@ -152,29 +152,34 @@ class ShareFinder:
         else:
             best_numsegs = self.node.num_segments
         for shnum, bucket in buckets.iteritems():
-            if shnum in self._commonshares:
-                cs = self._commonshares[shnum]
-            else:
-                cs = CommonShare(best_numsegs, self._si_prefix, shnum,
-                                 self._node_logparent)
-                # Share._get_satisfaction is responsible for updating
-                # CommonShare.set_numsegs after we know the UEB. Alternatives:
-                #  1: d = self.node.get_num_segments()
-                #     d.addCallback(cs.got_numsegs)
-                #   the problem is that the OneShotObserverList I was using
-                #   inserts an eventual-send between _get_satisfaction's
-                #   _satisfy_UEB and _satisfy_block_hash_tree, and the
-                #   CommonShare didn't get the num_segs message before
-                #   being asked to set block hash values. To resolve this
-                #   would require an immediate ObserverList instead of
-                #   an eventual-send -based one
-                #  2: break _get_satisfaction into Deferred-attached pieces.
-                #     Yuck.
-                self._commonshares[shnum] = cs
-            s = Share(bucket, server_version, self.verifycap, cs, self.node,
-                      self._download_status, peerid, shnum,
-                      self._node_logparent)
-            self.undelivered_shares.append(s)
+            self._create_share(best_numsegs, shnum, bucket, server_version,
+                               peerid)
+
+    def _create_share(self, best_numsegs, shnum, bucket, server_version,
+                      peerid):
+        if shnum in self._commonshares:
+            cs = self._commonshares[shnum]
+        else:
+            cs = CommonShare(best_numsegs, self._si_prefix, shnum,
+                             self._node_logparent)
+            # Share._get_satisfaction is responsible for updating
+            # CommonShare.set_numsegs after we know the UEB. Alternatives:
+            #  1: d = self.node.get_num_segments()
+            #     d.addCallback(cs.got_numsegs)
+            #   the problem is that the OneShotObserverList I was using
+            #   inserts an eventual-send between _get_satisfaction's
+            #   _satisfy_UEB and _satisfy_block_hash_tree, and the
+            #   CommonShare didn't get the num_segs message before
+            #   being asked to set block hash values. To resolve this
+            #   would require an immediate ObserverList instead of
+            #   an eventual-send -based one
+            #  2: break _get_satisfaction into Deferred-attached pieces.
+            #     Yuck.
+            self._commonshares[shnum] = cs
+        s = Share(bucket, server_version, self.verifycap, cs, self.node,
+                  self._download_status, peerid, shnum,
+                  self._node_logparent)
+        self.undelivered_shares.append(s)
 
     def _got_error(self, f, peerid, req, d_ev, lp):
         d_ev.finished("error", now())
diff --git a/src/allmydata/test/test_encode.py b/src/allmydata/test/test_encode.py
index 1108e18..c6ece2a 100644
--- a/src/allmydata/test/test_encode.py
+++ b/src/allmydata/test/test_encode.py
@@ -2,9 +2,11 @@ from zope.interface import implements
 from twisted.trial import unittest
 from twisted.internet import defer, reactor
 from twisted.python.failure import Failure
-from foolscap.api import fireEventually
+from foolscap.api import eventually, fireEventually
 from allmydata import hashtree, uri
-from allmydata.immutable import encode, upload, download
+from allmydata.immutable import encode, upload, checker, filenode
+from allmydata.immutable.downloader.share import Share
+#from allmydata.immutable.downloader.util import Observer2
 from allmydata.util import hashutil
 from allmydata.util.assertutil import _assert
 from allmydata.util.consumer import MemoryConsumer
@@ -21,6 +23,35 @@ def flip_bit(good): # flips the last bit
 
 class FakeStorageBroker:
     implements(IStorageBroker)
+    def get_servers_for_index(self, si):
+        return []
+
+class FakeShareFinder:
+    def __init__(self, node, shares):
+        self.node = node
+        self.shares = shares
+    def stop(self):
+        pass
+    def hungry(self):
+        def _deliver():
+            self.node.got_shares(set([self.shares.pop(0)]))
+        eventually(_deliver)
+class FakeShare(Share):
+    def __init__(self, data, mode, rref, server_version, verifycap,
+                 commonshare, node, download_status, peerid, shnum, logparent):
+        self._shnum = shnum
+        self._peerid = "peerid"
+        self._peerid_s = "peerid"
+        self.mode = mode
+        Share.__init__(self, stuff..)
+    def _send_request(self, start, length):
+        return fireEventually(self.data[start:start+length])
+
+    def OFFget_block(self, segnum):
+        NotImplementedError
+        o = Observer2()
+        o.notify(state=COMPLETE, block=block)
+        return o
 
 class FakeBucketReaderWriterProxy:
     implements(IStorageBucketWriter, IStorageBucketReader)
@@ -223,7 +254,7 @@ class ValidatedExtendedURIProxy(unittest.TestCase):
         fb = FakeBucketReaderWriterProxy()
         fb.put_uri_extension(uebstring)
         verifycap = uri.CHKFileVerifierURI(storage_index='x'*16, uri_extension_hash=uebhash, needed_shares=self.K, total_shares=self.M, size=self.SIZE)
-        vup = download.ValidatedExtendedURIProxy(fb, verifycap)
+        vup = checker.ValidatedExtendedURIProxy(fb, verifycap)
         return vup.start()
 
     def _test_accept(self, uebdict):
@@ -237,7 +268,7 @@ class ValidatedExtendedURIProxy(unittest.TestCase):
 
     def _test_reject(self, uebdict):
         d = self._test(uebdict)
-        d.addBoth(self._should_fail, (KeyError, download.BadURIExtension))
+        d.addBoth(self._should_fail, (KeyError, checker.BadURIExtension))
         return d
 
     def test_accept_minimal(self):
@@ -415,7 +446,7 @@ class StoppingConsumer(PausingConsumer):
     def write(self, data):
         self.producer.stopProducing()
 
-class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
+class Roundtrip_OFF(unittest.TestCase, testutil.ShouldFailMixin):
     timeout = 2400 # It takes longer than 240 seconds on Zandr's ARM box.
     def send_and_recover(self, k_and_happy_and_n=(25,75,100),
                          AVAILABLE_SHARES=None,
@@ -431,12 +462,13 @@ class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
         d = self.send(k_and_happy_and_n, AVAILABLE_SHARES,
                       max_segment_size, bucket_modes, data)
         # that fires with (uri_extension_hash, e, shareholders)
-        d.addCallback(self.recover, AVAILABLE_SHARES, recover_mode,
+        d.addCallback(self.recover, AVAILABLE_SHARES,
+                      bucket_modes, recover_mode,
                       consumer=consumer)
         # that fires with newdata
-        def _downloaded((newdata, fd)):
+        def _downloaded(newdata):
             self.failUnless(newdata == data, str((len(newdata), len(data))))
-            return fd
+            return None
         d.addCallback(_downloaded)
         return d
 
@@ -477,7 +509,7 @@ class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
         return d
 
     def recover(self, (res, key, shareholders), AVAILABLE_SHARES,
-                recover_mode, consumer=None):
+                bucket_modes, recover_mode, consumer=None):
         verifycap = res
 
         if "corrupt_key" in recover_mode:
@@ -493,60 +525,75 @@ class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
                            total_shares=verifycap.total_shares,
                            size=verifycap.size)
 
+        # we use a special StorageBroker to get more control over the
+        # download process: use a subset of the real servers, skip
+        # permuted-peerlist (to control which server is used first?), ..
         sb = FakeStorageBroker()
         if not consumer:
             consumer = MemoryConsumer()
-        innertarget = download.ConsumerAdapter(consumer)
-        target = download.DecryptingTarget(innertarget, u.key)
-        fd = download.CiphertextDownloader(sb, u.get_verify_cap(), target, monitor=Monitor())
-
-        # we manually cycle the CiphertextDownloader through a number of steps that
-        # would normally be sequenced by a Deferred chain in
-        # CiphertextDownloader.start(), to give us more control over the process.
-        # In particular, by bypassing _get_all_shareholders, we skip
-        # permuted-peerlist selection.
-        for shnum, bucket in shareholders.items():
-            if shnum < AVAILABLE_SHARES and bucket.closed:
-                fd.add_share_bucket(shnum, bucket)
-        fd._got_all_shareholders(None)
+        secret_holder = None; terminator = None; history = None
+        fn = filenode.ImmutableFileNode(u, sb, secret_holder,
+                                        terminator, history)
+
+        # populate our storagebroker with .. something?
+        #for shnum, bucket in shareholders.items():
+        #    if shnum < AVAILABLE_SHARES and bucket.closed:
+        #        fd.add_share_bucket(shnum, bucket)
+        #fd._got_all_shareholders(None)
+
+        # need to create replacement for FakeBucketReaderWriterProxy, since
+        # new-downloader doesn't use readerproxies
+        f = fn._cnode._node._sharefinder
+        # fille the ShareFinder with our doctored shares, so it will never
+        # ask the StorageBroker for servers
+        for shnum in range(AVAILABLE_SHARES):
+            bucket = 234
+            best_numsegs = 1 # good enough
+            server_version = NativeStorageClientDescriptor.VERSION_DEFAULTS
+            ...
+            f._create_share(best_numsegs, shnum, bucket, server_version,
+                            "peerid")
+            s = FakeShare(shnum, bucket_modes.get(shnum, "good"))
+            f.undelivered_shares.append(s)
 
         # Make it possible to obtain uri_extension from the shareholders.
         # Arrange for shareholders[0] to be the first, so we can selectively
         # corrupt the data it returns.
-        uri_extension_sources = shareholders.values()
-        uri_extension_sources.remove(shareholders[0])
-        uri_extension_sources.insert(0, shareholders[0])
-
-        d = defer.succeed(None)
-
-        # have the CiphertextDownloader retrieve a copy of uri_extension itself
-        d.addCallback(fd._obtain_uri_extension)
-
-        if "corrupt_crypttext_hashes" in recover_mode:
-            # replace everybody's crypttext hash trees with a different one
-            # (computed over a different file), then modify our uri_extension
-            # to reflect the new crypttext hash tree root
-            def _corrupt_crypttext_hashes(unused):
-                assert isinstance(fd._vup, download.ValidatedExtendedURIProxy), fd._vup
-                assert fd._vup.crypttext_root_hash, fd._vup
-                badhash = hashutil.tagged_hash("bogus", "data")
-                bad_crypttext_hashes = [badhash] * fd._vup.num_segments
-                badtree = hashtree.HashTree(bad_crypttext_hashes)
-                for bucket in shareholders.values():
-                    bucket.crypttext_hashes = list(badtree)
-                fd._crypttext_hash_tree = hashtree.IncompleteHashTree(fd._vup.num_segments)
-                fd._crypttext_hash_tree.set_hashes({0: badtree[0]})
-                return fd._vup
-            d.addCallback(_corrupt_crypttext_hashes)
-
-        # also have the CiphertextDownloader ask for hash trees
-        d.addCallback(fd._get_crypttext_hash_tree)
-
-        d.addCallback(fd._download_all_segments)
-        d.addCallback(fd._done)
+        #uri_extension_sources = shareholders.values()
+        #uri_extension_sources.remove(shareholders[0])
+        #uri_extension_sources.insert(0, shareholders[0])
+
+        d = fn.read(consumer)
+
+        if 0:
+            # have the CiphertextDownloader retrieve a copy of uri_extension itself
+            d.addCallback(fd._obtain_uri_extension)
+
+            if "corrupt_crypttext_hashes" in recover_mode:
+                # replace everybody's crypttext hash trees with a different one
+                # (computed over a different file), then modify our uri_extension
+                # to reflect the new crypttext hash tree root
+                def _corrupt_crypttext_hashes(unused):
+                    assert isinstance(fd._vup, checker.ValidatedExtendedURIProxy), fd._vup
+                    assert fd._vup.crypttext_root_hash, fd._vup
+                    badhash = hashutil.tagged_hash("bogus", "data")
+                    bad_crypttext_hashes = [badhash] * fd._vup.num_segments
+                    badtree = hashtree.HashTree(bad_crypttext_hashes)
+                    for bucket in shareholders.values():
+                        bucket.crypttext_hashes = list(badtree)
+                    fd._crypttext_hash_tree = hashtree.IncompleteHashTree(fd._vup.num_segments)
+                    fd._crypttext_hash_tree.set_hashes({0: badtree[0]})
+                    return fd._vup
+                d.addCallback(_corrupt_crypttext_hashes)
+
+            # also have the CiphertextDownloader ask for hash trees
+            d.addCallback(fd._get_crypttext_hash_tree)
+
+            d.addCallback(fd._download_all_segments)
+            d.addCallback(fd._done)
         def _done(t):
             newdata = "".join(consumer.chunks)
-            return (newdata, fd)
+            return newdata
         d.addCallback(_done)
         return d
 
@@ -592,7 +639,7 @@ class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
     def test_pause_then_stop(self):
         # use a download target that pauses, then stops.
         c = PausingAndStoppingConsumer()
-        d = self.shouldFail(download.DownloadStopped, "test_pause_then_stop",
+        d = self.shouldFail(DownloadStopped, "test_pause_then_stop",
                             "our Consumer called stopProducing()",
                             self.send_and_recover, consumer=c)
         return d
@@ -600,7 +647,7 @@ class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
     def test_stop(self):
         # use a download targetthat does an immediate stop (ticket #473)
         c = StoppingConsumer()
-        d = self.shouldFail(download.DownloadStopped, "test_stop",
+        d = self.shouldFail(DownloadStopped, "test_stop",
                             "our Consumer called stopProducing()",
                             self.send_and_recover, consumer=c)
         return d
@@ -806,3 +853,67 @@ class Roundtrip(unittest.TestCase, testutil.ShouldFailMixin):
             self.failUnless(res.check(UploadUnhappinessError))
         d.addBoth(_done)
         return d
+
+class Roundtrip(GridTestMixin, unittest.TestCase):
+    def test_74(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        u = upload.Data("p"*74, None))
+        u.max_segment_size = 25
+        u.???.encoding = (25,75,100)
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            return download_to_data(n)
+        d.addCallback(_uploaded)
+        def _downloaded(newdata):
+            self.failUnlessEqual(newdata, "p"*74)
+        d.addCallback(_downloaded)
+        return d
+
+
+"""
+NoNetworkGrid:
+    def test_good(self):
+    def test_one_share_per_peer(self):
+    def test_74(self):
+    def test_75(self):
+    def test_51(self):
+    def test_99(self):
+    def test_100(self):
+    def test_76(self):
+    def test_124(self):
+    def test_125(self):
+    def test_101(self):
+    def test_pause(self):
+    def test_pause_then_stop(self):
+    def test_stop(self):
+
+NoNetworkGrid and delete shares
+    def test_not_enough_shares(self):
+NoNetworkGrid and corrupt shares
+ (need to sequence retrievals?)
+ covered by test_download.Corruption.test_each_byte
+    def test_bad_blocks(self):
+    def test_bad_blocks_failure(self):
+    def test_bad_blockhashes(self):
+    def test_bad_blockhashes_failure(self):
+    def test_bad_sharehashes(self):
+    def test_bad_uri_extension(self):
+    def test_bad_crypttext_hashroot(self):
+    def test_bad_crypttext_hashes(self):
+    def test_bad_crypttext_hashes_failure(self):
+    def test_bad_sharehashes_failure(self):
+    def test_missing_sharehashes(self):
+    def test_missing_sharehashes_failure(self):
+
+NoNetworkGrid, upload part of ciphertext, kill server, continue upload
+    def test_lost_one_shareholder(self): # these are upload-side tests
+    def test_lost_one_shareholder_early(self):
+    def test_lost_many_shareholders(self):
+    def test_lost_all_shareholders(self):
+
+need to add to test_downloader
+ [read(seg0), kill server, read(seg1)]
+"""
diff --git a/src/allmydata/test/test_web.py b/src/allmydata/test/test_web.py
index 679fb14..6c8b688 100644
--- a/src/allmydata/test/test_web.py
+++ b/src/allmydata/test/test_web.py
@@ -11,7 +11,8 @@ from nevow import rend
 from allmydata import interfaces, uri, webish, dirnode
 from allmydata.storage.shares import get_share_file
 from allmydata.storage_client import StorageFarmBroker
-from allmydata.immutable import upload, download
+from allmydata.immutable import upload
+from allmydata.immutable.downloader.status import DownloadStatus
 from allmydata.dirnode import DirectoryNode
 from allmydata.nodemaker import NodeMaker
 from allmydata.unknown import UnknownNode
@@ -73,7 +74,7 @@ class FakeUploader(service.Service):
 
 class FakeHistory:
     _all_upload_status = [upload.UploadStatus()]
-    _all_download_status = [download.DownloadStatus()]
+    _all_download_status = [DownloadStatus("storage_index", 1234)]
     _all_mapupdate_statuses = [servermap.UpdateStatus()]
     _all_publish_statuses = [publish.PublishStatus()]
     _all_retrieve_statuses = [retrieve.RetrieveStatus()]

commit deb72375fcff3e3d1a602acf3ee30cd5551742f8
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 26 12:00:34 2010 -0700

    remove unused import
---
 src/allmydata/test/test_repairer.py |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/test/test_repairer.py b/src/allmydata/test/test_repairer.py
index 1f5a3a7..26cc8bb 100644
--- a/src/allmydata/test/test_repairer.py
+++ b/src/allmydata/test/test_repairer.py
@@ -3,7 +3,7 @@ from allmydata.test import common
 from allmydata.monitor import Monitor
 from allmydata import check_results
 from allmydata.interfaces import NotEnoughSharesError
-from allmydata.immutable import repairer, upload
+from allmydata.immutable import upload
 from allmydata.util.consumer import download_to_data
 from twisted.internet import defer
 from twisted.trial import unittest

commit 739dece4d177bb459b67b206073b7dfa31208de9
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 26 11:56:48 2010 -0700

    rewrite Repairer with new downloader
    
    Added CiphertextFileNode.get_segment_size() so repairer can get it before
    upload starts. Not very efficient yet (downloads all of seg0 when it really
    only needs a copy of the UEB). Still working on removing (or at least
    shrinking) download.py .
---
 src/allmydata/immutable/downloader/node.py |   20 +++-
 src/allmydata/immutable/filenode.py        |   15 ++-
 src/allmydata/immutable/repairer.py        |  211 ++++-----------------------
 src/allmydata/test/test_repairer.py        |   93 ------------
 4 files changed, 64 insertions(+), 275 deletions(-)

diff --git a/src/allmydata/immutable/downloader/node.py b/src/allmydata/immutable/downloader/node.py
index 9d97e9b..2991c9e 100644
--- a/src/allmydata/immutable/downloader/node.py
+++ b/src/allmydata/immutable/downloader/node.py
@@ -6,7 +6,7 @@ from twisted.internet import defer
 from foolscap.api import eventually
 from allmydata import uri
 from allmydata.codec import CRSDecoder
-from allmydata.util import base32, log, hashutil, mathutil
+from allmydata.util import base32, log, hashutil, mathutil, observer
 from allmydata.interfaces import DEFAULT_MAX_SEGMENT_SIZE
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
@@ -75,6 +75,8 @@ class DownloadNode:
         self._segment_requests = [] # (segnum, d, cancel_handle)
         self._active_segment = None # a SegmentFetcher, with .segnum
 
+        self._segsize_observers = observer.OneShotObserverList()
+
         # we create one top-level logparent for this _Node, and another one
         # for each read() call. Segmentation and get_segment() messages are
         # associated with the read() call, everything else is tied to the
@@ -184,6 +186,21 @@ class DownloadNode:
         self._start_new_segment()
         return (d, c)
 
+    def get_segsize(self):
+        """Return a Deferred that fires when we know the real segment size."""
+        if self.segment_size:
+            return defer.succeed(self.segment_size)
+        # TODO: this downloads (and discards) the first segment of the file.
+        # We could make this more efficient by writing
+        # fetcher.SegmentSizeFetcher, with the job of finding a single valid
+        # share and extracting the UEB. We'd add Share.get_UEB() to request
+        # just the UEB.
+        (d,c) = self.get_segment(0)
+        # this ensures that an error during get_segment() will errback the
+        # caller, so Repair won't wait forever on completely missing files
+        d.addCallback(lambda ign: self._segsize_observers.when_fired())
+        return d
+
     # things called by the Segmentation object used to transform
     # arbitrary-sized read() calls into quantized segment fetches
 
@@ -242,6 +259,7 @@ class DownloadNode:
         k, N = self._verifycap.needed_shares, self._verifycap.total_shares
 
         self.segment_size = d['segment_size']
+        self._segsize_observers.fire(self.segment_size)
 
         r = self._calculate_sizes(self.segment_size)
         self.tail_segment_size = r["tail_segment_size"]
diff --git a/src/allmydata/immutable/filenode.py b/src/allmydata/immutable/filenode.py
index ffc87fd..1d5be94 100644
--- a/src/allmydata/immutable/filenode.py
+++ b/src/allmydata/immutable/filenode.py
@@ -58,6 +58,17 @@ class CiphertextFileNode:
         """
         return self._node.get_segment(segnum)
 
+    def get_segment_size(self):
+        # return a Deferred that fires with the file's real segment size
+        return self._node.get_segsize()
+
+    def get_storage_index(self):
+        return self._verifycap.storage_index
+    def get_verify_cap(self):
+        return self._verifycap
+    def get_size(self):
+        return self._verifycap.size
+
     def raise_error(self):
         pass
 
@@ -112,8 +123,8 @@ class CiphertextFileNode:
                     crr.repair_successful = False
                     crr.repair_failure = f
                     return f
-                r = Repairer(storage_broker=sb, secret_holder=sh,
-                             verifycap=verifycap, monitor=monitor)
+                r = Repairer(self, storage_broker=sb, secret_holder=sh,
+                             monitor=monitor)
                 d = r.start()
                 d.addCallbacks(_gather_repair_results, _repair_error)
                 return d
diff --git a/src/allmydata/immutable/repairer.py b/src/allmydata/immutable/repairer.py
index fa6a604..64fb9a1 100644
--- a/src/allmydata/immutable/repairer.py
+++ b/src/allmydata/immutable/repairer.py
@@ -1,17 +1,14 @@
 from zope.interface import implements
 from twisted.internet import defer
 from allmydata.storage.server import si_b2a
-from allmydata.util import log, observer
-from allmydata.util.assertutil import precondition, _assert
-from allmydata.uri import CHKFileVerifierURI
-from allmydata.interfaces import IEncryptedUploadable, IDownloadTarget
-from twisted.internet.interfaces import IConsumer
+from allmydata.util import log, consumer
+from allmydata.util.assertutil import precondition
+from allmydata.interfaces import IEncryptedUploadable
 
-from allmydata.immutable import download, upload
-
-import collections
+from allmydata.immutable import upload
 
 class Repairer(log.PrefixingLogMixin):
+    implements(IEncryptedUploadable)
     """I generate any shares which were not available and upload them to
     servers.
 
@@ -43,195 +40,51 @@ class Repairer(log.PrefixingLogMixin):
     cancelled (by invoking its raise_if_cancelled() method).
     """
 
-    def __init__(self, storage_broker, secret_holder, verifycap, monitor):
-        assert precondition(isinstance(verifycap, CHKFileVerifierURI))
-
-        logprefix = si_b2a(verifycap.get_storage_index())[:5]
+    def __init__(self, filenode, storage_broker, secret_holder, monitor):
+        logprefix = si_b2a(filenode.get_storage_index())[:5]
         log.PrefixingLogMixin.__init__(self, "allmydata.immutable.repairer",
                                        prefix=logprefix)
-
+        self._filenode = filenode
         self._storage_broker = storage_broker
         self._secret_holder = secret_holder
-        self._verifycap = verifycap
         self._monitor = monitor
+        self._offset = 0
 
     def start(self):
         self.log("starting repair")
-        duc = DownUpConnector()
-        dl = download.CiphertextDownloader(self._storage_broker,
-                                           self._verifycap, target=duc,
-                                           monitor=self._monitor)
-        ul = upload.CHKUploader(self._storage_broker, self._secret_holder)
-
-        d = defer.Deferred()
-
-        # If the upload or the download fails or is stopped, then the repair
-        # failed.
-        def _errb(f):
-            d.errback(f)
-            return None
-
-        # If the upload succeeds, then the repair has succeeded.
-        def _cb(res):
-            d.callback(res)
-        ul.start(duc).addCallbacks(_cb, _errb)
-
-        # If the download fails or is stopped, then the repair failed.
-        d2 = dl.start()
-        d2.addErrback(_errb)
-
-        # We ignore the callback from d2.  Is this right?  Ugh.
-
+        d = self._filenode.get_segment_size()
+        def _got_segsize(segsize):
+            vcap = self._filenode.get_verify_cap()
+            k = vcap.needed_shares
+            N = vcap.total_shares
+            happy = upload.BaseUploadable.default_encoding_param_happy
+            self._encodingparams = (k, happy, N, segsize)
+            ul = upload.CHKUploader(self._storage_broker, self._secret_holder)
+            return ul.start(self) # I am the IEncryptedUploadable
+        d.addCallback(_got_segsize)
         return d
 
-class DownUpConnector(log.PrefixingLogMixin):
-    implements(IEncryptedUploadable, IDownloadTarget, IConsumer)
-    """I act like an 'encrypted uploadable' -- something that a local
-    uploader can read ciphertext from in order to upload the ciphertext.
-    However, unbeknownst to the uploader, I actually download the ciphertext
-    from a CiphertextDownloader instance as it is needed.
-
-    On the other hand, I act like a 'download target' -- something that a
-    local downloader can write ciphertext to as it downloads the ciphertext.
-    That downloader doesn't realize, of course, that I'm just turning around
-    and giving the ciphertext to the uploader."""
-
-    # The theory behind this class is nice: just satisfy two separate
-    # interfaces. The implementation is slightly horrible, because of
-    # "impedance mismatch" -- the downloader expects to be able to
-    # synchronously push data in, and the uploader expects to be able to read
-    # data out with a "read(THIS_SPECIFIC_LENGTH)" which returns a deferred.
-    # The two interfaces have different APIs for pausing/unpausing. The
-    # uploader requests metadata like size and encodingparams which the
-    # downloader provides either eventually or not at all (okay I just now
-    # extended the downloader to provide encodingparams). Most of this
-    # slightly horrible code would disappear if CiphertextDownloader just
-    # used this object as an IConsumer (plus maybe a couple of other methods)
-    # and if the Uploader simply expected to be treated as an IConsumer (plus
-    # maybe a couple of other things).
-
-    def __init__(self, buflim=2**19):
-        """If we're already holding at least buflim bytes, then tell the
-        downloader to pause until we have less than buflim bytes."""
-        log.PrefixingLogMixin.__init__(self, "allmydata.immutable.repairer")
-        self.buflim = buflim
-        self.bufs = collections.deque() # list of strings
-        self.bufsiz = 0 # how many bytes total in bufs
-
-        # list of deferreds which will fire with the requested ciphertext
-        self.next_read_ds = collections.deque()
-
-        # how many bytes of ciphertext were requested by each deferred
-        self.next_read_lens = collections.deque()
-
-        self._size_osol = observer.OneShotObserverList()
-        self._encodingparams_osol = observer.OneShotObserverList()
-        self._storageindex_osol = observer.OneShotObserverList()
-        self._closed_to_pusher = False
-
-        # once seg size is available, the following attribute will be created
-        # to hold it:
-
-        # self.encodingparams # (provided by the object which is pushing data
-        # into me, required by the object which is pulling data out of me)
-
-        # open() will create the following attribute:
-        # self.size # size of the whole file (provided by the object which is
-        # pushing data into me, required by the object which is pulling data
-        # out of me)
-
-        # set_upload_status() will create the following attribute:
-
-        # self.upload_status # XXX do we need to actually update this? Is
-        # anybody watching the results during a repair?
-
-    def _satisfy_reads_if_possible(self):
-        assert bool(self.next_read_ds) == bool(self.next_read_lens)
-        while self.next_read_ds and ((self.bufsiz >= self.next_read_lens[0])
-                                     or self._closed_to_pusher):
-            nrd = self.next_read_ds.popleft()
-            nrl = self.next_read_lens.popleft()
-
-            # Pick out the requested number of bytes from self.bufs, turn it
-            # into a string, and callback the deferred with that.
-            res = []
-            ressize = 0
-            while ressize < nrl and self.bufs:
-                nextbuf = self.bufs.popleft()
-                res.append(nextbuf)
-                ressize += len(nextbuf)
-                if ressize > nrl:
-                    extra = ressize - nrl
-                    self.bufs.appendleft(nextbuf[:-extra])
-                    res[-1] = nextbuf[:-extra]
-            assert _assert(sum(len(x) for x in res) <= nrl, [len(x) for x in res], nrl)
-            assert _assert(sum(len(x) for x in res) == nrl or self._closed_to_pusher, [len(x) for x in res], nrl)
-            self.bufsiz -= nrl
-            if self.bufsiz < self.buflim and self.producer:
-                self.producer.resumeProducing()
-            nrd.callback(res)
-
-    # methods to satisfy the IConsumer and IDownloadTarget interfaces. (From
-    # the perspective of a downloader I am an IDownloadTarget and an
-    # IConsumer.)
-    def registerProducer(self, producer, streaming):
-        assert streaming # We know how to handle only streaming producers.
-        self.producer = producer # the downloader
-    def unregisterProducer(self):
-        self.producer = None
-    def open(self, size):
-        self.size = size
-        self._size_osol.fire(self.size)
-    def set_encodingparams(self, encodingparams):
-        self.encodingparams = encodingparams
-        self._encodingparams_osol.fire(self.encodingparams)
-    def set_storageindex(self, storageindex):
-        self.storageindex = storageindex
-        self._storageindex_osol.fire(self.storageindex)
-    def write(self, data):
-        precondition(data) # please don't write empty strings
-        self.bufs.append(data)
-        self.bufsiz += len(data)
-        self._satisfy_reads_if_possible()
-        if self.bufsiz >= self.buflim and self.producer:
-            self.producer.pauseProducing()
-    def finish(self):
-        pass
-    def close(self):
-        self._closed_to_pusher = True
-        # Any reads which haven't been satisfied by now are going to
-        # have to be satisfied with short reads.
-        self._satisfy_reads_if_possible()
 
     # methods to satisfy the IEncryptedUploader interface
     # (From the perspective of an uploader I am an IEncryptedUploadable.)
     def set_upload_status(self, upload_status):
         self.upload_status = upload_status
     def get_size(self):
-        if hasattr(self, 'size'): # attribute created by self.open()
-            return defer.succeed(self.size)
-        else:
-            return self._size_osol.when_fired()
+        size = self._filenode.get_size()
+        assert size is not None
+        return defer.succeed(size)
     def get_all_encoding_parameters(self):
-        # We have to learn the encoding params from pusher.
-        if hasattr(self, 'encodingparams'):
-            # attribute created by self.set_encodingparams()
-            return defer.succeed(self.encodingparams)
-        else:
-            return self._encodingparams_osol.when_fired()
+        return defer.succeed(self._encodingparams)
     def read_encrypted(self, length, hash_only):
-        """Returns a deferred which eventually fired with the requested
-        ciphertext."""
+        """Returns a deferred which eventually fires with the requested
+        ciphertext, as a list of strings."""
         precondition(length) # please don't ask to read 0 bytes
-        d = defer.Deferred()
-        self.next_read_ds.append(d)
-        self.next_read_lens.append(length)
-        self._satisfy_reads_if_possible()
+        mc = consumer.MemoryConsumer()
+        d = self._filenode.read(mc, self._offset, length)
+        self._offset += length
+        d.addCallback(lambda ign: mc.chunks)
         return d
     def get_storage_index(self):
-        # We have to learn the storage index from pusher.
-        if hasattr(self, 'storageindex'):
-            # attribute created by self.set_storageindex()
-            return defer.succeed(self.storageindex)
-        else:
-            return self._storageindex.when_fired()
+        return self._filenode.get_storage_index()
+    def close(self):
+        pass
diff --git a/src/allmydata/test/test_repairer.py b/src/allmydata/test/test_repairer.py
index a4ccf2c..1f5a3a7 100644
--- a/src/allmydata/test/test_repairer.py
+++ b/src/allmydata/test/test_repairer.py
@@ -363,99 +363,6 @@ WRITE_LEEWAY = 35
 # Optimally, you could repair one of these (small) files in a single write.
 DELTA_WRITES_PER_SHARE = 1 * WRITE_LEEWAY
 
-class DownUpConnector(unittest.TestCase):
-    def test_deferred_satisfaction(self):
-        duc = repairer.DownUpConnector()
-        duc.registerProducer(None, True) # just because you have to call registerProducer first
-        # case 1: total data in buf is < requested data at time of request
-        duc.write('\x01')
-        d = duc.read_encrypted(2, False)
-        def _then(data):
-            self.failUnlessEqual(len(data), 2)
-            self.failUnlessEqual(data[0], '\x01')
-            self.failUnlessEqual(data[1], '\x02')
-        d.addCallback(_then)
-        duc.write('\x02')
-        return d
-
-    def test_extra(self):
-        duc = repairer.DownUpConnector()
-        duc.registerProducer(None, True) # just because you have to call registerProducer first
-        # case 1: total data in buf is < requested data at time of request
-        duc.write('\x01')
-        d = duc.read_encrypted(2, False)
-        def _then(data):
-            self.failUnlessEqual(len(data), 2)
-            self.failUnlessEqual(data[0], '\x01')
-            self.failUnlessEqual(data[1], '\x02')
-        d.addCallback(_then)
-        duc.write('\x02\0x03')
-        return d
-
-    def test_short_reads_1(self):
-        # You don't get fewer bytes than you requested -- instead you get no callback at all.
-        duc = repairer.DownUpConnector()
-        duc.registerProducer(None, True) # just because you have to call registerProducer first
-
-        d = duc.read_encrypted(2, False)
-        duc.write('\x04')
-
-        def _callb(res):
-            self.fail("Shouldn't have gotten this callback res: %s" % (res,))
-        d.addCallback(_callb)
-
-        # Also in the other order of read-vs-write:
-        duc2 = repairer.DownUpConnector()
-        duc2.registerProducer(None, True) # just because you have to call registerProducer first
-        duc2.write('\x04')
-        d = duc2.read_encrypted(2, False)
-
-        def _callb2(res):
-            self.fail("Shouldn't have gotten this callback res: %s" % (res,))
-        d.addCallback(_callb2)
-
-        # But once the DUC is closed then you *do* get short reads.
-        duc3 = repairer.DownUpConnector()
-        duc3.registerProducer(None, True) # just because you have to call registerProducer first
-
-        d = duc3.read_encrypted(2, False)
-        duc3.write('\x04')
-        duc3.close()
-        def _callb3(res):
-            self.failUnlessEqual(len(res), 1)
-            self.failUnlessEqual(res[0], '\x04')
-        d.addCallback(_callb3)
-        return d
-
-    def test_short_reads_2(self):
-        # Also in the other order of read-vs-write.
-        duc = repairer.DownUpConnector()
-        duc.registerProducer(None, True) # just because you have to call registerProducer first
-
-        duc.write('\x04')
-        d = duc.read_encrypted(2, False)
-        duc.close()
-
-        def _callb(res):
-            self.failUnlessEqual(len(res), 1)
-            self.failUnlessEqual(res[0], '\x04')
-        d.addCallback(_callb)
-        return d
-
-    def test_short_reads_3(self):
-        # Also if it is closed before the read.
-        duc = repairer.DownUpConnector()
-        duc.registerProducer(None, True) # just because you have to call registerProducer first
-
-        duc.write('\x04')
-        duc.close()
-        d = duc.read_encrypted(2, False)
-        def _callb(res):
-            self.failUnlessEqual(len(res), 1)
-            self.failUnlessEqual(res[0], '\x04')
-        d.addCallback(_callb)
-        return d
-
 class Repairer(GridTestMixin, unittest.TestCase, RepairTestMixin,
                common.ShouldFailMixin):
 

commit eef581cde21a70680f6e617c4ae1b87e2759f559
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 26 09:37:39 2010 -0700

    move check/repair out of downloader/node.py into filenode.py
---
 src/allmydata/immutable/downloader/node.py |   77 +--------------------------
 src/allmydata/immutable/filenode.py        |   79 ++++++++++++++++++++++++++-
 2 files changed, 77 insertions(+), 79 deletions(-)

diff --git a/src/allmydata/immutable/downloader/node.py b/src/allmydata/immutable/downloader/node.py
index b18c6bc..9d97e9b 100644
--- a/src/allmydata/immutable/downloader/node.py
+++ b/src/allmydata/immutable/downloader/node.py
@@ -1,5 +1,4 @@
 
-import copy
 import time
 now = time.time
 from twisted.python.failure import Failure
@@ -7,14 +6,10 @@ from twisted.internet import defer
 from foolscap.api import eventually
 from allmydata import uri
 from allmydata.codec import CRSDecoder
-from allmydata.check_results import CheckResults, CheckAndRepairResults
 from allmydata.util import base32, log, hashutil, mathutil
-from allmydata.util.dictutil import DictOfSets
-from allmydata.interfaces import IUploadResults, DEFAULT_MAX_SEGMENT_SIZE
+from allmydata.interfaces import DEFAULT_MAX_SEGMENT_SIZE
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
-from allmydata.immutable.checker import Checker
-from allmydata.immutable.repairer import Repairer
 
 # local imports
 from finder import ShareFinder
@@ -456,73 +451,3 @@ class DownloadNode:
             self._active_segment.stop()
             self._active_segment = None
             self._start_new_segment()
-
-    def check_and_repair(self, monitor, verify=False, add_lease=False):
-        verifycap = self._verifycap
-        storage_index = verifycap.storage_index
-        sb = self._storage_broker
-        servers = sb.get_all_servers()
-        sh = self._secret_holder
-
-        c = Checker(verifycap=verifycap, servers=servers,
-                    verify=verify, add_lease=add_lease, secret_holder=sh,
-                    monitor=monitor)
-        d = c.start()
-        def _maybe_repair(cr):
-            crr = CheckAndRepairResults(storage_index)
-            crr.pre_repair_results = cr
-            if cr.is_healthy():
-                crr.post_repair_results = cr
-                return defer.succeed(crr)
-            else:
-                crr.repair_attempted = True
-                crr.repair_successful = False # until proven successful
-                def _gather_repair_results(ur):
-                    assert IUploadResults.providedBy(ur), ur
-                    # clone the cr (check results) to form the basis of the
-                    # prr (post-repair results)
-                    prr = CheckResults(cr.uri, cr.storage_index)
-                    prr.data = copy.deepcopy(cr.data)
-
-                    sm = prr.data['sharemap']
-                    assert isinstance(sm, DictOfSets), sm
-                    sm.update(ur.sharemap)
-                    servers_responding = set(prr.data['servers-responding'])
-                    servers_responding.union(ur.sharemap.iterkeys())
-                    prr.data['servers-responding'] = list(servers_responding)
-                    prr.data['count-shares-good'] = len(sm)
-                    prr.data['count-good-share-hosts'] = len(sm)
-                    is_healthy = bool(len(sm) >= verifycap.total_shares)
-                    is_recoverable = bool(len(sm) >= verifycap.needed_shares)
-                    prr.set_healthy(is_healthy)
-                    prr.set_recoverable(is_recoverable)
-                    crr.repair_successful = is_healthy
-                    prr.set_needs_rebalancing(len(sm) >= verifycap.total_shares)
-
-                    crr.post_repair_results = prr
-                    return crr
-                def _repair_error(f):
-                    # as with mutable repair, I'm not sure if I want to pass
-                    # through a failure or not. TODO
-                    crr.repair_successful = False
-                    crr.repair_failure = f
-                    return f
-                r = Repairer(storage_broker=sb, secret_holder=sh,
-                             verifycap=verifycap, monitor=monitor)
-                d = r.start()
-                d.addCallbacks(_gather_repair_results, _repair_error)
-                return d
-
-        d.addCallback(_maybe_repair)
-        return d
-
-    def check(self, monitor, verify=False, add_lease=False):
-        verifycap = self._verifycap
-        sb = self._storage_broker
-        servers = sb.get_all_servers()
-        sh = self._secret_holder
-
-        v = Checker(verifycap=verifycap, servers=servers,
-                    verify=verify, add_lease=add_lease, secret_holder=sh,
-                    monitor=monitor)
-        return v.start()
diff --git a/src/allmydata/immutable/filenode.py b/src/allmydata/immutable/filenode.py
index 85f1c66..ffc87fd 100644
--- a/src/allmydata/immutable/filenode.py
+++ b/src/allmydata/immutable/filenode.py
@@ -1,16 +1,21 @@
 
 import binascii
+import copy
 import time
 now = time.time
 from zope.interface import implements
 from twisted.internet import defer
 from twisted.internet.interfaces import IConsumer
 
-from allmydata.interfaces import IImmutableFileNode
+from allmydata.interfaces import IImmutableFileNode, IUploadResults
 from allmydata import uri
+from allmydata.check_results import CheckResults, CheckAndRepairResults
+from allmydata.util.dictutil import DictOfSets
 from pycryptopp.cipher.aes import AES
 
 # local imports
+from allmydata.immutable.checker import Checker
+from allmydata.immutable.repairer import Repairer
 from allmydata.immutable.downloader.node import DownloadNode
 from allmydata.immutable.downloader.status import DownloadStatus
 
@@ -18,6 +23,9 @@ class CiphertextFileNode:
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history, download_status=None):
         assert isinstance(verifycap, uri.CHKFileVerifierURI)
+        self._verifycap = verifycap
+        self._storage_broker = storage_broker
+        self._secret_holder = secret_holder
         if download_status is None:
             ds = DownloadStatus(verifycap.storage_index, verifycap.size)
             if history:
@@ -55,9 +63,74 @@ class CiphertextFileNode:
 
 
     def check_and_repair(self, monitor, verify=False, add_lease=False):
-        return self._node.check_and_repair(monitor, verify, add_lease)
+        verifycap = self._verifycap
+        storage_index = verifycap.storage_index
+        sb = self._storage_broker
+        servers = sb.get_all_servers()
+        sh = self._secret_holder
+
+        c = Checker(verifycap=verifycap, servers=servers,
+                    verify=verify, add_lease=add_lease, secret_holder=sh,
+                    monitor=monitor)
+        d = c.start()
+        def _maybe_repair(cr):
+            crr = CheckAndRepairResults(storage_index)
+            crr.pre_repair_results = cr
+            if cr.is_healthy():
+                crr.post_repair_results = cr
+                return defer.succeed(crr)
+            else:
+                crr.repair_attempted = True
+                crr.repair_successful = False # until proven successful
+                def _gather_repair_results(ur):
+                    assert IUploadResults.providedBy(ur), ur
+                    # clone the cr (check results) to form the basis of the
+                    # prr (post-repair results)
+                    prr = CheckResults(cr.uri, cr.storage_index)
+                    prr.data = copy.deepcopy(cr.data)
+
+                    sm = prr.data['sharemap']
+                    assert isinstance(sm, DictOfSets), sm
+                    sm.update(ur.sharemap)
+                    servers_responding = set(prr.data['servers-responding'])
+                    servers_responding.union(ur.sharemap.iterkeys())
+                    prr.data['servers-responding'] = list(servers_responding)
+                    prr.data['count-shares-good'] = len(sm)
+                    prr.data['count-good-share-hosts'] = len(sm)
+                    is_healthy = bool(len(sm) >= verifycap.total_shares)
+                    is_recoverable = bool(len(sm) >= verifycap.needed_shares)
+                    prr.set_healthy(is_healthy)
+                    prr.set_recoverable(is_recoverable)
+                    crr.repair_successful = is_healthy
+                    prr.set_needs_rebalancing(len(sm) >= verifycap.total_shares)
+
+                    crr.post_repair_results = prr
+                    return crr
+                def _repair_error(f):
+                    # as with mutable repair, I'm not sure if I want to pass
+                    # through a failure or not. TODO
+                    crr.repair_successful = False
+                    crr.repair_failure = f
+                    return f
+                r = Repairer(storage_broker=sb, secret_holder=sh,
+                             verifycap=verifycap, monitor=monitor)
+                d = r.start()
+                d.addCallbacks(_gather_repair_results, _repair_error)
+                return d
+
+        d.addCallback(_maybe_repair)
+        return d
+
     def check(self, monitor, verify=False, add_lease=False):
-        return self._node.check(monitor, verify, add_lease)
+        verifycap = self._verifycap
+        sb = self._storage_broker
+        servers = sb.get_all_servers()
+        sh = self._secret_holder
+
+        v = Checker(verifycap=verifycap, servers=servers,
+                    verify=verify, add_lease=add_lease, secret_holder=sh,
+                    monitor=monitor)
+        return v.start()
 
 
 class DecryptingConsumer:

commit a0570ac7c033ad52aeb5cee759494f4ed395f698
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 22 12:23:24 2010 -0700

    finally move download2.py into its rightful place: filenode.py
---
 src/allmydata/immutable/download2.py |  188 ----------------------------------
 src/allmydata/immutable/filenode.py  |  188 ++++++++++++++++++++++++++++++++++
 src/allmydata/nodemaker.py           |    2 +-
 src/allmydata/test/bench_dirnode.py  |    2 +-
 src/allmydata/test/test_filenode.py  |    2 +-
 src/allmydata/test/test_system.py    |    2 +-
 6 files changed, 192 insertions(+), 192 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
deleted file mode 100644
index 85f1c66..0000000
--- a/src/allmydata/immutable/download2.py
+++ /dev/null
@@ -1,188 +0,0 @@
-
-import binascii
-import time
-now = time.time
-from zope.interface import implements
-from twisted.internet import defer
-from twisted.internet.interfaces import IConsumer
-
-from allmydata.interfaces import IImmutableFileNode
-from allmydata import uri
-from pycryptopp.cipher.aes import AES
-
-# local imports
-from allmydata.immutable.downloader.node import DownloadNode
-from allmydata.immutable.downloader.status import DownloadStatus
-
-class CiphertextFileNode:
-    def __init__(self, verifycap, storage_broker, secret_holder,
-                 terminator, history, download_status=None):
-        assert isinstance(verifycap, uri.CHKFileVerifierURI)
-        if download_status is None:
-            ds = DownloadStatus(verifycap.storage_index, verifycap.size)
-            if history:
-                history.add_download(ds)
-            download_status = ds
-        self._node = DownloadNode(verifycap, storage_broker, secret_holder,
-                                  terminator, history, download_status)
-
-    def read(self, consumer, offset=0, size=None, read_ev=None):
-        """I am the main entry point, from which FileNode.read() can get
-        data. I feed the consumer with the desired range of ciphertext. I
-        return a Deferred that fires (with the consumer) when the read is
-        finished."""
-        return self._node.read(consumer, offset, size, read_ev)
-
-    def get_segment(self, segnum):
-        """Begin downloading a segment. I return a tuple (d, c): 'd' is a
-        Deferred that fires with (offset,data) when the desired segment is
-        available, and c is an object on which c.cancel() can be called to
-        disavow interest in the segment (after which 'd' will never fire).
-
-        You probably need to know the segment size before calling this,
-        unless you want the first few bytes of the file. If you ask for a
-        segment number which turns out to be too large, the Deferred will
-        errback with BadSegmentNumberError.
-
-        The Deferred fires with the offset of the first byte of the data
-        segment, so that you can call get_segment() before knowing the
-        segment size, and still know which data you received.
-        """
-        return self._node.get_segment(segnum)
-
-    def raise_error(self):
-        pass
-
-
-    def check_and_repair(self, monitor, verify=False, add_lease=False):
-        return self._node.check_and_repair(monitor, verify, add_lease)
-    def check(self, monitor, verify=False, add_lease=False):
-        return self._node.check(monitor, verify, add_lease)
-
-
-class DecryptingConsumer:
-    """I sit between a CiphertextDownloader (which acts as a Producer) and
-    the real Consumer, decrypting everything that passes by. The real
-    Consumer sees the real Producer, but the Producer sees us instead of the
-    real consumer."""
-    implements(IConsumer)
-
-    def __init__(self, consumer, readkey, offset, read_event):
-        self._consumer = consumer
-        self._read_event = read_event
-        # TODO: pycryptopp CTR-mode needs random-access operations: I want
-        # either a=AES(readkey, offset) or better yet both of:
-        #  a=AES(readkey, offset=0)
-        #  a.process(ciphertext, offset=xyz)
-        # For now, we fake it with the existing iv= argument.
-        offset_big = offset // 16
-        offset_small = offset % 16
-        iv = binascii.unhexlify("%032x" % offset_big)
-        self._decryptor = AES(readkey, iv=iv)
-        self._decryptor.process("\x00"*offset_small)
-
-    def registerProducer(self, producer, streaming):
-        # this passes through, so the real consumer can flow-control the real
-        # producer. Therefore we don't need to provide any IPushProducer
-        # methods. We implement all the IConsumer methods as pass-throughs,
-        # and only intercept write() to perform decryption.
-        self._consumer.registerProducer(producer, streaming)
-    def unregisterProducer(self):
-        self._consumer.unregisterProducer()
-    def write(self, ciphertext):
-        started = now()
-        plaintext = self._decryptor.process(ciphertext)
-        elapsed = now() - started
-        self._read_event.update(0, elapsed, 0)
-        self._consumer.write(plaintext)
-
-class ImmutableFileNode:
-    implements(IImmutableFileNode)
-
-    # I wrap a CiphertextFileNode with a decryption key
-    def __init__(self, filecap, storage_broker, secret_holder, terminator,
-                 history):
-        assert isinstance(filecap, uri.CHKFileURI)
-        verifycap = filecap.get_verify_cap()
-        ds = DownloadStatus(verifycap.storage_index, verifycap.size)
-        if history:
-            history.add_download(ds)
-        self._download_status = ds
-        self._cnode = CiphertextFileNode(verifycap, storage_broker,
-                                         secret_holder, terminator, history, ds)
-        assert isinstance(filecap, uri.CHKFileURI)
-        self.u = filecap
-        self._readkey = filecap.key
-
-    # TODO: I'm not sure about this.. what's the use case for node==node? If
-    # we keep it here, we should also put this on CiphertextFileNode
-    def __hash__(self):
-        return self.u.__hash__()
-    def __eq__(self, other):
-        if isinstance(other, ImmutableFileNode):
-            return self.u.__eq__(other.u)
-        else:
-            return False
-    def __ne__(self, other):
-        if isinstance(other, ImmutableFileNode):
-            return self.u.__eq__(other.u)
-        else:
-            return True
-
-    def read(self, consumer, offset=0, size=None):
-        actual_size = size
-        if actual_size == None:
-            actual_size = self.u.size
-        actual_size = actual_size - offset
-        read_ev = self._download_status.add_read_event(offset,actual_size,
-                                                       now())
-        decryptor = DecryptingConsumer(consumer, self._readkey, offset, read_ev)
-        d = self._cnode.read(decryptor, offset, size, read_ev)
-        d.addCallback(lambda dc: consumer)
-        return d
-
-    def raise_error(self):
-        pass
-
-    def get_write_uri(self):
-        return None
-
-    def get_readonly_uri(self):
-        return self.get_uri()
-
-    def get_uri(self):
-        return self.u.to_string()
-    def get_cap(self):
-        return self.u
-    def get_readcap(self):
-        return self.u.get_readonly()
-    def get_verify_cap(self):
-        return self.u.get_verify_cap()
-    def get_repair_cap(self):
-        # CHK files can be repaired with just the verifycap
-        return self.u.get_verify_cap()
-
-    def get_storage_index(self):
-        return self.u.get_storage_index()
-
-    def get_size(self):
-        return self.u.get_size()
-    def get_current_size(self):
-        return defer.succeed(self.get_size())
-
-    def is_mutable(self):
-        return False
-
-    def is_readonly(self):
-        return True
-
-    def is_unknown(self):
-        return False
-
-    def is_allowed_in_immutable_directory(self):
-        return True
-
-    def check_and_repair(self, monitor, verify=False, add_lease=False):
-        return self._cnode.check_and_repair(monitor, verify, add_lease)
-    def check(self, monitor, verify=False, add_lease=False):
-        return self._cnode.check(monitor, verify, add_lease)
diff --git a/src/allmydata/immutable/filenode.py b/src/allmydata/immutable/filenode.py
new file mode 100644
index 0000000..85f1c66
--- /dev/null
+++ b/src/allmydata/immutable/filenode.py
@@ -0,0 +1,188 @@
+
+import binascii
+import time
+now = time.time
+from zope.interface import implements
+from twisted.internet import defer
+from twisted.internet.interfaces import IConsumer
+
+from allmydata.interfaces import IImmutableFileNode
+from allmydata import uri
+from pycryptopp.cipher.aes import AES
+
+# local imports
+from allmydata.immutable.downloader.node import DownloadNode
+from allmydata.immutable.downloader.status import DownloadStatus
+
+class CiphertextFileNode:
+    def __init__(self, verifycap, storage_broker, secret_holder,
+                 terminator, history, download_status=None):
+        assert isinstance(verifycap, uri.CHKFileVerifierURI)
+        if download_status is None:
+            ds = DownloadStatus(verifycap.storage_index, verifycap.size)
+            if history:
+                history.add_download(ds)
+            download_status = ds
+        self._node = DownloadNode(verifycap, storage_broker, secret_holder,
+                                  terminator, history, download_status)
+
+    def read(self, consumer, offset=0, size=None, read_ev=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data. I feed the consumer with the desired range of ciphertext. I
+        return a Deferred that fires (with the consumer) when the read is
+        finished."""
+        return self._node.read(consumer, offset, size, read_ev)
+
+    def get_segment(self, segnum):
+        """Begin downloading a segment. I return a tuple (d, c): 'd' is a
+        Deferred that fires with (offset,data) when the desired segment is
+        available, and c is an object on which c.cancel() can be called to
+        disavow interest in the segment (after which 'd' will never fire).
+
+        You probably need to know the segment size before calling this,
+        unless you want the first few bytes of the file. If you ask for a
+        segment number which turns out to be too large, the Deferred will
+        errback with BadSegmentNumberError.
+
+        The Deferred fires with the offset of the first byte of the data
+        segment, so that you can call get_segment() before knowing the
+        segment size, and still know which data you received.
+        """
+        return self._node.get_segment(segnum)
+
+    def raise_error(self):
+        pass
+
+
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        return self._node.check_and_repair(monitor, verify, add_lease)
+    def check(self, monitor, verify=False, add_lease=False):
+        return self._node.check(monitor, verify, add_lease)
+
+
+class DecryptingConsumer:
+    """I sit between a CiphertextDownloader (which acts as a Producer) and
+    the real Consumer, decrypting everything that passes by. The real
+    Consumer sees the real Producer, but the Producer sees us instead of the
+    real consumer."""
+    implements(IConsumer)
+
+    def __init__(self, consumer, readkey, offset, read_event):
+        self._consumer = consumer
+        self._read_event = read_event
+        # TODO: pycryptopp CTR-mode needs random-access operations: I want
+        # either a=AES(readkey, offset) or better yet both of:
+        #  a=AES(readkey, offset=0)
+        #  a.process(ciphertext, offset=xyz)
+        # For now, we fake it with the existing iv= argument.
+        offset_big = offset // 16
+        offset_small = offset % 16
+        iv = binascii.unhexlify("%032x" % offset_big)
+        self._decryptor = AES(readkey, iv=iv)
+        self._decryptor.process("\x00"*offset_small)
+
+    def registerProducer(self, producer, streaming):
+        # this passes through, so the real consumer can flow-control the real
+        # producer. Therefore we don't need to provide any IPushProducer
+        # methods. We implement all the IConsumer methods as pass-throughs,
+        # and only intercept write() to perform decryption.
+        self._consumer.registerProducer(producer, streaming)
+    def unregisterProducer(self):
+        self._consumer.unregisterProducer()
+    def write(self, ciphertext):
+        started = now()
+        plaintext = self._decryptor.process(ciphertext)
+        elapsed = now() - started
+        self._read_event.update(0, elapsed, 0)
+        self._consumer.write(plaintext)
+
+class ImmutableFileNode:
+    implements(IImmutableFileNode)
+
+    # I wrap a CiphertextFileNode with a decryption key
+    def __init__(self, filecap, storage_broker, secret_holder, terminator,
+                 history):
+        assert isinstance(filecap, uri.CHKFileURI)
+        verifycap = filecap.get_verify_cap()
+        ds = DownloadStatus(verifycap.storage_index, verifycap.size)
+        if history:
+            history.add_download(ds)
+        self._download_status = ds
+        self._cnode = CiphertextFileNode(verifycap, storage_broker,
+                                         secret_holder, terminator, history, ds)
+        assert isinstance(filecap, uri.CHKFileURI)
+        self.u = filecap
+        self._readkey = filecap.key
+
+    # TODO: I'm not sure about this.. what's the use case for node==node? If
+    # we keep it here, we should also put this on CiphertextFileNode
+    def __hash__(self):
+        return self.u.__hash__()
+    def __eq__(self, other):
+        if isinstance(other, ImmutableFileNode):
+            return self.u.__eq__(other.u)
+        else:
+            return False
+    def __ne__(self, other):
+        if isinstance(other, ImmutableFileNode):
+            return self.u.__eq__(other.u)
+        else:
+            return True
+
+    def read(self, consumer, offset=0, size=None):
+        actual_size = size
+        if actual_size == None:
+            actual_size = self.u.size
+        actual_size = actual_size - offset
+        read_ev = self._download_status.add_read_event(offset,actual_size,
+                                                       now())
+        decryptor = DecryptingConsumer(consumer, self._readkey, offset, read_ev)
+        d = self._cnode.read(decryptor, offset, size, read_ev)
+        d.addCallback(lambda dc: consumer)
+        return d
+
+    def raise_error(self):
+        pass
+
+    def get_write_uri(self):
+        return None
+
+    def get_readonly_uri(self):
+        return self.get_uri()
+
+    def get_uri(self):
+        return self.u.to_string()
+    def get_cap(self):
+        return self.u
+    def get_readcap(self):
+        return self.u.get_readonly()
+    def get_verify_cap(self):
+        return self.u.get_verify_cap()
+    def get_repair_cap(self):
+        # CHK files can be repaired with just the verifycap
+        return self.u.get_verify_cap()
+
+    def get_storage_index(self):
+        return self.u.get_storage_index()
+
+    def get_size(self):
+        return self.u.get_size()
+    def get_current_size(self):
+        return defer.succeed(self.get_size())
+
+    def is_mutable(self):
+        return False
+
+    def is_readonly(self):
+        return True
+
+    def is_unknown(self):
+        return False
+
+    def is_allowed_in_immutable_directory(self):
+        return True
+
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        return self._cnode.check_and_repair(monitor, verify, add_lease)
+    def check(self, monitor, verify=False, add_lease=False):
+        return self._cnode.check(monitor, verify, add_lease)
diff --git a/src/allmydata/nodemaker.py b/src/allmydata/nodemaker.py
index 227f58f..531d7f1 100644
--- a/src/allmydata/nodemaker.py
+++ b/src/allmydata/nodemaker.py
@@ -2,7 +2,7 @@ import weakref
 from zope.interface import implements
 from allmydata.interfaces import INodeMaker, MustBeDeepImmutableError
 from allmydata.immutable.literal import LiteralFileNode
-from allmydata.immutable.download2 import ImmutableFileNode, CiphertextFileNode
+from allmydata.immutable.filenode import ImmutableFileNode, CiphertextFileNode
 from allmydata.immutable.upload import Data
 from allmydata.mutable.filenode import MutableFileNode
 from allmydata.dirnode import DirectoryNode, pack_children
diff --git a/src/allmydata/test/bench_dirnode.py b/src/allmydata/test/bench_dirnode.py
index 6fa1563..ffc6637 100644
--- a/src/allmydata/test/bench_dirnode.py
+++ b/src/allmydata/test/bench_dirnode.py
@@ -6,7 +6,7 @@ from zope.interface import implements
 from allmydata import dirnode, uri
 from allmydata.interfaces import IFileNode
 from allmydata.mutable.filenode import MutableFileNode
-from allmydata.immutable.download2 import ImmutableFileNode
+from allmydata.immutable.filenode import ImmutableFileNode
 
 class ContainerNode:
     implements(IFileNode)
diff --git a/src/allmydata/test/test_filenode.py b/src/allmydata/test/test_filenode.py
index 5d86203..61bb0e8 100644
--- a/src/allmydata/test/test_filenode.py
+++ b/src/allmydata/test/test_filenode.py
@@ -3,7 +3,7 @@ from twisted.trial import unittest
 from allmydata import uri, client
 from allmydata.monitor import Monitor
 from allmydata.immutable.literal import LiteralFileNode
-from allmydata.immutable.download2 import ImmutableFileNode
+from allmydata.immutable.filenode import ImmutableFileNode
 from allmydata.mutable.filenode import MutableFileNode
 from allmydata.util import hashutil
 from allmydata.util.consumer import download_to_data
diff --git a/src/allmydata/test/test_system.py b/src/allmydata/test/test_system.py
index b7d2fc1..8657da0 100644
--- a/src/allmydata/test/test_system.py
+++ b/src/allmydata/test/test_system.py
@@ -10,7 +10,7 @@ from allmydata.storage.mutable import MutableShareFile
 from allmydata.storage.server import si_a2b
 from allmydata.immutable import offloaded, upload
 from allmydata.immutable.literal import LiteralFileNode
-from allmydata.immutable.download2 import ImmutableFileNode
+from allmydata.immutable.filenode import ImmutableFileNode
 from allmydata.util import idlib, mathutil
 from allmydata.util import log, base32
 from allmydata.util.consumer import MemoryConsumer, download_to_data

commit 7a3e6ef94f2bf3a01e01134e51567b19d17181df
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 22 12:20:46 2010 -0700

    move notes out of download2.py and into notes.txt
---
 src/allmydata/immutable/download2.py |   70 ----
 src/allmydata/immutable/notes.txt    |  708 ++++++++++++++++++++++++++++++++++
 2 files changed, 708 insertions(+), 70 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 61de459..85f1c66 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -14,14 +14,6 @@ from pycryptopp.cipher.aes import AES
 from allmydata.immutable.downloader.node import DownloadNode
 from allmydata.immutable.downloader.status import DownloadStatus
 
-# all classes are also Services, and the rule is that you don't initiate more
-# work unless self.running
-
-# GC: decide whether each service is restartable or not. For non-restartable
-# services, stopService() should delete a lot of attributes to kill reference
-# cycles. The primary goal is to decref remote storage BucketReaders when a
-# download is complete.
-
 class CiphertextFileNode:
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history, download_status=None):
@@ -194,65 +186,3 @@ class ImmutableFileNode:
         return self._cnode.check_and_repair(monitor, verify, add_lease)
     def check(self, monitor, verify=False, add_lease=False):
         return self._cnode.check(monitor, verify, add_lease)
-
-# TODO: if server1 has all shares, and server2-10 have one each, make the
-# loop stall slightly before requesting all shares from the first server, to
-# give it a chance to learn about the other shares and get some diversity.
-# Or, don't bother, let the first block all come from one server, and take
-# comfort in the fact that we'll learn about the other servers by the time we
-# fetch the second block.
-#
-# davidsarah points out that we could use sequential (instead of parallel)
-# fetching of multiple block from a single server: by the time the first
-# block arrives, we'll hopefully have heard about other shares. This would
-# induce some RTT delays (i.e. lose pipelining) in the case that this server
-# has the only shares, but that seems tolerable. We could rig it to only use
-# sequential requests on the first segment.
-
-# as a query gets later, we're more willing to duplicate work.
-
-# should change server read protocol to allow small shares to be fetched in a
-# single RTT. Instead of get_buckets-then-read, just use read(shnums, readv),
-# where shnums=[] means all shares, and the return value is a dict of
-# # shnum->ta (like with mutable files). The DYHB query should also fetch the
-# offset table, since everything else can be located once we have that.
-
-
-# ImmutableFileNode
-#    DecryptingConsumer
-#  CiphertextFileNode
-#    Segmentation
-#   ShareFinder
-#   SegmentFetcher[segnum] (one at a time)
-#   CommonShare[shnum]
-#   Share[shnum,server]
-
-
-# TODO: if offset table is corrupt, attacker could cause us to fetch whole
-# (large) share. But only from that one server, and they could throw lots of
-# data at our connection anyways.
-
-# log budget: when downloading at 1MBps (i.e. 8 segments-per-second), 10
-# log.OPERATIONAL per second, 100 log.NOISY per second. With k=3, that's 3
-# log.NOISY per block fetch.
-
-
-# test_cli.Error failed for a while: ShareFinder created, used up
-# (NotEnoughSharesError), started again. The self.running=False is the
-# problem.
-#
-# The second download is hungry, but because ShareFinder.running is false, it
-# never notifies the SegmentFetcher that there are no more shares coming, so
-# the download never completes. To trigger this in tests, we need the first
-# download to want more shares (so it must fail with NotEnoughSharesError, or
-# we must lose a share/server between downloads).
-#
-# fix was to not call self.stop when ShareFinder runs out of shares. stop()
-# is now only called by the Terminator.
-
-# TODO: make sure that _signal_corruption(f) isn't sending private local
-# variables in the CopiedFailure
-
-# tests to write:
-# * truncated share, so _satisfy_* doesn't get all it wants
-# * slow server
diff --git a/src/allmydata/immutable/notes.txt b/src/allmydata/immutable/notes.txt
new file mode 100644
index 0000000..ad11565
--- /dev/null
+++ b/src/allmydata/immutable/notes.txt
@@ -0,0 +1,708 @@
+
+# TODO: if server1 has all shares, and server2-10 have one each, make the
+# loop stall slightly before requesting all shares from the first server, to
+# give it a chance to learn about the other shares and get some diversity.
+# Or, don't bother, let the first block all come from one server, and take
+# comfort in the fact that we'll learn about the other servers by the time we
+# fetch the second block.
+#
+# davidsarah points out that we could use sequential (instead of parallel)
+# fetching of multiple block from a single server: by the time the first
+# block arrives, we'll hopefully have heard about other shares. This would
+# induce some RTT delays (i.e. lose pipelining) in the case that this server
+# has the only shares, but that seems tolerable. We could rig it to only use
+# sequential requests on the first segment.
+
+# as a query gets later, we're more willing to duplicate work.
+
+# should change server read protocol to allow small shares to be fetched in a
+# single RTT. Instead of get_buckets-then-read, just use read(shnums, readv),
+# where shnums=[] means all shares, and the return value is a dict of
+# # shnum->ta (like with mutable files). The DYHB query should also fetch the
+# offset table, since everything else can be located once we have that.
+
+
+# ImmutableFileNode
+#    DecryptingConsumer
+#  CiphertextFileNode
+#    Segmentation
+#   ShareFinder
+#   SegmentFetcher[segnum] (one at a time)
+#   CommonShare[shnum]
+#   Share[shnum,server]
+
+
+# TODO: if offset table is corrupt, attacker could cause us to fetch whole
+# (large) share. But only from that one server, and they could throw lots of
+# data at our connection anyways.
+
+# log budget: when downloading at 1MBps (i.e. 8 segments-per-second), 10
+# log.OPERATIONAL per second, 100 log.NOISY per second. With k=3, that's 3
+# log.NOISY per block fetch.
+
+
+# test_cli.Error failed for a while: ShareFinder created, used up
+# (NotEnoughSharesError), started again. The self.running=False is the
+# problem.
+#
+# The second download is hungry, but because ShareFinder.running is false, it
+# never notifies the SegmentFetcher that there are no more shares coming, so
+# the download never completes. To trigger this in tests, we need the first
+# download to want more shares (so it must fail with NotEnoughSharesError, or
+# we must lose a share/server between downloads).
+#
+# fix was to not call self.stop when ShareFinder runs out of shares. stop()
+# is now only called by the Terminator.
+
+# TODO: make sure that _signal_corruption(f) isn't sending private local
+# variables in the CopiedFailure
+
+# tests to write:
+# * truncated share, so _satisfy_* doesn't get all it wants
+# * slow server
+
+# all classes are also Services, and the rule is that you don't initiate more
+# work unless self.running
+
+# GC: decide whether each service is restartable or not. For non-restartable
+# services, stopService() should delete a lot of attributes to kill reference
+# cycles. The primary goal is to decref remote storage BucketReaders when a
+# download is complete.
+
+========================================
+old stuff from download2_off:
+
+#! /usr/bin/python
+
+# known (shnum,Server) pairs are sorted into a list according to
+# desireability. This sort is picking a winding path through a matrix of
+# [shnum][server]. The goal is to get diversity of both shnum and server.
+
+# The initial order is:
+#  find the lowest shnum on the first server, add it
+#  look at the next server, find the lowest shnum that we don't already have
+#   if any
+#  next server, etc, until all known servers are checked
+#  now look at servers that we skipped (because ...
+
+# Keep track of which block requests are outstanding by (shnum,Server). Don't
+# bother prioritizing "validated" shares: the overhead to pull the share hash
+# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
+# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
+# 832 bytes). Each time a block request is sent, also request any necessary
+# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
+# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
+
+# For each outstanding segread, walk the list and send requests (skipping
+# outstanding shnums) until requests for k distinct shnums are in flight. If
+# we can't do that, ask for more. If we get impatient on a request, find the
+# first non-outstanding
+
+# start with the first Share in the list, and send a request. Then look at
+# the next one. If we already have a pending request for the same shnum or
+# server, push that Share down onto the fallback list and try the next one,
+# etc. If we run out of non-fallback shares, use the fallback ones,
+# preferring shnums that we don't have outstanding requests for (i.e. assume
+# that all requests will complete). Do this by having a second fallback list.
+
+# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
+# per file, not thousands.
+
+# actually, don't bother sorting the initial list. Append Shares as the
+# responses come back, that will put the fastest servers at the front of the
+# list, and give a tiny preference to servers that are earlier in the
+# permuted order.
+
+# more ideas:
+#  sort shares by:
+#   1: number of roundtrips needed to get some data
+#   2: share number
+#   3: ms of RTT delay
+# maybe measure average time-to-completion of requests, compare completion
+# time against that, much larger indicates congestion on the server side
+# or the server's upstream speed is less than our downstream. Minimum
+# time-to-completion indicates min(our-downstream,their-upstream). Could
+# fetch shares one-at-a-time to measure that better.
+
+# when should we risk duplicate work and send a new request?
+
+def walk(self):
+    shares = sorted(list)
+    oldshares = copy(shares)
+    outstanding = list()
+    fallbacks = list()
+    second_fallbacks = list()
+    while len(outstanding.nonlate.shnums) < k: # need more requests
+        while oldshares:
+            s = shares.pop(0)
+            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
+                fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # must use fallback list. Ask for more servers while we're at it.
+        ask_for_more_servers()
+        while fallbacks:
+            s = fallbacks.pop(0)
+            if s.shnum in outstanding.shnums:
+                # assume that the outstanding requests will complete, but
+                # send new requests for other shnums to existing servers
+                second_fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # if we get here, we're being forced to send out multiple queries per
+        # share. We've already asked for more servers, which might help. If
+        # there are no late outstanding queries, then duplicate shares won't
+        # help. Don't send queries for duplicate shares until some of the
+        # queries are late.
+        if outstanding.late:
+            # we're allowed to try any non-outstanding share
+            while second_fallbacks:
+                pass
+    newshares = outstanding + fallbacks + second_fallbacks + oldshares
+        
+
+class Server:
+    """I represent an abstract Storage Server. One day, the StorageBroker
+    will return instances of me. For now, the StorageBroker returns (peerid,
+    RemoteReference) tuples, and this code wraps a Server instance around
+    them.
+    """
+    def __init__(self, peerid, ss):
+        self.peerid = peerid
+        self.remote = ss
+        self._remote_buckets = {} # maps shnum to RIBucketReader
+        # TODO: release the bucket references on shares that we no longer
+        # want. OTOH, why would we not want them? Corruption?
+
+    def send_query(self, storage_index):
+        """I return a Deferred that fires with a set of shnums. If the server
+        had shares available, I will retain the RemoteReferences to its
+        buckets, so that get_data(shnum, range) can be called later."""
+        d = self.remote.callRemote("get_buckets", self.storage_index)
+        d.addCallback(self._got_response)
+        return d
+
+    def _got_response(self, r):
+        self._remote_buckets = r
+        return set(r.keys())
+
+class ShareOnAServer:
+    """I represent one instance of a share, known to live on a specific
+    server. I am created every time a server responds affirmatively to a
+    do-you-have-block query."""
+
+    def __init__(self, shnum, server):
+        self._shnum = shnum
+        self._server = server
+        self._block_hash_tree = None
+
+    def cost(self, segnum):
+        """I return a tuple of (roundtrips, bytes, rtt), indicating how
+        expensive I think it would be to fetch the given segment. Roundtrips
+        indicates how many roundtrips it is likely to take (one to get the
+        data and hashes, plus one to get the offset table and UEB if this is
+        the first segment we've ever fetched). 'bytes' is how many bytes we
+        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
+        seconds for a trivial request. The downloading algorithm will compare
+        costs to decide which shares should be used."""
+        # the most significant factor here is roundtrips: a Share for which
+        # we already have the offset table is better to than a brand new one
+
+    def max_bandwidth(self):
+        """Return a float, indicating the highest plausible bytes-per-second
+        that I've observed coming from this share. This will be based upon
+        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
+        can we used to estimate the server's upstream bandwidth. Clearly this
+        is only accurate if a share is retrieved with no contention for
+        either the upstream, downstream, or middle of the connection, but it
+        may still serve as a useful metric for deciding which servers to pull
+        from."""
+
+    def get_segment(self, segnum):
+        """I return a Deferred that will fire with the segment data, or
+        errback."""
+
+class NativeShareOnAServer(ShareOnAServer):
+    """For tahoe native (foolscap) servers, I contain a RemoteReference to
+    the RIBucketReader instance."""
+    def __init__(self, shnum, server, rref):
+        ShareOnAServer.__init__(self, shnum, server)
+        self._rref = rref # RIBucketReader
+
+class Share:
+    def __init__(self, shnum):
+        self._shnum = shnum
+        # _servers are the Server instances which appear to hold a copy of
+        # this share. It is populated when the ValidShare is first created,
+        # or when we receive a get_buckets() response for a shnum that
+        # already has a ValidShare instance. When we lose the connection to a
+        # server, we remove it.
+        self._servers = set()
+        # offsets, UEB, and share_hash_tree all live in the parent.
+        # block_hash_tree lives here.
+        self._block_hash_tree = None
+
+        self._want
+
+    def get_servers(self):
+        return self._servers
+
+
+    def get_block(self, segnum):
+        # read enough data to obtain a single validated block
+        if not self.have_offsets:
+            # we get the offsets in their own read, since they tell us where
+            # everything else lives. We must fetch offsets for each share
+            # separately, since they aren't directly covered by the UEB.
+            pass
+        if not self.parent.have_ueb:
+            # use _guessed_segsize to make a guess about the layout, so we
+            # can fetch both the offset table and the UEB in the same read.
+            # This also requires making a guess about the presence or absence
+            # of the plaintext_hash_tree. Oh, and also the version number. Oh
+            # well.
+            pass
+
+class CiphertextDownloader:
+    """I manage all downloads for a single file. I operate a state machine
+    with input events that are local read() requests, responses to my remote
+    'get_bucket' and 'read_bucket' messages, and connection establishment and
+    loss. My outbound events are connection establishment requests and bucket
+    read requests messages.
+    """
+    # eventually this will merge into the FileNode
+    ServerClass = Server # for tests to override
+
+    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
+                 shutdowner):
+        # values we get from the filecap
+        self._storage_index = si = storage_index
+        self._ueb_hash = ueb_hash
+        self._size = size
+        self._needed_shares = k
+        self._total_shares = N
+        self._share_hash_tree = IncompleteHashTree(self._total_shares)
+        # values we discover when we first fetch the UEB
+        self._ueb = None # is dict after UEB fetch+validate
+        self._segsize = None
+        self._numsegs = None
+        self._blocksize = None
+        self._tail_segsize = None
+        self._ciphertext_hash = None # optional
+        # structures we create when we fetch the UEB, then continue to fill
+        # as we download the file
+        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
+        self._ciphertext_hash_tree = None
+
+        # values we learn as we download the file
+        self._offsets = {} # (shnum,Server) to offset table (dict)
+        self._block_hash_tree = {} # shnum to IncompleteHashTree
+        # other things which help us
+        self._guessed_segsize = min(128*1024, size)
+        self._active_share_readers = {} # maps shnum to Reader instance
+        self._share_readers = [] # sorted by preference, best first
+        self._readers = set() # set of Reader instances
+        self._recent_horizon = 10 # seconds
+
+        # 'shutdowner' is a MultiService parent used to cancel all downloads
+        # when the node is shutting down, to let tests have a clean reactor.
+
+        self._init_available_servers()
+        self._init_find_enough_shares()
+
+    # _available_servers is an iterator that provides us with Server
+    # instances. Each time we pull out a Server, we immediately send it a
+    # query, so we don't need to keep track of who we've sent queries to.
+
+    def _init_available_servers(self):
+        self._available_servers = self._get_available_servers()
+        self._no_more_available_servers = False
+
+    def _get_available_servers(self):
+        """I am a generator of servers to use, sorted by the order in which
+        we should query them. I make sure there are no duplicates in this
+        list."""
+        # TODO: make StorageBroker responsible for this non-duplication, and
+        # replace this method with a simple iter(get_servers_for_index()),
+        # plus a self._no_more_available_servers=True
+        seen = set()
+        sb = self._storage_broker
+        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
+            if peerid not in seen:
+                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
+                seen.add(peerid)
+        self._no_more_available_servers = True
+
+    # this block of code is responsible for having enough non-problematic
+    # distinct shares/servers available and ready for download, and for
+    # limiting the number of queries that are outstanding. The idea is that
+    # we'll use the k fastest/best shares, and have the other ones in reserve
+    # in case those servers stop responding or respond too slowly. We keep
+    # track of all known shares, but we also keep track of problematic shares
+    # (ones with hash failures or lost connections), so we can put them at
+    # the bottom of the list.
+
+    def _init_find_enough_shares(self):
+        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
+        # where viable (but not yet validated) shares are located. Each
+        # get_bucket() response adds to this map, each act of validation
+        # removes from it.
+        self._sharemap = DictOfSets()
+
+        # _sharemap maps shnum to set of Servers, and remembers where viable
+        # shares are located. Each get_bucket() response adds to this map,
+        # each hash failure or disconnect removes from it. (TODO: if we
+        # disconnect but reconnect later, we should be allowed to re-query).
+        self._sharemap = DictOfSets()
+
+        # _problem_shares is a set of (shnum, Server) tuples, and
+
+        # _queries_in_flight maps a Server to a timestamp, which remembers
+        # which servers we've sent queries to (and when) but have not yet
+        # heard a response. This lets us put a limit on the number of
+        # outstanding queries, to limit the size of the work window (how much
+        # extra work we ask servers to do in the hopes of keeping our own
+        # pipeline filled). We remove a Server from _queries_in_flight when
+        # we get an answer/error or we finally give up. If we ever switch to
+        # a non-connection-oriented protocol (like UDP, or forwarded Chord
+        # queries), we can use this information to retransmit any query that
+        # has gone unanswered for too long.
+        self._queries_in_flight = dict()
+
+    def _count_recent_queries_in_flight(self):
+        now = time.time()
+        recent = now - self._recent_horizon
+        return len([s for (s,when) in self._queries_in_flight.items()
+                    if when > recent])
+
+    def _find_enough_shares(self):
+        # goal: have 2*k distinct not-invalid shares available for reading,
+        # from 2*k distinct servers. Do not have more than 4*k "recent"
+        # queries in flight at a time.
+        if (len(self._sharemap) >= 2*self._needed_shares
+            and len(self._sharemap.values) >= 2*self._needed_shares):
+            return
+        num = self._count_recent_queries_in_flight()
+        while num < 4*self._needed_shares:
+            try:
+                s = self._available_servers.next()
+            except StopIteration:
+                return # no more progress can be made
+            self._queries_in_flight[s] = time.time()
+            d = s.send_query(self._storage_index)
+            d.addBoth(incidentally, self._queries_in_flight.discard, s)
+            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
+                                           for shnum in shnums],
+                           lambda f: self._query_error(f, s))
+            d.addErrback(self._error)
+            d.addCallback(self._reschedule)
+            num += 1
+
+    def _query_error(self, f, s):
+        # a server returned an error, log it gently and ignore
+        level = log.WEIRD
+        if f.check(DeadReferenceError):
+            level = log.UNUSUAL
+        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
+                failure=f, level=level, umid="3uuBUQ")
+
+    # this block is responsible for turning known shares into usable shares,
+    # by fetching enough data to validate their contents.
+
+    # UEB (from any share)
+    # share hash chain, validated (from any share, for given shnum)
+    # block hash (any share, given shnum)
+
+    def _got_ueb(self, ueb_data, share):
+        if self._ueb is not None:
+            return
+        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
+            share.error("UEB hash does not match")
+            return
+        d = uri.unpack_extension(ueb_data)
+        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
+
+
+        # There are several kinds of things that can be found in a UEB.
+        # First, things that we really need to learn from the UEB in order to
+        # do this download. Next: things which are optional but not redundant
+        # -- if they are present in the UEB they will get used. Next, things
+        # that are optional and redundant. These things are required to be
+        # consistent: they don't have to be in the UEB, but if they are in
+        # the UEB then they will be checked for consistency with the
+        # already-known facts, and if they are inconsistent then an exception
+        # will be raised. These things aren't actually used -- they are just
+        # tested for consistency and ignored. Finally: things which are
+        # deprecated -- they ought not be in the UEB at all, and if they are
+        # present then a warning will be logged but they are otherwise
+        # ignored.
+
+        # First, things that we really need to learn from the UEB:
+        # segment_size, crypttext_root_hash, and share_root_hash.
+        self._segsize = d['segment_size']
+
+        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
+        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
+
+        self._tail_segsize = self._size % self._segsize
+        if self._tail_segsize == 0:
+            self._tail_segsize = self._segsize
+        # padding for erasure code
+        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
+                                                    self._needed_shares)
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B.
+        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
+        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
+
+        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
+
+
+        # Next: things that are optional and not redundant: crypttext_hash
+        if 'crypttext_hash' in d:
+            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
+                self._ciphertext_hash = d['crypttext_hash']
+            else:
+                log.msg("ignoring bad-length UEB[crypttext_hash], "
+                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
+                                                   hashutil.CRYPTO_VAL_SIZE),
+                        umid="oZkGLA", level=log.WEIRD)
+
+        # we ignore all of the redundant fields when downloading. The
+        # Verifier uses a different code path which does not ignore them.
+
+        # finally, set self._ueb as a marker that we don't need to request it
+        # anymore
+        self._ueb = d
+
+    def _got_share_hashes(self, hashes, share):
+        assert isinstance(hashes, dict)
+        try:
+            self._share_hash_tree.set_hashes(hashes)
+        except (IndexError, BadHashError, NotEnoughHashesError), le:
+            share.error("Bad or missing hashes")
+            return
+
+    #def _got_block_hashes(
+
+    def _init_validate_enough_shares(self):
+        # _valid_shares maps shnum to ValidatedShare instances, and is
+        # populated once the block hash root has been fetched and validated
+        # (which requires any valid copy of the UEB, and a valid copy of the
+        # share hash chain for each shnum)
+        self._valid_shares = {}
+
+        # _target_shares is an ordered list of ReadyShare instances, each of
+        # which is a (shnum, server) tuple. It is sorted in order of
+        # preference: we expect to get the fastest response from the
+        # ReadyShares at the front of the list. It is also sorted to
+        # distribute the shnums, so that fetching shares from
+        # _target_shares[:k] is likely (but not guaranteed) to give us k
+        # distinct shares. The rule is that we skip over entries for blocks
+        # that we've already received, limit the number of recent queries for
+        # the same block, 
+        self._target_shares = []
+
+    def _validate_enough_shares(self):
+        # my goal is to have at least 2*k distinct validated shares from at
+        # least 2*k distinct servers
+        valid_share_servers = set()
+        for vs in self._valid_shares.values():
+            valid_share_servers.update(vs.get_servers())
+        if (len(self._valid_shares) >= 2*self._needed_shares
+            and len(self._valid_share_servers) >= 2*self._needed_shares):
+            return
+        #for 
+
+    def _reschedule(self, _ign):
+        # fire the loop again
+        if not self._scheduled:
+            self._scheduled = True
+            eventually(self._loop)
+
+    def _loop(self):
+        self._scheduled = False
+        # what do we need?
+
+        self._find_enough_shares()
+        self._validate_enough_shares()
+
+        if not self._ueb:
+            # we always need a copy of the UEB
+            pass
+
+    def _error(self, f):
+        # this is an unexpected error: a coding bug
+        log.err(f, level=log.UNUSUAL)
+            
+
+
+# using a single packed string (and an offset table) may be an artifact of
+# our native storage server: other backends might allow cheap multi-part
+# files (think S3, several buckets per share, one for each section).
+
+# find new names for:
+#  data_holder
+#  Share / Share2  (ShareInstance / Share? but the first is more useful)
+
+class IShare(Interface):
+    """I represent a single instance of a single share (e.g. I reference the
+    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
+    This interface is used by SegmentFetcher to retrieve validated blocks.
+    """
+    def get_block(segnum):
+        """Return an Observer2, which will be notified with the following
+        events:
+         state=COMPLETE, block=data (terminal): validated block data
+         state=OVERDUE (non-terminal): we have reason to believe that the
+                                       request might have stalled, or we
+                                       might just be impatient
+         state=CORRUPT (terminal): the data we received was corrupt
+         state=DEAD (terminal): the connection has failed
+        """
+
+
+# it'd be nice if we receive the hashes before the block, or just
+# afterwards, so we aren't stuck holding on to unvalidated blocks
+# that we can't process. If we guess the offsets right, we can
+# accomplish this by sending the block request after the metadata
+# requests (by keeping two separate requestlists), and have a one RTT
+# pipeline like:
+#  1a=metadata, 1b=block
+#  1b->process+deliver : one RTT
+
+# But if we guess wrong, and fetch the wrong part of the block, we'll
+# have a pipeline that looks like:
+#  1a=wrong metadata, 1b=wrong block
+#  1a->2a=right metadata,2b=right block
+#  2b->process+deliver
+# which means two RTT and buffering one block (which, since we'll
+# guess the segsize wrong for everything, means buffering one
+# segment)
+
+# if we start asking for multiple segments, we could get something
+# worse:
+#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
+#  1a->2a=right metadata,2b=right block0,2c=right block1, .
+#  2b->process+deliver
+
+# which means two RTT but fetching and buffering the whole file
+# before delivering anything. However, since we don't know when the
+# other shares are going to arrive, we need to avoid having more than
+# one block in the pipeline anyways. So we shouldn't be able to get
+# into this state.
+
+# it also means that, instead of handling all of
+# self._requested_blocks at once, we should only be handling one
+# block at a time: one of the requested block should be special
+# (probably FIFO). But retire all we can.
+
+    # this might be better with a Deferred, using COMPLETE as the success
+    # case and CORRUPT/DEAD in an errback, because that would let us hold the
+    # 'share' and 'shnum' arguments locally (instead of roundtripping them
+    # through Share.send_request). But that OVERDUE is not terminal. So I
+    # want a new sort of callback mechanism, with the extra-argument-passing
+    # aspects of Deferred, but without being so one-shot. Is this a job for
+    # Observer? No, it doesn't take extra arguments. So this uses Observer2.
+
+
+class Reader:
+    """I am responsible for a single offset+size read of the file. I handle
+    segmentation: I figure out which segments are necessary, request them
+    (from my CiphertextDownloader) in order, and trim the segments down to
+    match the offset+size span. I use the Producer/Consumer interface to only
+    request one segment at a time.
+    """
+    implements(IPushProducer)
+    def __init__(self, consumer, offset, size):
+        self._needed = []
+        self._consumer = consumer
+        self._hungry = False
+        self._offset = offset
+        self._size = size
+        self._segsize = None
+    def start(self):
+        self._alive = True
+        self._deferred = defer.Deferred()
+        # the process doesn't actually start until set_segment_size()
+        return self._deferred
+
+    def set_segment_size(self, segsize):
+        if self._segsize is not None:
+            return
+        self._segsize = segsize
+        self._compute_segnums()
+
+    def _compute_segnums(self, segsize):
+        # now that we know the file's segsize, what segments (and which
+        # ranges of each) will we need?
+        size = self._size
+        offset = self._offset
+        while size:
+            assert size >= 0
+            this_seg_num = int(offset / self._segsize)
+            this_seg_offset = offset - (seg_num*self._segsize)
+            this_seg_size = min(size, self._segsize-seg_offset)
+            size -= this_seg_size
+            if size:
+                offset += this_seg_size
+            yield (this_seg_num, this_seg_offset, this_seg_size)
+
+    def get_needed_segments(self):
+        return set([segnum for (segnum, off, size) in self._needed])
+
+
+    def stopProducing(self):
+        self._hungry = False
+        self._alive = False
+        # TODO: cancel the segment requests
+    def pauseProducing(self):
+        self._hungry = False
+    def resumeProducing(self):
+        self._hungry = True
+    def add_segment(self, segnum, offset, size):
+        self._needed.append( (segnum, offset, size) )
+    def got_segment(self, segnum, segdata):
+        """Return True if this schedule has more to go, or False if it is
+        done."""
+        assert self._needed[0][segnum] == segnum
+        (_ign, offset, size) = self._needed.pop(0)
+        data = segdata[offset:offset+size]
+        self._consumer.write(data)
+        if not self._needed:
+            # we're done
+            self._alive = False
+            self._hungry = False
+            self._consumer.unregisterProducer()
+            self._deferred.callback(self._consumer)
+    def error(self, f):
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
+
+
+
+class x:
+    def OFFread(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data."""
+        # tolerate concurrent operations: each gets its own Reader
+        if size is None:
+            size = self._size - offset
+        r = Reader(consumer, offset, size)
+        self._readers.add(r)
+        d = r.start()
+        if self.segment_size is not None:
+            r.set_segment_size(self.segment_size)
+            # TODO: if we can't find any segments, and thus never get a
+            # segsize, tell the Readers to give up
+        return d

commit 8ae03448551164c0906f64cb9c6520457d1bc8c9
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 20:51:20 2010 -0700

    move immutable/filenode.py to literal.py, since that's all that remains in it
---
 src/allmydata/immutable/filenode.py |  104 -----------------------------------
 src/allmydata/immutable/literal.py  |  104 +++++++++++++++++++++++++++++++++++
 src/allmydata/nodemaker.py          |    3 +-
 src/allmydata/test/bench_dirnode.py |    2 +-
 src/allmydata/test/test_filenode.py |    2 +-
 src/allmydata/test/test_system.py   |    2 +-
 6 files changed, 109 insertions(+), 108 deletions(-)

diff --git a/src/allmydata/immutable/filenode.py b/src/allmydata/immutable/filenode.py
deleted file mode 100644
index 09466cb..0000000
--- a/src/allmydata/immutable/filenode.py
+++ /dev/null
@@ -1,104 +0,0 @@
-from cStringIO import StringIO
-from zope.interface import implements
-from twisted.internet import defer
-from twisted.internet.interfaces import IPushProducer
-from twisted.protocols import basic
-from allmydata.interfaces import IImmutableFileNode, ICheckable
-from allmydata.uri import LiteralFileURI
-
-class _ImmutableFileNodeBase(object):
-    implements(IImmutableFileNode, ICheckable)
-
-    def get_write_uri(self):
-        return None
-
-    def get_readonly_uri(self):
-        return self.get_uri()
-
-    def is_mutable(self):
-        return False
-
-    def is_readonly(self):
-        return True
-
-    def is_unknown(self):
-        return False
-
-    def is_allowed_in_immutable_directory(self):
-        return True
-
-    def raise_error(self):
-        pass
-
-    def __hash__(self):
-        return self.u.__hash__()
-    def __eq__(self, other):
-        if isinstance(other, _ImmutableFileNodeBase):
-            return self.u.__eq__(other.u)
-        else:
-            return False
-    def __ne__(self, other):
-        if isinstance(other, _ImmutableFileNodeBase):
-            return self.u.__eq__(other.u)
-        else:
-            return True
-
-
-class LiteralProducer:
-    implements(IPushProducer)
-    def resumeProducing(self):
-        pass
-    def stopProducing(self):
-        pass
-
-
-class LiteralFileNode(_ImmutableFileNodeBase):
-
-    def __init__(self, filecap):
-        assert isinstance(filecap, LiteralFileURI)
-        self.u = filecap
-
-    def get_size(self):
-        return len(self.u.data)
-    def get_current_size(self):
-        return defer.succeed(self.get_size())
-
-    def get_cap(self):
-        return self.u
-    def get_readcap(self):
-        return self.u
-    def get_verify_cap(self):
-        return None
-    def get_repair_cap(self):
-        return None
-
-    def get_uri(self):
-        return self.u.to_string()
-
-    def get_storage_index(self):
-        return None
-
-    def check(self, monitor, verify=False, add_lease=False):
-        return defer.succeed(None)
-
-    def check_and_repair(self, monitor, verify=False, add_lease=False):
-        return defer.succeed(None)
-
-    def read(self, consumer, offset=0, size=None):
-        if size is None:
-            data = self.u.data[offset:]
-        else:
-            data = self.u.data[offset:offset+size]
-
-        # We use twisted.protocols.basic.FileSender, which only does
-        # non-streaming, i.e. PullProducer, where the receiver/consumer must
-        # ask explicitly for each chunk of data. There are only two places in
-        # the Twisted codebase that can't handle streaming=False, both of
-        # which are in the upload path for an FTP/SFTP server
-        # (protocols.ftp.FileConsumer and
-        # vfs.adapters.ftp._FileToConsumerAdapter), neither of which is
-        # likely to be used as the target for a Tahoe download.
-
-        d = basic.FileSender().beginFileTransfer(StringIO(data), consumer)
-        d.addCallback(lambda lastSent: consumer)
-        return d
diff --git a/src/allmydata/immutable/literal.py b/src/allmydata/immutable/literal.py
new file mode 100644
index 0000000..09466cb
--- /dev/null
+++ b/src/allmydata/immutable/literal.py
@@ -0,0 +1,104 @@
+from cStringIO import StringIO
+from zope.interface import implements
+from twisted.internet import defer
+from twisted.internet.interfaces import IPushProducer
+from twisted.protocols import basic
+from allmydata.interfaces import IImmutableFileNode, ICheckable
+from allmydata.uri import LiteralFileURI
+
+class _ImmutableFileNodeBase(object):
+    implements(IImmutableFileNode, ICheckable)
+
+    def get_write_uri(self):
+        return None
+
+    def get_readonly_uri(self):
+        return self.get_uri()
+
+    def is_mutable(self):
+        return False
+
+    def is_readonly(self):
+        return True
+
+    def is_unknown(self):
+        return False
+
+    def is_allowed_in_immutable_directory(self):
+        return True
+
+    def raise_error(self):
+        pass
+
+    def __hash__(self):
+        return self.u.__hash__()
+    def __eq__(self, other):
+        if isinstance(other, _ImmutableFileNodeBase):
+            return self.u.__eq__(other.u)
+        else:
+            return False
+    def __ne__(self, other):
+        if isinstance(other, _ImmutableFileNodeBase):
+            return self.u.__eq__(other.u)
+        else:
+            return True
+
+
+class LiteralProducer:
+    implements(IPushProducer)
+    def resumeProducing(self):
+        pass
+    def stopProducing(self):
+        pass
+
+
+class LiteralFileNode(_ImmutableFileNodeBase):
+
+    def __init__(self, filecap):
+        assert isinstance(filecap, LiteralFileURI)
+        self.u = filecap
+
+    def get_size(self):
+        return len(self.u.data)
+    def get_current_size(self):
+        return defer.succeed(self.get_size())
+
+    def get_cap(self):
+        return self.u
+    def get_readcap(self):
+        return self.u
+    def get_verify_cap(self):
+        return None
+    def get_repair_cap(self):
+        return None
+
+    def get_uri(self):
+        return self.u.to_string()
+
+    def get_storage_index(self):
+        return None
+
+    def check(self, monitor, verify=False, add_lease=False):
+        return defer.succeed(None)
+
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        return defer.succeed(None)
+
+    def read(self, consumer, offset=0, size=None):
+        if size is None:
+            data = self.u.data[offset:]
+        else:
+            data = self.u.data[offset:offset+size]
+
+        # We use twisted.protocols.basic.FileSender, which only does
+        # non-streaming, i.e. PullProducer, where the receiver/consumer must
+        # ask explicitly for each chunk of data. There are only two places in
+        # the Twisted codebase that can't handle streaming=False, both of
+        # which are in the upload path for an FTP/SFTP server
+        # (protocols.ftp.FileConsumer and
+        # vfs.adapters.ftp._FileToConsumerAdapter), neither of which is
+        # likely to be used as the target for a Tahoe download.
+
+        d = basic.FileSender().beginFileTransfer(StringIO(data), consumer)
+        d.addCallback(lambda lastSent: consumer)
+        return d
diff --git a/src/allmydata/nodemaker.py b/src/allmydata/nodemaker.py
index 033c34a..227f58f 100644
--- a/src/allmydata/nodemaker.py
+++ b/src/allmydata/nodemaker.py
@@ -1,6 +1,7 @@
 import weakref
 from zope.interface import implements
-from allmydata.immutable.filenode import LiteralFileNode
+from allmydata.interfaces import INodeMaker, MustBeDeepImmutableError
+from allmydata.immutable.literal import LiteralFileNode
 from allmydata.immutable.download2 import ImmutableFileNode, CiphertextFileNode
 from allmydata.immutable.upload import Data
 from allmydata.mutable.filenode import MutableFileNode
diff --git a/src/allmydata/test/bench_dirnode.py b/src/allmydata/test/bench_dirnode.py
index ffc6637..6fa1563 100644
--- a/src/allmydata/test/bench_dirnode.py
+++ b/src/allmydata/test/bench_dirnode.py
@@ -6,7 +6,7 @@ from zope.interface import implements
 from allmydata import dirnode, uri
 from allmydata.interfaces import IFileNode
 from allmydata.mutable.filenode import MutableFileNode
-from allmydata.immutable.filenode import ImmutableFileNode
+from allmydata.immutable.download2 import ImmutableFileNode
 
 class ContainerNode:
     implements(IFileNode)
diff --git a/src/allmydata/test/test_filenode.py b/src/allmydata/test/test_filenode.py
index cd9a561..5d86203 100644
--- a/src/allmydata/test/test_filenode.py
+++ b/src/allmydata/test/test_filenode.py
@@ -2,7 +2,7 @@
 from twisted.trial import unittest
 from allmydata import uri, client
 from allmydata.monitor import Monitor
-from allmydata.immutable.filenode import LiteralFileNode
+from allmydata.immutable.literal import LiteralFileNode
 from allmydata.immutable.download2 import ImmutableFileNode
 from allmydata.mutable.filenode import MutableFileNode
 from allmydata.util import hashutil
diff --git a/src/allmydata/test/test_system.py b/src/allmydata/test/test_system.py
index 3c0f143..b7d2fc1 100644
--- a/src/allmydata/test/test_system.py
+++ b/src/allmydata/test/test_system.py
@@ -9,7 +9,7 @@ from allmydata import uri
 from allmydata.storage.mutable import MutableShareFile
 from allmydata.storage.server import si_a2b
 from allmydata.immutable import offloaded, upload
-from allmydata.immutable.filenode import LiteralFileNode
+from allmydata.immutable.literal import LiteralFileNode
 from allmydata.immutable.download2 import ImmutableFileNode
 from allmydata.util import idlib, mathutil
 from allmydata.util import log, base32

commit db840c98079f158f0249c4dfd0580fc732462d2b
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 20:40:34 2010 -0700

    oops, moving Terminator to downloader.util needs updating client.py
---
 src/allmydata/client.py |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/client.py b/src/allmydata/client.py
index b01fbe8..d3ae29b 100644
--- a/src/allmydata/client.py
+++ b/src/allmydata/client.py
@@ -12,7 +12,7 @@ import allmydata
 from allmydata.storage.server import StorageServer
 from allmydata import storage_client
 from allmydata.immutable.upload import Uploader
-from allmydata.immutable.download2_util import Terminator
+from allmydata.immutable.downloader.util import Terminator
 from allmydata.immutable.offloaded import Helper
 from allmydata.control import ControlServer
 from allmydata.introducer.client import IntroducerClient

commit 2c40602fd8456242370124498660b8f5b9c1b4b0
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 19:10:01 2010 -0700

    storage-overhead: try to fix, probably still broken
---
 misc/simulators/storage-overhead.py |   10 ++++++----
 1 files changed, 6 insertions(+), 4 deletions(-)

diff --git a/misc/simulators/storage-overhead.py b/misc/simulators/storage-overhead.py
index 75a0bf6..a294b8d 100644
--- a/misc/simulators/storage-overhead.py
+++ b/misc/simulators/storage-overhead.py
@@ -1,7 +1,9 @@
 #!/usr/bin/env python
 
 import sys, math
-from allmydata import upload, uri, encode, storage
+from allmydata import uri, storage
+from allmydata.immutable import upload
+from allmydata.interfaces import DEFAULT_MAX_SEGMENT_SIZE
 from allmydata.util import mathutil
 
 def roundup(size, blocksize=4096):
@@ -22,14 +24,14 @@ class BigFakeString:
     def tell(self):
         return self.fp
 
-def calc(filesize, params=(3,7,10), segsize=encode.Encoder.MAX_SEGMENT_SIZE):
+def calc(filesize, params=(3,7,10), segsize=DEFAULT_MAX_SEGMENT_SIZE):
     num_shares = params[2]
     if filesize <= upload.Uploader.URI_LIT_SIZE_THRESHOLD:
-        urisize = len(uri.pack_lit("A"*filesize))
+        urisize = len(uri.LiteralFileURI("A"*filesize).to_string())
         sharesize = 0
         sharespace = 0
     else:
-        u = upload.FileUploader(None)
+        u = upload.FileUploader(None) # XXX changed
         u.set_params(params)
         # unfortunately, Encoder doesn't currently lend itself to answering
         # this question without measuring a filesize, so we have to give it a

commit 6a4e15520d6ea0284155ba61613aa778bee2f200
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 19:09:43 2010 -0700

    test_immutable comments: NEEDS WORK
---
 src/allmydata/test/test_immutable.py |    4 ++++
 1 files changed, 4 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/test/test_immutable.py b/src/allmydata/test/test_immutable.py
index 63fb7ca..a61c058 100644
--- a/src/allmydata/test/test_immutable.py
+++ b/src/allmydata/test/test_immutable.py
@@ -51,6 +51,7 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
         before_download_reads = self._count_reads()
         def _after_download(unused=None):
             after_download_reads = self._count_reads()
+            #print before_download_reads, after_download_reads
             self.failIf(after_download_reads-before_download_reads > 27,
                         (after_download_reads, before_download_reads))
         d.addCallback(self._download_and_check_plaintext)
@@ -68,6 +69,7 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
         d.addCallback(_then_delete_7)
         def _after_download(unused=None):
             after_download_reads = self._count_reads()
+            #print before_download_reads, after_download_reads
             self.failIf(after_download_reads-before_download_reads > 27, (after_download_reads, before_download_reads))
         d.addCallback(self._download_and_check_plaintext)
         d.addCallback(_after_download)
@@ -126,6 +128,7 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
 
         def _after_attempt(unused=None):
             after_download_reads = self._count_reads()
+            #print before_download_reads, after_download_reads
             # To pass this test, you are required to give up before reading
             # all of the share data. Actually, we could give up sooner than
             # 45 reads, but currently our download code does 45 reads. This
@@ -143,3 +146,4 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
 
 # XXX test disconnect DeadReferenceError from get_buckets and get_block_whatsit
 
+# TODO: delete this whole file

commit 82163900e63089121583a0d272495427a8548c50
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 19:09:21 2010 -0700

    disable test_hung_server, NEEDS WORK
---
 src/allmydata/test/test_hung_server.py |    7 +++++--
 1 files changed, 5 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/test/test_hung_server.py b/src/allmydata/test/test_hung_server.py
index a4f53f8..948fe9f 100644
--- a/src/allmydata/test/test_hung_server.py
+++ b/src/allmydata/test/test_hung_server.py
@@ -23,6 +23,7 @@ class HungServerDownloadTest(GridTestMixin, ShouldFailMixin, unittest.TestCase):
     # MM's buildslave varies a lot in how long it takes to run tests.
 
     timeout = 240
+    skip="not ready"
 
     def _break(self, servers):
         for (id, ss) in servers:
@@ -113,7 +114,8 @@ class HungServerDownloadTest(GridTestMixin, ShouldFailMixin, unittest.TestCase):
             stage_4_d = None # currently we aren't doing any tests which require this for mutable files
         else:
             d = download_to_data(n)
-            stage_4_d = n._downloader._all_downloads.keys()[0]._stage_4_d # too ugly! FIXME
+            #stage_4_d = n._downloader._all_downloads.keys()[0]._stage_4_d # too ugly! FIXME
+            stage_4_d = None
         return (d, stage_4_d,)
 
     def _wait_for_data(self, n):
@@ -141,7 +143,7 @@ class HungServerDownloadTest(GridTestMixin, ShouldFailMixin, unittest.TestCase):
                                    self._download_and_check)
         else:
             return self.shouldFail(NotEnoughSharesError, self.basedir,
-                                   "Failed to get enough shareholders",
+                                   "ran out of shares",
                                    self._download_and_check)
 
 
@@ -234,6 +236,7 @@ class HungServerDownloadTest(GridTestMixin, ShouldFailMixin, unittest.TestCase):
         return d
 
     def test_failover_during_stage_4(self):
+        raise unittest.SkipTest("needs rewrite")
         # See #287
         d = defer.succeed(None)
         for mutable in [False]:

commit d937f3964fdf18f5379bb207b9a953889d941847
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 19:07:00 2010 -0700

    move old stuff out of the tree
---
 src/allmydata/immutable/download2_off.pyOFF |  634 ---------------------------
 1 files changed, 0 insertions(+), 634 deletions(-)

diff --git a/src/allmydata/immutable/download2_off.pyOFF b/src/allmydata/immutable/download2_off.pyOFF
deleted file mode 100755
index d2b8b99..0000000
--- a/src/allmydata/immutable/download2_off.pyOFF
+++ /dev/null
@@ -1,634 +0,0 @@
-#! /usr/bin/python
-
-# known (shnum,Server) pairs are sorted into a list according to
-# desireability. This sort is picking a winding path through a matrix of
-# [shnum][server]. The goal is to get diversity of both shnum and server.
-
-# The initial order is:
-#  find the lowest shnum on the first server, add it
-#  look at the next server, find the lowest shnum that we don't already have
-#   if any
-#  next server, etc, until all known servers are checked
-#  now look at servers that we skipped (because ...
-
-# Keep track of which block requests are outstanding by (shnum,Server). Don't
-# bother prioritizing "validated" shares: the overhead to pull the share hash
-# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
-# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
-# 832 bytes). Each time a block request is sent, also request any necessary
-# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
-# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
-
-# For each outstanding segread, walk the list and send requests (skipping
-# outstanding shnums) until requests for k distinct shnums are in flight. If
-# we can't do that, ask for more. If we get impatient on a request, find the
-# first non-outstanding
-
-# start with the first Share in the list, and send a request. Then look at
-# the next one. If we already have a pending request for the same shnum or
-# server, push that Share down onto the fallback list and try the next one,
-# etc. If we run out of non-fallback shares, use the fallback ones,
-# preferring shnums that we don't have outstanding requests for (i.e. assume
-# that all requests will complete). Do this by having a second fallback list.
-
-# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
-# per file, not thousands.
-
-# actually, don't bother sorting the initial list. Append Shares as the
-# responses come back, that will put the fastest servers at the front of the
-# list, and give a tiny preference to servers that are earlier in the
-# permuted order.
-
-# more ideas:
-#  sort shares by:
-#   1: number of roundtrips needed to get some data
-#   2: share number
-#   3: ms of RTT delay
-# maybe measure average time-to-completion of requests, compare completion
-# time against that, much larger indicates congestion on the server side
-# or the server's upstream speed is less than our downstream. Minimum
-# time-to-completion indicates min(our-downstream,their-upstream). Could
-# fetch shares one-at-a-time to measure that better.
-
-# when should we risk duplicate work and send a new request?
-
-def walk(self):
-    shares = sorted(list)
-    oldshares = copy(shares)
-    outstanding = list()
-    fallbacks = list()
-    second_fallbacks = list()
-    while len(outstanding.nonlate.shnums) < k: # need more requests
-        while oldshares:
-            s = shares.pop(0)
-            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
-                fallbacks.append(s)
-                continue
-            outstanding.append(s)
-            send_request(s)
-            break #'while need_more_requests'
-        # must use fallback list. Ask for more servers while we're at it.
-        ask_for_more_servers()
-        while fallbacks:
-            s = fallbacks.pop(0)
-            if s.shnum in outstanding.shnums:
-                # assume that the outstanding requests will complete, but
-                # send new requests for other shnums to existing servers
-                second_fallbacks.append(s)
-                continue
-            outstanding.append(s)
-            send_request(s)
-            break #'while need_more_requests'
-        # if we get here, we're being forced to send out multiple queries per
-        # share. We've already asked for more servers, which might help. If
-        # there are no late outstanding queries, then duplicate shares won't
-        # help. Don't send queries for duplicate shares until some of the
-        # queries are late.
-        if outstanding.late:
-            # we're allowed to try any non-outstanding share
-            while second_fallbacks:
-                pass
-    newshares = outstanding + fallbacks + second_fallbacks + oldshares
-        
-
-class Server:
-    """I represent an abstract Storage Server. One day, the StorageBroker
-    will return instances of me. For now, the StorageBroker returns (peerid,
-    RemoteReference) tuples, and this code wraps a Server instance around
-    them.
-    """
-    def __init__(self, peerid, ss):
-        self.peerid = peerid
-        self.remote = ss
-        self._remote_buckets = {} # maps shnum to RIBucketReader
-        # TODO: release the bucket references on shares that we no longer
-        # want. OTOH, why would we not want them? Corruption?
-
-    def send_query(self, storage_index):
-        """I return a Deferred that fires with a set of shnums. If the server
-        had shares available, I will retain the RemoteReferences to its
-        buckets, so that get_data(shnum, range) can be called later."""
-        d = self.remote.callRemote("get_buckets", self.storage_index)
-        d.addCallback(self._got_response)
-        return d
-
-    def _got_response(self, r):
-        self._remote_buckets = r
-        return set(r.keys())
-
-class ShareOnAServer:
-    """I represent one instance of a share, known to live on a specific
-    server. I am created every time a server responds affirmatively to a
-    do-you-have-block query."""
-
-    def __init__(self, shnum, server):
-        self._shnum = shnum
-        self._server = server
-        self._block_hash_tree = None
-
-    def cost(self, segnum):
-        """I return a tuple of (roundtrips, bytes, rtt), indicating how
-        expensive I think it would be to fetch the given segment. Roundtrips
-        indicates how many roundtrips it is likely to take (one to get the
-        data and hashes, plus one to get the offset table and UEB if this is
-        the first segment we've ever fetched). 'bytes' is how many bytes we
-        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
-        seconds for a trivial request. The downloading algorithm will compare
-        costs to decide which shares should be used."""
-        # the most significant factor here is roundtrips: a Share for which
-        # we already have the offset table is better to than a brand new one
-
-    def max_bandwidth(self):
-        """Return a float, indicating the highest plausible bytes-per-second
-        that I've observed coming from this share. This will be based upon
-        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
-        can we used to estimate the server's upstream bandwidth. Clearly this
-        is only accurate if a share is retrieved with no contention for
-        either the upstream, downstream, or middle of the connection, but it
-        may still serve as a useful metric for deciding which servers to pull
-        from."""
-
-    def get_segment(self, segnum):
-        """I return a Deferred that will fire with the segment data, or
-        errback."""
-
-class NativeShareOnAServer(ShareOnAServer):
-    """For tahoe native (foolscap) servers, I contain a RemoteReference to
-    the RIBucketReader instance."""
-    def __init__(self, shnum, server, rref):
-        ShareOnAServer.__init__(self, shnum, server)
-        self._rref = rref # RIBucketReader
-
-class Share:
-    def __init__(self, shnum):
-        self._shnum = shnum
-        # _servers are the Server instances which appear to hold a copy of
-        # this share. It is populated when the ValidShare is first created,
-        # or when we receive a get_buckets() response for a shnum that
-        # already has a ValidShare instance. When we lose the connection to a
-        # server, we remove it.
-        self._servers = set()
-        # offsets, UEB, and share_hash_tree all live in the parent.
-        # block_hash_tree lives here.
-        self._block_hash_tree = None
-
-        self._want
-
-    def get_servers(self):
-        return self._servers
-
-
-    def get_block(self, segnum):
-        # read enough data to obtain a single validated block
-        if not self.have_offsets:
-            # we get the offsets in their own read, since they tell us where
-            # everything else lives. We must fetch offsets for each share
-            # separately, since they aren't directly covered by the UEB.
-            pass
-        if not self.parent.have_ueb:
-            # use _guessed_segsize to make a guess about the layout, so we
-            # can fetch both the offset table and the UEB in the same read.
-            # This also requires making a guess about the presence or absence
-            # of the plaintext_hash_tree. Oh, and also the version number. Oh
-            # well.
-            pass
-
-class CiphertextDownloader:
-    """I manage all downloads for a single file. I operate a state machine
-    with input events that are local read() requests, responses to my remote
-    'get_bucket' and 'read_bucket' messages, and connection establishment and
-    loss. My outbound events are connection establishment requests and bucket
-    read requests messages.
-    """
-    # eventually this will merge into the FileNode
-    ServerClass = Server # for tests to override
-
-    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
-                 shutdowner):
-        # values we get from the filecap
-        self._storage_index = si = storage_index
-        self._ueb_hash = ueb_hash
-        self._size = size
-        self._needed_shares = k
-        self._total_shares = N
-        self._share_hash_tree = IncompleteHashTree(self._total_shares)
-        # values we discover when we first fetch the UEB
-        self._ueb = None # is dict after UEB fetch+validate
-        self._segsize = None
-        self._numsegs = None
-        self._blocksize = None
-        self._tail_segsize = None
-        self._ciphertext_hash = None # optional
-        # structures we create when we fetch the UEB, then continue to fill
-        # as we download the file
-        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
-        self._ciphertext_hash_tree = None
-
-        # values we learn as we download the file
-        self._offsets = {} # (shnum,Server) to offset table (dict)
-        self._block_hash_tree = {} # shnum to IncompleteHashTree
-        # other things which help us
-        self._guessed_segsize = min(128*1024, size)
-        self._active_share_readers = {} # maps shnum to Reader instance
-        self._share_readers = [] # sorted by preference, best first
-        self._readers = set() # set of Reader instances
-        self._recent_horizon = 10 # seconds
-
-        # 'shutdowner' is a MultiService parent used to cancel all downloads
-        # when the node is shutting down, to let tests have a clean reactor.
-
-        self._init_available_servers()
-        self._init_find_enough_shares()
-
-    # _available_servers is an iterator that provides us with Server
-    # instances. Each time we pull out a Server, we immediately send it a
-    # query, so we don't need to keep track of who we've sent queries to.
-
-    def _init_available_servers(self):
-        self._available_servers = self._get_available_servers()
-        self._no_more_available_servers = False
-
-    def _get_available_servers(self):
-        """I am a generator of servers to use, sorted by the order in which
-        we should query them. I make sure there are no duplicates in this
-        list."""
-        # TODO: make StorageBroker responsible for this non-duplication, and
-        # replace this method with a simple iter(get_servers_for_index()),
-        # plus a self._no_more_available_servers=True
-        seen = set()
-        sb = self._storage_broker
-        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
-            if peerid not in seen:
-                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
-                seen.add(peerid)
-        self._no_more_available_servers = True
-
-    # this block of code is responsible for having enough non-problematic
-    # distinct shares/servers available and ready for download, and for
-    # limiting the number of queries that are outstanding. The idea is that
-    # we'll use the k fastest/best shares, and have the other ones in reserve
-    # in case those servers stop responding or respond too slowly. We keep
-    # track of all known shares, but we also keep track of problematic shares
-    # (ones with hash failures or lost connections), so we can put them at
-    # the bottom of the list.
-
-    def _init_find_enough_shares(self):
-        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
-        # where viable (but not yet validated) shares are located. Each
-        # get_bucket() response adds to this map, each act of validation
-        # removes from it.
-        self._sharemap = DictOfSets()
-
-        # _sharemap maps shnum to set of Servers, and remembers where viable
-        # shares are located. Each get_bucket() response adds to this map,
-        # each hash failure or disconnect removes from it. (TODO: if we
-        # disconnect but reconnect later, we should be allowed to re-query).
-        self._sharemap = DictOfSets()
-
-        # _problem_shares is a set of (shnum, Server) tuples, and
-
-        # _queries_in_flight maps a Server to a timestamp, which remembers
-        # which servers we've sent queries to (and when) but have not yet
-        # heard a response. This lets us put a limit on the number of
-        # outstanding queries, to limit the size of the work window (how much
-        # extra work we ask servers to do in the hopes of keeping our own
-        # pipeline filled). We remove a Server from _queries_in_flight when
-        # we get an answer/error or we finally give up. If we ever switch to
-        # a non-connection-oriented protocol (like UDP, or forwarded Chord
-        # queries), we can use this information to retransmit any query that
-        # has gone unanswered for too long.
-        self._queries_in_flight = dict()
-
-    def _count_recent_queries_in_flight(self):
-        now = time.time()
-        recent = now - self._recent_horizon
-        return len([s for (s,when) in self._queries_in_flight.items()
-                    if when > recent])
-
-    def _find_enough_shares(self):
-        # goal: have 2*k distinct not-invalid shares available for reading,
-        # from 2*k distinct servers. Do not have more than 4*k "recent"
-        # queries in flight at a time.
-        if (len(self._sharemap) >= 2*self._needed_shares
-            and len(self._sharemap.values) >= 2*self._needed_shares):
-            return
-        num = self._count_recent_queries_in_flight()
-        while num < 4*self._needed_shares:
-            try:
-                s = self._available_servers.next()
-            except StopIteration:
-                return # no more progress can be made
-            self._queries_in_flight[s] = time.time()
-            d = s.send_query(self._storage_index)
-            d.addBoth(incidentally, self._queries_in_flight.discard, s)
-            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
-                                           for shnum in shnums],
-                           lambda f: self._query_error(f, s))
-            d.addErrback(self._error)
-            d.addCallback(self._reschedule)
-            num += 1
-
-    def _query_error(self, f, s):
-        # a server returned an error, log it gently and ignore
-        level = log.WEIRD
-        if f.check(DeadReferenceError):
-            level = log.UNUSUAL
-        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
-                failure=f, level=level, umid="3uuBUQ")
-
-    # this block is responsible for turning known shares into usable shares,
-    # by fetching enough data to validate their contents.
-
-    # UEB (from any share)
-    # share hash chain, validated (from any share, for given shnum)
-    # block hash (any share, given shnum)
-
-    def _got_ueb(self, ueb_data, share):
-        if self._ueb is not None:
-            return
-        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
-            share.error("UEB hash does not match")
-            return
-        d = uri.unpack_extension(ueb_data)
-        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
-
-
-        # There are several kinds of things that can be found in a UEB.
-        # First, things that we really need to learn from the UEB in order to
-        # do this download. Next: things which are optional but not redundant
-        # -- if they are present in the UEB they will get used. Next, things
-        # that are optional and redundant. These things are required to be
-        # consistent: they don't have to be in the UEB, but if they are in
-        # the UEB then they will be checked for consistency with the
-        # already-known facts, and if they are inconsistent then an exception
-        # will be raised. These things aren't actually used -- they are just
-        # tested for consistency and ignored. Finally: things which are
-        # deprecated -- they ought not be in the UEB at all, and if they are
-        # present then a warning will be logged but they are otherwise
-        # ignored.
-
-        # First, things that we really need to learn from the UEB:
-        # segment_size, crypttext_root_hash, and share_root_hash.
-        self._segsize = d['segment_size']
-
-        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
-        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
-
-        self._tail_segsize = self._size % self._segsize
-        if self._tail_segsize == 0:
-            self._tail_segsize = self._segsize
-        # padding for erasure code
-        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
-                                                    self._needed_shares)
-
-        # Ciphertext hash tree root is mandatory, so that there is at most
-        # one ciphertext that matches this read-cap or verify-cap. The
-        # integrity check on the shares is not sufficient to prevent the
-        # original encoder from creating some shares of file A and other
-        # shares of file B.
-        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
-        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
-
-        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
-
-
-        # Next: things that are optional and not redundant: crypttext_hash
-        if 'crypttext_hash' in d:
-            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
-                self._ciphertext_hash = d['crypttext_hash']
-            else:
-                log.msg("ignoring bad-length UEB[crypttext_hash], "
-                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
-                                                   hashutil.CRYPTO_VAL_SIZE),
-                        umid="oZkGLA", level=log.WEIRD)
-
-        # we ignore all of the redundant fields when downloading. The
-        # Verifier uses a different code path which does not ignore them.
-
-        # finally, set self._ueb as a marker that we don't need to request it
-        # anymore
-        self._ueb = d
-
-    def _got_share_hashes(self, hashes, share):
-        assert isinstance(hashes, dict)
-        try:
-            self._share_hash_tree.set_hashes(hashes)
-        except (IndexError, BadHashError, NotEnoughHashesError), le:
-            share.error("Bad or missing hashes")
-            return
-
-    #def _got_block_hashes(
-
-    def _init_validate_enough_shares(self):
-        # _valid_shares maps shnum to ValidatedShare instances, and is
-        # populated once the block hash root has been fetched and validated
-        # (which requires any valid copy of the UEB, and a valid copy of the
-        # share hash chain for each shnum)
-        self._valid_shares = {}
-
-        # _target_shares is an ordered list of ReadyShare instances, each of
-        # which is a (shnum, server) tuple. It is sorted in order of
-        # preference: we expect to get the fastest response from the
-        # ReadyShares at the front of the list. It is also sorted to
-        # distribute the shnums, so that fetching shares from
-        # _target_shares[:k] is likely (but not guaranteed) to give us k
-        # distinct shares. The rule is that we skip over entries for blocks
-        # that we've already received, limit the number of recent queries for
-        # the same block, 
-        self._target_shares = []
-
-    def _validate_enough_shares(self):
-        # my goal is to have at least 2*k distinct validated shares from at
-        # least 2*k distinct servers
-        valid_share_servers = set()
-        for vs in self._valid_shares.values():
-            valid_share_servers.update(vs.get_servers())
-        if (len(self._valid_shares) >= 2*self._needed_shares
-            and len(self._valid_share_servers) >= 2*self._needed_shares):
-            return
-        #for 
-
-    def _reschedule(self, _ign):
-        # fire the loop again
-        if not self._scheduled:
-            self._scheduled = True
-            eventually(self._loop)
-
-    def _loop(self):
-        self._scheduled = False
-        # what do we need?
-
-        self._find_enough_shares()
-        self._validate_enough_shares()
-
-        if not self._ueb:
-            # we always need a copy of the UEB
-            pass
-
-    def _error(self, f):
-        # this is an unexpected error: a coding bug
-        log.err(f, level=log.UNUSUAL)
-            
-
-
-# using a single packed string (and an offset table) may be an artifact of
-# our native storage server: other backends might allow cheap multi-part
-# files (think S3, several buckets per share, one for each section).
-
-# find new names for:
-#  data_holder
-#  Share / Share2  (ShareInstance / Share? but the first is more useful)
-
-class IShare(Interface):
-    """I represent a single instance of a single share (e.g. I reference the
-    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
-    This interface is used by SegmentFetcher to retrieve validated blocks.
-    """
-    def get_block(segnum):
-        """Return an Observer2, which will be notified with the following
-        events:
-         state=COMPLETE, block=data (terminal): validated block data
-         state=OVERDUE (non-terminal): we have reason to believe that the
-                                       request might have stalled, or we
-                                       might just be impatient
-         state=CORRUPT (terminal): the data we received was corrupt
-         state=DEAD (terminal): the connection has failed
-        """
-
-
-# it'd be nice if we receive the hashes before the block, or just
-# afterwards, so we aren't stuck holding on to unvalidated blocks
-# that we can't process. If we guess the offsets right, we can
-# accomplish this by sending the block request after the metadata
-# requests (by keeping two separate requestlists), and have a one RTT
-# pipeline like:
-#  1a=metadata, 1b=block
-#  1b->process+deliver : one RTT
-
-# But if we guess wrong, and fetch the wrong part of the block, we'll
-# have a pipeline that looks like:
-#  1a=wrong metadata, 1b=wrong block
-#  1a->2a=right metadata,2b=right block
-#  2b->process+deliver
-# which means two RTT and buffering one block (which, since we'll
-# guess the segsize wrong for everything, means buffering one
-# segment)
-
-# if we start asking for multiple segments, we could get something
-# worse:
-#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
-#  1a->2a=right metadata,2b=right block0,2c=right block1, .
-#  2b->process+deliver
-
-# which means two RTT but fetching and buffering the whole file
-# before delivering anything. However, since we don't know when the
-# other shares are going to arrive, we need to avoid having more than
-# one block in the pipeline anyways. So we shouldn't be able to get
-# into this state.
-
-# it also means that, instead of handling all of
-# self._requested_blocks at once, we should only be handling one
-# block at a time: one of the requested block should be special
-# (probably FIFO). But retire all we can.
-
-    # this might be better with a Deferred, using COMPLETE as the success
-    # case and CORRUPT/DEAD in an errback, because that would let us hold the
-    # 'share' and 'shnum' arguments locally (instead of roundtripping them
-    # through Share.send_request). But that OVERDUE is not terminal. So I
-    # want a new sort of callback mechanism, with the extra-argument-passing
-    # aspects of Deferred, but without being so one-shot. Is this a job for
-    # Observer? No, it doesn't take extra arguments. So this uses Observer2.
-
-
-class Reader:
-    """I am responsible for a single offset+size read of the file. I handle
-    segmentation: I figure out which segments are necessary, request them
-    (from my CiphertextDownloader) in order, and trim the segments down to
-    match the offset+size span. I use the Producer/Consumer interface to only
-    request one segment at a time.
-    """
-    implements(IPushProducer)
-    def __init__(self, consumer, offset, size):
-        self._needed = []
-        self._consumer = consumer
-        self._hungry = False
-        self._offset = offset
-        self._size = size
-        self._segsize = None
-    def start(self):
-        self._alive = True
-        self._deferred = defer.Deferred()
-        # the process doesn't actually start until set_segment_size()
-        return self._deferred
-
-    def set_segment_size(self, segsize):
-        if self._segsize is not None:
-            return
-        self._segsize = segsize
-        self._compute_segnums()
-
-    def _compute_segnums(self, segsize):
-        # now that we know the file's segsize, what segments (and which
-        # ranges of each) will we need?
-        size = self._size
-        offset = self._offset
-        while size:
-            assert size >= 0
-            this_seg_num = int(offset / self._segsize)
-            this_seg_offset = offset - (seg_num*self._segsize)
-            this_seg_size = min(size, self._segsize-seg_offset)
-            size -= this_seg_size
-            if size:
-                offset += this_seg_size
-            yield (this_seg_num, this_seg_offset, this_seg_size)
-
-    def get_needed_segments(self):
-        return set([segnum for (segnum, off, size) in self._needed])
-
-
-    def stopProducing(self):
-        self._hungry = False
-        self._alive = False
-        # TODO: cancel the segment requests
-    def pauseProducing(self):
-        self._hungry = False
-    def resumeProducing(self):
-        self._hungry = True
-    def add_segment(self, segnum, offset, size):
-        self._needed.append( (segnum, offset, size) )
-    def got_segment(self, segnum, segdata):
-        """Return True if this schedule has more to go, or False if it is
-        done."""
-        assert self._needed[0][segnum] == segnum
-        (_ign, offset, size) = self._needed.pop(0)
-        data = segdata[offset:offset+size]
-        self._consumer.write(data)
-        if not self._needed:
-            # we're done
-            self._alive = False
-            self._hungry = False
-            self._consumer.unregisterProducer()
-            self._deferred.callback(self._consumer)
-    def error(self, f):
-        self._alive = False
-        self._hungry = False
-        self._consumer.unregisterProducer()
-        self._deferred.errback(f)
-
-
-
-class x:
-    def OFFread(self, consumer, offset=0, size=None):
-        """I am the main entry point, from which FileNode.read() can get
-        data."""
-        # tolerate concurrent operations: each gets its own Reader
-        if size is None:
-            size = self._size - offset
-        r = Reader(consumer, offset, size)
-        self._readers.add(r)
-        d = r.start()
-        if self.segment_size is not None:
-            r.set_segment_size(self.segment_size)
-            # TODO: if we can't find any segments, and thus never get a
-            # segsize, tell the Readers to give up
-        return d

commit 118fe48bbb136b891fde1ed7b24a99a3153ccaf7
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 19:02:34 2010 -0700

    old immutable/filenode.py: remove old ImmutableFileNode, support classes
---
 src/allmydata/immutable/filenode.py |  275 +----------------------------------
 src/allmydata/test/test_filenode.py |    2 +-
 2 files changed, 3 insertions(+), 274 deletions(-)

diff --git a/src/allmydata/immutable/filenode.py b/src/allmydata/immutable/filenode.py
index 70044a7..09466cb 100644
--- a/src/allmydata/immutable/filenode.py
+++ b/src/allmydata/immutable/filenode.py
@@ -1,18 +1,10 @@
-import copy, os.path, stat
 from cStringIO import StringIO
 from zope.interface import implements
 from twisted.internet import defer
 from twisted.internet.interfaces import IPushProducer
 from twisted.protocols import basic
-from foolscap.api import eventually
-from allmydata.interfaces import IImmutableFileNode, ICheckable, \
-     IDownloadTarget, IUploadResults
-from allmydata.util import dictutil, log, base32
-from allmydata.uri import CHKFileURI, LiteralFileURI
-from allmydata.immutable.checker import Checker
-from allmydata.check_results import CheckResults, CheckAndRepairResults
-from allmydata.immutable.repairer import Repairer
-from allmydata.immutable import download
+from allmydata.interfaces import IImmutableFileNode, ICheckable
+from allmydata.uri import LiteralFileURI
 
 class _ImmutableFileNodeBase(object):
     implements(IImmutableFileNode, ICheckable)
@@ -51,269 +43,6 @@ class _ImmutableFileNodeBase(object):
         else:
             return True
 
-class PortionOfFile:
-    # like a list slice (things[2:14]), but for a file on disk
-    def __init__(self, fn, offset=0, size=None):
-        self.f = open(fn, "rb")
-        self.f.seek(offset)
-        self.bytes_left = size
-
-    def read(self, size=None):
-        # bytes_to_read = min(size, self.bytes_left), but None>anything
-        if size is None:
-            bytes_to_read = self.bytes_left
-        elif self.bytes_left is None:
-            bytes_to_read = size
-        else:
-            bytes_to_read = min(size, self.bytes_left)
-        data = self.f.read(bytes_to_read)
-        if self.bytes_left is not None:
-            self.bytes_left -= len(data)
-        return data
-
-class DownloadCache:
-    implements(IDownloadTarget)
-
-    def __init__(self, filecap, storage_index, downloader,
-                 cachedirectorymanager):
-        self._downloader = downloader
-        self._uri = filecap
-        self._storage_index = storage_index
-        self.milestones = set() # of (offset,size,Deferred)
-        self.cachedirectorymanager = cachedirectorymanager
-        self.cachefile = None
-        self.download_in_progress = False
-        # five states:
-        #  new ImmutableFileNode, no downloads ever performed
-        #  new ImmutableFileNode, leftover file (partial)
-        #  new ImmutableFileNode, leftover file (whole)
-        #  download in progress, not yet complete
-        #  download complete
-
-    def when_range_available(self, offset, size):
-        assert isinstance(offset, (int,long))
-        assert isinstance(size, (int,long))
-
-        d = defer.Deferred()
-        self.milestones.add( (offset,size,d) )
-        self._check_milestones()
-        if self.milestones and not self.download_in_progress:
-            self.download_in_progress = True
-            log.msg(format=("immutable filenode read [%(si)s]: " +
-                            "starting download"),
-                    si=base32.b2a(self._storage_index),
-                    umid="h26Heg", level=log.OPERATIONAL)
-            d2 = self._downloader.download(self._uri, self)
-            d2.addBoth(self._download_done)
-            d2.addErrback(self._download_failed)
-            d2.addErrback(log.err, umid="cQaM9g")
-        return d
-
-    def read(self, consumer, offset, size):
-        assert offset+size <= self.get_filesize()
-        if not self.cachefile:
-            self.cachefile = self.cachedirectorymanager.get_file(base32.b2a(self._storage_index))
-        f = PortionOfFile(self.cachefile.get_filename(), offset, size)
-        d = basic.FileSender().beginFileTransfer(f, consumer)
-        d.addCallback(lambda lastSent: consumer)
-        return d
-
-    def _download_done(self, res):
-        # clear download_in_progress, so failed downloads can be re-tried
-        self.download_in_progress = False
-        return res
-
-    def _download_failed(self, f):
-        # tell anyone who's waiting that we failed
-        for m in self.milestones:
-            (offset,size,d) = m
-            eventually(d.errback, f)
-        self.milestones.clear()
-
-    def _check_milestones(self):
-        current_size = self.get_filesize()
-        for m in list(self.milestones):
-            (offset,size,d) = m
-            if offset+size <= current_size:
-                log.msg(format=("immutable filenode read [%(si)s] " +
-                                "%(offset)d+%(size)d vs %(filesize)d: " +
-                                "done"),
-                        si=base32.b2a(self._storage_index),
-                        offset=offset, size=size, filesize=current_size,
-                        umid="nuedUg", level=log.NOISY)
-                self.milestones.discard(m)
-                eventually(d.callback, None)
-            else:
-                log.msg(format=("immutable filenode read [%(si)s] " +
-                                "%(offset)d+%(size)d vs %(filesize)d: " +
-                                "still waiting"),
-                        si=base32.b2a(self._storage_index),
-                        offset=offset, size=size, filesize=current_size,
-                        umid="8PKOhg", level=log.NOISY)
-
-    def get_filesize(self):
-        if not self.cachefile:
-            self.cachefile = self.cachedirectorymanager.get_file(base32.b2a(self._storage_index))
-        try:
-            filesize = os.stat(self.cachefile.get_filename())[stat.ST_SIZE]
-        except OSError:
-            filesize = 0
-        return filesize
-
-
-    def open(self, size):
-        if not self.cachefile:
-            self.cachefile = self.cachedirectorymanager.get_file(base32.b2a(self._storage_index))
-        self.f = open(self.cachefile.get_filename(), "wb")
-
-    def write(self, data):
-        self.f.write(data)
-        self._check_milestones()
-
-    def close(self):
-        self.f.close()
-        self._check_milestones()
-
-    def fail(self, why):
-        pass
-    def register_canceller(self, cb):
-        pass
-    def finish(self):
-        return None
-    # The following methods are just because the target might be a
-    # repairer.DownUpConnector, and just because the current CHKUpload object
-    # expects to find the storage index and encoding parameters in its
-    # Uploadable.
-    def set_storageindex(self, storageindex):
-        pass
-    def set_encodingparams(self, encodingparams):
-        pass
-
-
-class ImmutableFileNode(_ImmutableFileNodeBase, log.PrefixingLogMixin):
-    def __init__(self, filecap, storage_broker, secret_holder,
-                 downloader, history, cachedirectorymanager):
-        assert isinstance(filecap, CHKFileURI)
-        self.u = filecap
-        self._storage_broker = storage_broker
-        self._secret_holder = secret_holder
-        self._downloader = downloader
-        self._history = history
-        storage_index = self.get_storage_index()
-        self.download_cache = DownloadCache(filecap, storage_index, downloader,
-                                            cachedirectorymanager)
-        prefix = self.u.get_verify_cap().to_string()
-        log.PrefixingLogMixin.__init__(self, "allmydata.immutable.filenode", prefix=prefix)
-        self.log("starting", level=log.OPERATIONAL)
-
-    def get_size(self):
-        return self.u.get_size()
-    def get_current_size(self):
-        return defer.succeed(self.get_size())
-
-    def get_cap(self):
-        return self.u
-    def get_readcap(self):
-        return self.u.get_readonly()
-    def get_verify_cap(self):
-        return self.u.get_verify_cap()
-    def get_repair_cap(self):
-        # CHK files can be repaired with just the verifycap
-        return self.u.get_verify_cap()
-
-    def get_uri(self):
-        return self.u.to_string()
-
-    def get_storage_index(self):
-        return self.u.get_storage_index()
-
-    def check_and_repair(self, monitor, verify=False, add_lease=False):
-        verifycap = self.get_verify_cap()
-        sb = self._storage_broker
-        servers = sb.get_all_servers()
-        sh = self._secret_holder
-
-        c = Checker(verifycap=verifycap, servers=servers,
-                    verify=verify, add_lease=add_lease, secret_holder=sh,
-                    monitor=monitor)
-        d = c.start()
-        def _maybe_repair(cr):
-            crr = CheckAndRepairResults(self.u.get_storage_index())
-            crr.pre_repair_results = cr
-            if cr.is_healthy():
-                crr.post_repair_results = cr
-                return defer.succeed(crr)
-            else:
-                crr.repair_attempted = True
-                crr.repair_successful = False # until proven successful
-                def _gather_repair_results(ur):
-                    assert IUploadResults.providedBy(ur), ur
-                    # clone the cr -- check results to form the basic of the prr -- post-repair results
-                    prr = CheckResults(cr.uri, cr.storage_index)
-                    prr.data = copy.deepcopy(cr.data)
-
-                    sm = prr.data['sharemap']
-                    assert isinstance(sm, dictutil.DictOfSets), sm
-                    sm.update(ur.sharemap)
-                    servers_responding = set(prr.data['servers-responding'])
-                    servers_responding.union(ur.sharemap.iterkeys())
-                    prr.data['servers-responding'] = list(servers_responding)
-                    prr.data['count-shares-good'] = len(sm)
-                    prr.data['count-good-share-hosts'] = len(sm)
-                    is_healthy = bool(len(sm) >= self.u.total_shares)
-                    is_recoverable = bool(len(sm) >= self.u.needed_shares)
-                    prr.set_healthy(is_healthy)
-                    prr.set_recoverable(is_recoverable)
-                    crr.repair_successful = is_healthy
-                    prr.set_needs_rebalancing(len(sm) >= self.u.total_shares)
-
-                    crr.post_repair_results = prr
-                    return crr
-                def _repair_error(f):
-                    # as with mutable repair, I'm not sure if I want to pass
-                    # through a failure or not. TODO
-                    crr.repair_successful = False
-                    crr.repair_failure = f
-                    return f
-                r = Repairer(storage_broker=sb, secret_holder=sh,
-                             verifycap=verifycap, monitor=monitor)
-                d = r.start()
-                d.addCallbacks(_gather_repair_results, _repair_error)
-                return d
-
-        d.addCallback(_maybe_repair)
-        return d
-
-    def check(self, monitor, verify=False, add_lease=False):
-        verifycap = self.get_verify_cap()
-        sb = self._storage_broker
-        servers = sb.get_all_servers()
-        sh = self._secret_holder
-
-        v = Checker(verifycap=verifycap, servers=servers,
-                    verify=verify, add_lease=add_lease, secret_holder=sh,
-                    monitor=monitor)
-        return v.start()
-
-    def read(self, consumer, offset=0, size=None):
-        self.log("read", offset=offset, size=size,
-                 umid="UPP8FA", level=log.OPERATIONAL)
-        if size is None:
-            size = self.get_size() - offset
-        size = min(size, self.get_size() - offset)
-
-        if offset == 0 and size == self.get_size():
-            # don't use the cache, just do a normal streaming download
-            self.log("doing normal full download", umid="VRSBwg", level=log.OPERATIONAL)
-            target = download.ConsumerAdapter(consumer)
-            return self._downloader.download(self.get_cap(), target,
-                                             self._parentmsgid,
-                                             history=self._history)
-
-        d = self.download_cache.when_range_available(offset, size)
-        d.addCallback(lambda res:
-                      self.download_cache.read(consumer, offset, size))
-        return d
 
 class LiteralProducer:
     implements(IPushProducer)
diff --git a/src/allmydata/test/test_filenode.py b/src/allmydata/test/test_filenode.py
index 19d6f5e..cd9a561 100644
--- a/src/allmydata/test/test_filenode.py
+++ b/src/allmydata/test/test_filenode.py
@@ -5,7 +5,7 @@ from allmydata.monitor import Monitor
 from allmydata.immutable.filenode import LiteralFileNode
 from allmydata.immutable.download2 import ImmutableFileNode
 from allmydata.mutable.filenode import MutableFileNode
-from allmydata.util import hashutil, cachedir
+from allmydata.util import hashutil
 from allmydata.util.consumer import download_to_data
 
 class NotANode:

commit 6618dc4166f220c28cd97fded19caf077318786d
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 19:00:43 2010 -0700

    split most of download2.py out into downloader/*.py
---
 src/allmydata/immutable/download2.py               | 2074 +-------------------
 src/allmydata/immutable/download2_util.py          |   73 -
 src/allmydata/immutable/downloader/common.py       |   11 +
 src/allmydata/immutable/downloader/fetcher.py      |  228 +++
 src/allmydata/immutable/downloader/finder.py       |  185 ++
 src/allmydata/immutable/downloader/node.py         |  528 +++++
 src/allmydata/immutable/downloader/segmentation.py |  157 ++
 src/allmydata/immutable/downloader/share.py        |  840 ++++++++
 src/allmydata/immutable/downloader/status.py       |  170 ++
 src/allmydata/immutable/downloader/util.py         |   73 +
 src/allmydata/test/test_download.py                |    4 +-
 11 files changed, 2201 insertions(+), 2142 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index ec284b3..61de459 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1,865 +1,18 @@
 
 import binascii
-import struct
-import copy
 import time
-import itertools
 now = time.time
 from zope.interface import implements
-from twisted.python.failure import Failure
 from twisted.internet import defer
-from twisted.internet.interfaces import IPushProducer, IConsumer
+from twisted.internet.interfaces import IConsumer
 
-from foolscap.api import eventually
-from allmydata.interfaces import IImmutableFileNode, IUploadResults, \
-     IDownloadStatus, HASH_SIZE, DEFAULT_MAX_SEGMENT_SIZE, \
-     NotEnoughSharesError, NoSharesError
-from allmydata.hashtree import IncompleteHashTree, BadHashError, \
-     NotEnoughHashesError
-from allmydata.util import base32, log, hashutil, mathutil, idlib
-from allmydata.util.spans import Spans, DataSpans, overlap
-from allmydata.util.dictutil import DictOfSets
-from allmydata.check_results import CheckResults, CheckAndRepairResults
-from allmydata.codec import CRSDecoder
+from allmydata.interfaces import IImmutableFileNode
 from allmydata import uri
 from pycryptopp.cipher.aes import AES
-from download2_util import Observer2, incidentally
-from layout import make_write_bucket_proxy
-from checker import Checker
-from repairer import Repairer
 
-(AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
- ("AVAILABLE", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
-
-KiB = 1024
-class BadSegmentNumberError(Exception):
-    pass
-class WrongSegmentError(Exception):
-    pass
-class BadCiphertextHashError(Exception):
-    pass
-class LayoutInvalid(Exception):
-    pass
-class DataUnavailable(Exception):
-    pass
-
-class Share:
-    """I represent a single instance of a single share (e.g. I reference the
-    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
-    I am associated with a CommonShare that remembers data that is held in
-    common among e.g. SI=abcde/shnum2 across all servers. I am also
-    associated with a CiphertextFileNode for e.g. SI=abcde (all shares, all
-    servers).
-    """
-    # this is a specific implementation of IShare for tahoe's native storage
-    # servers. A different backend would use a different class.
-
-    def __init__(self, rref, server_version, verifycap, commonshare, node,
-                 download_status, peerid, shnum, logparent):
-        self._rref = rref
-        self._server_version = server_version
-        self._node = node # holds share_hash_tree and UEB
-        self.actual_segment_size = node.segment_size # might still be None
-        # XXX change node.guessed_segment_size to
-        # node.best_guess_segment_size(), which should give us the real ones
-        # if known, else its guess.
-        self._guess_offsets(verifycap, node.guessed_segment_size)
-        self.actual_offsets = None
-        self._UEB_length = None
-        self._commonshare = commonshare # holds block_hash_tree
-        self._download_status = download_status
-        self._peerid = peerid
-        self._peerid_s = base32.b2a(peerid)[:5]
-        self._storage_index = verifycap.storage_index
-        self._si_prefix = base32.b2a(verifycap.storage_index)[:8]
-        self._shnum = shnum
-        # self._alive becomes False upon fatal corruption or server error
-        self._alive = True
-        self._lp = log.msg(format="%(share)s created", share=repr(self),
-                           level=log.NOISY, parent=logparent, umid="P7hv2w")
-
-        self._pending = Spans() # request sent but no response received yet
-        self._received = DataSpans() # ACK response received, with data
-        self._unavailable = Spans() # NAK response received, no data
-
-        # any given byte of the share can be in one of four states:
-        #  in: _wanted, _requested, _received
-        #      FALSE    FALSE       FALSE : don't care about it at all
-        #      TRUE     FALSE       FALSE : want it, haven't yet asked for it
-        #      TRUE     TRUE        FALSE : request is in-flight
-        #                                   or didn't get it
-        #      FALSE    TRUE        TRUE  : got it, haven't used it yet
-        #      FALSE    TRUE        FALSE : got it and used it
-        #      FALSE    FALSE       FALSE : block consumed, ready to ask again
-        #
-        # when we request data and get a NAK, we leave it in _requested
-        # to remind ourself to not ask for it again. We don't explicitly
-        # remove it from anything (maybe this should change).
-        #
-        # We retain the hashtrees in the Node, so we leave those spans in
-        # _requested (and never ask for them again, as long as the Node is
-        # alive). But we don't retain data blocks (too big), so when we
-        # consume a data block, we remove it from _requested, so a later
-        # download can re-fetch it.
-
-        self._requested_blocks = [] # (segnum, set(observer2..))
-        ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
-        self._overrun_ok = ver["tolerates-immutable-read-overrun"]
-        # If _overrun_ok and we guess the offsets correctly, we can get
-        # everything in one RTT. If _overrun_ok and we guess wrong, we might
-        # need two RTT (but we could get lucky and do it in one). If overrun
-        # is *not* ok (tahoe-1.3.0 or earlier), we need four RTT: 1=version,
-        # 2=offset table, 3=UEB_length and everything else (hashes, block),
-        # 4=UEB.
-
-        self.had_corruption = False # for unit tests
-
-    def __repr__(self):
-        return "Share(sh%d-on-%s)" % (self._shnum, self._peerid_s)
-
-    def is_alive(self):
-        # XXX: reconsider. If the share sees a single error, should it remain
-        # dead for all time? Or should the next segment try again? This DEAD
-        # state is stored elsewhere too (SegmentFetcher per-share states?)
-        # and needs to be consistent. We clear _alive in self._fail(), which
-        # is called upon a network error, or layout failure, or hash failure
-        # in the UEB or a hash tree. We do not _fail() for a hash failure in
-        # a block, but of course we still tell our callers about
-        # state=CORRUPT so they'll find a different share.
-        return self._alive
-
-    def _guess_offsets(self, verifycap, guessed_segment_size):
-        self.guessed_segment_size = guessed_segment_size
-        size = verifycap.size
-        k = verifycap.needed_shares
-        N = verifycap.total_shares
-        r = self._node._calculate_sizes(guessed_segment_size)
-        # num_segments, block_size/tail_block_size
-        # guessed_segment_size/tail_segment_size/tail_segment_padded
-        share_size = mathutil.div_ceil(size, k)
-        # share_size is the amount of block data that will be put into each
-        # share, summed over all segments. It does not include hashes, the
-        # UEB, or other overhead.
-
-        # use the upload-side code to get this as accurate as possible
-        ht = IncompleteHashTree(N)
-        num_share_hashes = len(ht.needed_hashes(0, include_leaf=True))
-        wbp = make_write_bucket_proxy(None, share_size, r["block_size"],
-                                      r["num_segments"], num_share_hashes, 0,
-                                      None)
-        self._fieldsize = wbp.fieldsize
-        self._fieldstruct = wbp.fieldstruct
-        self.guessed_offsets = wbp._offsets
-
-    # called by our client, the SegmentFetcher
-    def get_block(self, segnum):
-        """Add a block number to the list of requests. This will eventually
-        result in a fetch of the data necessary to validate the block, then
-        the block itself. The fetch order is generally
-        first-come-first-served, but requests may be answered out-of-order if
-        data becomes available sooner.
-
-        I return an Observer2, which has two uses. The first is to call
-        o.subscribe(), which gives me a place to send state changes and
-        eventually the data block. The second is o.cancel(), which removes
-        the request (if it is still active).
-
-        I will distribute the following events through my Observer2:
-         - state=OVERDUE: ?? I believe I should have had an answer by now.
-                          You may want to ask another share instead.
-         - state=BADSEGNUM: the segnum you asked for is too large. I must
-                            fetch a valid UEB before I can determine this,
-                            so the notification is asynchronous
-         - state=COMPLETE, block=data: here is a valid block
-         - state=CORRUPT: this share contains corrupted data
-         - state=DEAD, f=Failure: the server reported an error, this share
-                                  is unusable
-        """
-        log.msg("%s.get_block(%d)" % (repr(self), segnum),
-                level=log.NOISY, parent=self._lp, umid="RTo9MQ")
-        assert segnum >= 0
-        o = Observer2()
-        o.set_canceler(self, "_cancel_block_request")
-        for i,(segnum0,observers) in enumerate(self._requested_blocks):
-            if segnum0 == segnum:
-                observers.add(o)
-                break
-        else:
-            self._requested_blocks.append( (segnum, set([o])) )
-        eventually(self.loop)
-        return o
-
-    def _cancel_block_request(self, o):
-        new_requests = []
-        for e in self._requested_blocks:
-            (segnum0, observers) = e
-            observers.discard(o)
-            if observers:
-                new_requests.append(e)
-        self._requested_blocks = new_requests
-
-    # internal methods
-    def _active_segnum_and_observers(self):
-        if self._requested_blocks:
-            # we only retrieve information for one segment at a time, to
-            # minimize alacrity (first come, first served)
-            return self._requested_blocks[0]
-        return None, []
-
-    def loop(self):
-        try:
-            # if any exceptions occur here, kill the download
-            log.msg("%s.loop, reqs=[%s], pending=%s, received=%s,"
-                    " unavailable=%s" %
-                    (repr(self),
-                     ",".join([str(req[0]) for req in self._requested_blocks]),
-                     self._pending.dump(), self._received.dump(),
-                     self._unavailable.dump() ),
-                    level=log.NOISY, parent=self._lp, umid="BaL1zw")
-            self._do_loop()
-            # all exception cases call self._fail(), which clears self._alive
-        except (BadHashError, NotEnoughHashesError, LayoutInvalid), e:
-            # Abandon this share. We do this if we see corruption in the
-            # offset table, the UEB, or a hash tree. We don't abandon the
-            # whole share if we see corruption in a data block (we abandon
-            # just the one block, and still try to get data from other blocks
-            # on the same server). In theory, we could get good data from a
-            # share with a corrupt UEB (by first getting the UEB from some
-            # other share), or corrupt hash trees, but the logic to decide
-            # when this is safe is non-trivial. So for now, give up at the
-            # first sign of corruption.
-            #
-            # _satisfy_*() code which detects corruption should first call
-            # self._signal_corruption(), and then raise the exception.
-            log.msg(format="corruption detected in %(share)s",
-                    share=repr(self),
-                    level=log.UNUSUAL, parent=self._lp, umid="gWspVw")
-            self._fail(Failure(e), log.UNUSUAL)
-        except DataUnavailable, e:
-            # Abandon this share.
-            log.msg(format="need data that will never be available"
-                    " from %s: pending=%s, received=%s, unavailable=%s" %
-                    (repr(self),
-                     self._pending.dump(), self._received.dump(),
-                     self._unavailable.dump() ),
-                    level=log.UNUSUAL, parent=self._lp, umid="F7yJnQ")
-            self._fail(Failure(e), log.UNUSUAL)
-        except BaseException:
-            self._fail(Failure())
-            raise
-        log.msg("%s.loop done, reqs=[%s], pending=%s, received=%s,"
-                " unavailable=%s" %
-                (repr(self),
-                 ",".join([str(req[0]) for req in self._requested_blocks]),
-                 self._pending.dump(), self._received.dump(),
-                 self._unavailable.dump() ),
-                level=log.NOISY, parent=self._lp, umid="9lRaRA")
-
-    def _do_loop(self):
-        # we are (eventually) called after all state transitions:
-        #  new segments added to self._requested_blocks
-        #  new data received from servers (responses to our read() calls)
-        #  impatience timer fires (server appears slow)
-        if not self._alive:
-            return
-
-        # First, consume all of the information that we currently have, for
-        # all the segments people currently want.
-        while self._get_satisfaction():
-            pass
-
-        # When we get no satisfaction (from the data we've received so far),
-        # we determine what data we desire (to satisfy more requests). The
-        # number of segments is finite, so I can't get no satisfaction
-        # forever.
-        wanted, needed = self._desire()
-
-        # Finally, send out requests for whatever we need (desire minus
-        # have). You can't always get what you want, but if you try
-        # sometimes, you just might find, you get what you need.
-        self._send_requests(wanted + needed)
-
-        # and sometimes you can't even get what you need
-        disappointment = needed & self._unavailable
-        if len(disappointment):
-            self.had_corruption = True
-            raise DataUnavailable("need %s but will never get it" %
-                                  disappointment.dump())
-
-    def _get_satisfaction(self):
-        # return True if we retired a data block, and should therefore be
-        # called again. Return False if we don't retire a data block (even if
-        # we do retire some other data, like hash chains).
-
-        if self.actual_offsets is None:
-            if not self._satisfy_offsets():
-                # can't even look at anything without the offset table
-                return False
-
-        if not self._node.have_UEB:
-            if not self._satisfy_UEB():
-                # can't check any hashes without the UEB
-                return False
-        self.actual_segment_size = self._node.segment_size # might be updated
-        assert self.actual_segment_size is not None
-
-        # knowing the UEB means knowing num_segments. Despite the redundancy,
-        # this is the best place to set this. CommonShare.set_numsegs will
-        # ignore duplicate calls.
-        assert self._node.num_segments is not None
-        cs = self._commonshare
-        cs.set_numsegs(self._node.num_segments)
-
-        segnum, observers = self._active_segnum_and_observers()
-        # if segnum is None, we don't really need to do anything (we have no
-        # outstanding readers right now), but we'll fill in the bits that
-        # aren't tied to any particular segment.
-
-        if segnum is not None and segnum >= self._node.num_segments:
-            for o in observers:
-                o.notify(state=BADSEGNUM)
-            self._requested_blocks.pop(0)
-            return True
-
-        if self._node.share_hash_tree.needed_hashes(self._shnum):
-            if not self._satisfy_share_hash_tree():
-                # can't check block_hash_tree without a root
-                return False
-
-        if cs.need_block_hash_root():
-            block_hash_root = self._node.share_hash_tree.get_leaf(self._shnum)
-            cs.set_block_hash_root(block_hash_root)
-
-        if segnum is None:
-            return False # we don't want any particular segment right now
-
-        # block_hash_tree
-        needed_hashes = self._commonshare.get_needed_block_hashes(segnum)
-        if needed_hashes:
-            if not self._satisfy_block_hash_tree(needed_hashes):
-                # can't check block without block_hash_tree
-                return False
-
-        # ciphertext_hash_tree
-        needed_hashes = self._node.get_needed_ciphertext_hashes(segnum)
-        if needed_hashes:
-            if not self._satisfy_ciphertext_hash_tree(needed_hashes):
-                # can't check decoded blocks without ciphertext_hash_tree
-                return False
-
-        # data blocks
-        return self._satisfy_data_block(segnum, observers)
-
-    def _satisfy_offsets(self):
-        version_s = self._received.get(0, 4)
-        if version_s is None:
-            return False
-        (version,) = struct.unpack(">L", version_s)
-        if version == 1:
-            table_start = 0x0c
-            self._fieldsize = 0x4
-            self._fieldstruct = "L"
-        elif version == 2:
-            table_start = 0x14
-            self._fieldsize = 0x8
-            self._fieldstruct = "Q"
-        else:
-            self.had_corruption = True
-            raise LayoutInvalid("unknown version %d (I understand 1 and 2)"
-                                % version)
-        offset_table_size = 6 * self._fieldsize
-        table_s = self._received.pop(table_start, offset_table_size)
-        if table_s is None:
-            return False
-        fields = struct.unpack(">"+6*self._fieldstruct, table_s)
-        offsets = {}
-        for i,field in enumerate(['data',
-                                  'plaintext_hash_tree', # UNUSED
-                                  'crypttext_hash_tree',
-                                  'block_hashes',
-                                  'share_hashes',
-                                  'uri_extension',
-                                  ] ):
-            offsets[field] = fields[i]
-        self.actual_offsets = offsets
-        log.msg("actual offsets: data=%d, plaintext_hash_tree=%d, crypttext_hash_tree=%d, block_hashes=%d, share_hashes=%d, uri_extension=%d" % tuple(fields))
-        self._received.remove(0, 4) # don't need this anymore
-
-        # validate the offsets a bit
-        share_hashes_size = offsets["uri_extension"] - offsets["share_hashes"]
-        if share_hashes_size < 0 or share_hashes_size % (2+HASH_SIZE) != 0:
-            # the share hash chain is stored as (hashnum,hash) pairs
-            self.had_corruption = True
-            raise LayoutInvalid("share hashes malformed -- should be a"
-                                " multiple of %d bytes -- not %d" %
-                                (2+HASH_SIZE, share_hashes_size))
-        block_hashes_size = offsets["share_hashes"] - offsets["block_hashes"]
-        if block_hashes_size < 0 or block_hashes_size % (HASH_SIZE) != 0:
-            # the block hash tree is stored as a list of hashes
-            self.had_corruption = True
-            raise LayoutInvalid("block hashes malformed -- should be a"
-                                " multiple of %d bytes -- not %d" %
-                                (HASH_SIZE, block_hashes_size))
-        # we only look at 'crypttext_hash_tree' if the UEB says we're
-        # actually using it. Same with 'plaintext_hash_tree'. This gives us
-        # some wiggle room: a place to stash data for later extensions.
-
-        return True
-
-    def _satisfy_UEB(self):
-        o = self.actual_offsets
-        fsize = self._fieldsize
-        UEB_length_s = self._received.get(o["uri_extension"], fsize)
-        if not UEB_length_s:
-            return False
-        (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
-        UEB_s = self._received.pop(o["uri_extension"]+fsize, UEB_length)
-        if not UEB_s:
-            return False
-        self._received.remove(o["uri_extension"], fsize)
-        try:
-            self._node.validate_and_store_UEB(UEB_s)
-            return True
-        except (LayoutInvalid, BadHashError), e:
-            # TODO: if this UEB was bad, we'll keep trying to validate it
-            # over and over again. Only log.err on the first one, or better
-            # yet skip all but the first
-            f = Failure(e)
-            self._signal_corruption(f, o["uri_extension"], fsize+UEB_length)
-            self.had_corruption = True
-            raise
-
-    def _satisfy_share_hash_tree(self):
-        # the share hash chain is stored as (hashnum,hash) tuples, so you
-        # can't fetch just the pieces you need, because you don't know
-        # exactly where they are. So fetch everything, and parse the results
-        # later.
-        o = self.actual_offsets
-        hashlen = o["uri_extension"] - o["share_hashes"]
-        assert hashlen % (2+HASH_SIZE) == 0
-        hashdata = self._received.get(o["share_hashes"], hashlen)
-        if not hashdata:
-            return False
-        share_hashes = {}
-        for i in range(0, hashlen, 2+HASH_SIZE):
-            (hashnum,) = struct.unpack(">H", hashdata[i:i+2])
-            hashvalue = hashdata[i+2:i+2+HASH_SIZE]
-            share_hashes[hashnum] = hashvalue
-        try:
-            self._node.process_share_hashes(share_hashes)
-            # adds to self._node.share_hash_tree
-        except (BadHashError, NotEnoughHashesError), e:
-            f = Failure(e)
-            self._signal_corruption(f, o["share_hashes"], hashlen)
-            self.had_corruption = True
-            raise
-        self._received.remove(o["share_hashes"], hashlen)
-        return True
-
-    def _signal_corruption(self, f, start, offset):
-        # there was corruption somewhere in the given range
-        reason = "corruption in share[%d-%d): %s" % (start, start+offset,
-                                                     str(f.value))
-        self._rref.callRemoteOnly("advise_corrupt_share", reason)
-
-    def _satisfy_block_hash_tree(self, needed_hashes):
-        o_bh = self.actual_offsets["block_hashes"]
-        block_hashes = {}
-        for hashnum in needed_hashes:
-            hashdata = self._received.get(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
-            if hashdata:
-                block_hashes[hashnum] = hashdata
-            else:
-                return False # missing some hashes
-        # note that we don't submit any hashes to the block_hash_tree until
-        # we've gotten them all, because the hash tree will throw an
-        # exception if we only give it a partial set (which it therefore
-        # cannot validate)
-        try:
-            self._commonshare.process_block_hashes(block_hashes)
-        except (BadHashError, NotEnoughHashesError), e:
-            f = Failure(e)
-            hashnums = ",".join([str(n) for n in sorted(block_hashes.keys())])
-            log.msg(format="hash failure in block_hashes=(%(hashnums)s),"
-                    " from %(share)s",
-                    hashnums=hashnums, shnum=self._shnum, share=repr(self),
-                    failure=f, level=log.WEIRD, parent=self._lp, umid="yNyFdA")
-            hsize = max(0, max(needed_hashes)) * HASH_SIZE
-            self._signal_corruption(f, o_bh, hsize)
-            self.had_corruption = True
-            raise
-        for hashnum in needed_hashes:
-            self._received.remove(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
-        return True
-
-    def _satisfy_ciphertext_hash_tree(self, needed_hashes):
-        start = self.actual_offsets["crypttext_hash_tree"]
-        hashes = {}
-        for hashnum in needed_hashes:
-            hashdata = self._received.get(start+hashnum*HASH_SIZE, HASH_SIZE)
-            if hashdata:
-                hashes[hashnum] = hashdata
-            else:
-                return False # missing some hashes
-        # we don't submit any hashes to the ciphertext_hash_tree until we've
-        # gotten them all
-        try:
-            self._node.process_ciphertext_hashes(hashes)
-        except (BadHashError, NotEnoughHashesError), e:
-            f = Failure(e)
-            hashnums = ",".join([str(n) for n in sorted(hashes.keys())])
-            log.msg(format="hash failure in ciphertext_hashes=(%(hashnums)s),"
-                    " from %(share)s",
-                    hashnums=hashnums, share=repr(self), failure=f,
-                    level=log.WEIRD, parent=self._lp, umid="iZI0TA")
-            hsize = max(0, max(needed_hashes))*HASH_SIZE
-            self._signal_corruption(f, start, hsize)
-            self.had_corruption = True
-            raise
-        for hashnum in needed_hashes:
-            self._received.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
-        return True
-
-    def _satisfy_data_block(self, segnum, observers):
-        tail = (segnum == self._node.num_segments-1)
-        datastart = self.actual_offsets["data"]
-        blockstart = datastart + segnum * self._node.block_size
-        blocklen = self._node.block_size
-        if tail:
-            blocklen = self._node.tail_block_size
-
-        block = self._received.pop(blockstart, blocklen)
-        if not block:
-            log.msg("no data for block %s (want [%d:+%d])" % (repr(self),
-                                                              blockstart, blocklen))
-            return False
-        log.msg(format="%(share)s._satisfy_data_block [%(start)d:+%(length)d]",
-                share=repr(self), start=blockstart, length=blocklen,
-                level=log.NOISY, parent=self._lp, umid="uTDNZg")
-        # this block is being retired, either as COMPLETE or CORRUPT, since
-        # no further data reads will help
-        assert self._requested_blocks[0][0] == segnum
-        try:
-            self._commonshare.check_block(segnum, block)
-            # hurrah, we have a valid block. Deliver it.
-            for o in observers:
-                # goes to SegmentFetcher._block_request_activity
-                o.notify(state=COMPLETE, block=block)
-        except (BadHashError, NotEnoughHashesError), e:
-            # rats, we have a corrupt block. Notify our clients that they
-            # need to look elsewhere, and advise the server. Unlike
-            # corruption in other parts of the share, this doesn't cause us
-            # to abandon the whole share.
-            f = Failure(e)
-            log.msg(format="hash failure in block %(segnum)d, from %(share)s",
-                    segnum=segnum, share=repr(self), failure=f,
-                    level=log.WEIRD, parent=self._lp, umid="mZjkqA")
-            for o in observers:
-                o.notify(state=CORRUPT)
-            self._signal_corruption(f, blockstart, blocklen)
-            self.had_corruption = True
-        # in either case, we've retired this block
-        self._requested_blocks.pop(0)
-        # popping the request keeps us from turning around and wanting the
-        # block again right away
-        return True # got satisfaction
-
-    def _desire(self):
-        segnum, observers = self._active_segnum_and_observers() # maybe None
-
-        # 'want_it' is for data we merely want: we know that we don't really
-        # need it. This includes speculative reads, like the first 1KB of the
-        # share (for the offset table) and the first 2KB of the UEB.
-        #
-        # 'need_it' is for data that, if we have the real offset table, we'll
-        # need. If we are only guessing at the offset table, it's merely
-        # wanted. (The share is abandoned if we can't get data that we really
-        # need).
-        #
-        # 'gotta_gotta_have_it' is for data that we absolutely need,
-        # independent of whether we're still guessing about the offset table:
-        # the version number and the offset table itself.
-        #
-        # Mr. Popeil, I'm in trouble, need your assistance on the double. Aww..
-
-        desire = Spans(), Spans(), Spans()
-        (want_it, need_it, gotta_gotta_have_it) = desire
-
-        self.actual_segment_size = self._node.segment_size # might be updated
-        o = self.actual_offsets or self.guessed_offsets
-        segsize = self.actual_segment_size or self.guessed_segment_size
-        r = self._node._calculate_sizes(segsize)
-
-        if not self.actual_offsets:
-            # all _desire functions add bits to the three desire[] spans
-            self._desire_offsets(desire)
-
-        # we can use guessed offsets as long as this server tolerates
-        # overrun. Otherwise, we must wait for the offsets to arrive before
-        # we try to read anything else.
-        if self.actual_offsets or self._overrun_ok:
-            if not self._node.have_UEB:
-                self._desire_UEB(desire, o)
-            # They might ask for a segment that doesn't look right.
-            # _satisfy() will catch+reject bad segnums once we know the UEB
-            # (and therefore segsize and numsegs), so we'll only fail this
-            # test if we're still guessing. We want to avoid asking the
-            # hashtrees for needed_hashes() for bad segnums. So don't enter
-            # _desire_hashes or _desire_data unless the segnum looks
-            # reasonable.
-            if segnum < r["num_segments"]:
-                # XXX somehow we're getting here for sh5. we don't yet know
-                # the actual_segment_size, we're still working off the guess.
-                # the ciphertext_hash_tree has been corrected, but the
-                # commonshare._block_hash_tree is still in the guessed state.
-                self._desire_share_hashes(desire, o)
-                if segnum is not None:
-                    self._desire_block_hashes(desire, o, segnum)
-                    self._desire_data(desire, o, r, segnum, segsize)
-            else:
-                log.msg("_desire: segnum(%d) looks wrong (numsegs=%d)"
-                        % (segnum, r["num_segments"]),
-                        level=log.UNUSUAL, parent=self._lp, umid="tuYRQQ")
-
-        log.msg("end _desire: want_it=%s need_it=%s gotta=%s"
-                % (want_it.dump(), need_it.dump(), gotta_gotta_have_it.dump()))
-        if self.actual_offsets:
-            return (want_it, need_it+gotta_gotta_have_it)
-        else:
-            return (want_it+need_it, gotta_gotta_have_it)
-
-    def _desire_offsets(self, desire):
-        (want_it, need_it, gotta_gotta_have_it) = desire
-        if self._overrun_ok:
-            # easy! this includes version number, sizes, and offsets
-            want_it.add(0, 1024)
-            return
-
-        # v1 has an offset table that lives [0x0,0x24). v2 lives [0x0,0x44).
-        # To be conservative, only request the data that we know lives there,
-        # even if that means more roundtrips.
-
-        gotta_gotta_have_it.add(0, 4)  # version number, always safe
-        version_s = self._received.get(0, 4)
-        if not version_s:
-            return
-        (version,) = struct.unpack(">L", version_s)
-        # The code in _satisfy_offsets will have checked this version
-        # already. There is no code path to get this far with version>2.
-        assert 1 <= version <= 2, "can't get here, version=%d" % version
-        if version == 1:
-            table_start = 0x0c
-            fieldsize = 0x4
-        elif version == 2:
-            table_start = 0x14
-            fieldsize = 0x8
-        offset_table_size = 6 * fieldsize
-        gotta_gotta_have_it.add(table_start, offset_table_size)
-
-    def _desire_UEB(self, desire, o):
-        (want_it, need_it, gotta_gotta_have_it) = desire
-
-        # UEB data is stored as (length,data).
-        if self._overrun_ok:
-            # We can pre-fetch 2kb, which should probably cover it. If it
-            # turns out to be larger, we'll come back here later with a known
-            # length and fetch the rest.
-            want_it.add(o["uri_extension"], 2048)
-            # now, while that is probably enough to fetch the whole UEB, it
-            # might not be, so we need to do the next few steps as well. In
-            # most cases, the following steps will not actually add anything
-            # to need_it
-
-        need_it.add(o["uri_extension"], self._fieldsize)
-        # only use a length if we're sure it's correct, otherwise we'll
-        # probably fetch a huge number
-        if not self.actual_offsets:
-            return
-        UEB_length_s = self._received.get(o["uri_extension"], self._fieldsize)
-        if UEB_length_s:
-            (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
-            # we know the length, so make sure we grab everything
-            need_it.add(o["uri_extension"]+self._fieldsize, UEB_length)
-
-    def _desire_share_hashes(self, desire, o):
-        (want_it, need_it, gotta_gotta_have_it) = desire
-
-        if self._node.share_hash_tree.needed_hashes(self._shnum):
-            hashlen = o["uri_extension"] - o["share_hashes"]
-            need_it.add(o["share_hashes"], hashlen)
-
-    def _desire_block_hashes(self, desire, o, segnum):
-        (want_it, need_it, gotta_gotta_have_it) = desire
-
-        # block hash chain
-        for hashnum in self._commonshare.get_needed_block_hashes(segnum):
-            need_it.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
-
-        # ciphertext hash chain
-        for hashnum in self._node.get_needed_ciphertext_hashes(segnum):
-            need_it.add(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
-
-    def _desire_data(self, desire, o, r, segnum, segsize):
-        (want_it, need_it, gotta_gotta_have_it) = desire
-        tail = (segnum == r["num_segments"]-1)
-        datastart = o["data"]
-        blockstart = datastart + segnum * r["block_size"]
-        blocklen = r["block_size"]
-        if tail:
-            blocklen = r["tail_block_size"]
-        need_it.add(blockstart, blocklen)
-
-    def _send_requests(self, desired):
-        ask = desired - self._pending - self._received.get_spans()
-        log.msg("%s._send_requests, desired=%s, pending=%s, ask=%s" %
-                (repr(self), desired.dump(), self._pending.dump(), ask.dump()),
-                level=log.NOISY, parent=self._lp, umid="E94CVA")
-        # XXX At one time, this code distinguished between data blocks and
-        # hashes, and made sure to send (small) requests for hashes before
-        # sending (big) requests for blocks. The idea was to make sure that
-        # all hashes arrive before the blocks, so the blocks can be consumed
-        # and released in a single turn. I removed this for simplicity.
-        # Reconsider the removal: maybe bring it back.
-        ds = self._download_status
-
-        for (start, length) in ask:
-            # TODO: quantize to reasonably-large blocks
-            self._pending.add(start, length)
-            lp = log.msg(format="%(share)s._send_request"
-                         " [%(start)d:+%(length)d]",
-                         share=repr(self),
-                         start=start, length=length,
-                         level=log.NOISY, parent=self._lp, umid="sgVAyA")
-            req_ev = ds.add_request_sent(self._peerid, self._shnum,
-                                         start, length, now())
-            d = self._send_request(start, length)
-            d.addCallback(self._got_data, start, length, req_ev, lp)
-            d.addErrback(self._got_error, start, length, req_ev, lp)
-            d.addCallback(self._trigger_loop)
-            d.addErrback(lambda f:
-                         log.err(format="unhandled error during send_request",
-                                 failure=f, parent=self._lp,
-                                 level=log.WEIRD, umid="qZu0wg"))
-
-    def _send_request(self, start, length):
-        return self._rref.callRemote("read", start, length)
-
-    def _got_data(self, data, start, length, req_ev, lp):
-        req_ev.finished(len(data), now())
-        if not self._alive:
-            return
-        log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
-                share=repr(self), start=start, length=length, datalen=len(data),
-                level=log.NOISY, parent=lp, umid="5Qn6VQ")
-        self._pending.remove(start, length)
-        self._received.add(start, data)
-
-        # if we ask for [a:c], and we get back [a:b] (b<c), that means we're
-        # never going to get [b:c]. If we really need that data, this block
-        # will never complete. The easiest way to get into this situation is
-        # to hit a share with a corrupted offset table, or one that's somehow
-        # been truncated. On the other hand, when overrun_ok is true, we ask
-        # for data beyond the end of the share all the time (it saves some
-        # RTT when we don't know the length of the share ahead of time). So
-        # not every asked-for-but-not-received byte is fatal.
-        if len(data) < length:
-            self._unavailable.add(start+len(data), length-len(data))
-
-        # XXX if table corruption causes our sections to overlap, then one
-        # consumer (i.e. block hash tree) will pop/remove the data that
-        # another consumer (i.e. block data) mistakenly thinks it needs. It
-        # won't ask for that data again, because the span is in
-        # self._requested. But that span won't be in self._unavailable
-        # because we got it back from the server. TODO: handle this properly
-        # (raise DataUnavailable). Then add sanity-checking
-        # no-overlaps-allowed tests to the offset-table unpacking code to
-        # catch this earlier. XXX
-
-        # accumulate a wanted/needed span (not as self._x, but passed into
-        # desire* functions). manage a pending/in-flight list. when the
-        # requests are sent out, empty/discard the wanted/needed span and
-        # populate/augment the pending list. when the responses come back,
-        # augment either received+data or unavailable.
-
-        # if a corrupt offset table results in double-usage, we'll send
-        # double requests.
-
-        # the wanted/needed span is only "wanted" for the first pass. Once
-        # the offset table arrives, it's all "needed".
-
-    def _got_error(self, f, start, length, req_ev, lp):
-        req_ev.finished("error", now())
-        log.msg(format="error requesting %(start)d+%(length)d"
-                " from %(server)s for si %(si)s",
-                start=start, length=length,
-                server=self._peerid_s, si=self._si_prefix,
-                failure=f, parent=lp, level=log.UNUSUAL, umid="BZgAJw")
-        # retire our observers, assuming we won't be able to make any
-        # further progress
-        self._fail(f, log.UNUSUAL)
-
-    def _trigger_loop(self, res):
-        if self._alive:
-            eventually(self.loop)
-        return res
-
-    def _fail(self, f, level=log.WEIRD):
-        log.msg(format="abandoning %(share)s",
-                share=repr(self), failure=f,
-                level=level, parent=self._lp, umid="JKM2Og")
-        self._alive = False
-        for (segnum, observers) in self._requested_blocks:
-            for o in observers:
-                o.notify(state=DEAD, f=f)
-
-
-class CommonShare:
-    """I hold data that is common across all instances of a single share,
-    like sh2 on both servers A and B. This is just the block hash tree.
-    """
-    def __init__(self, guessed_numsegs, si_prefix, shnum, logparent):
-        self.si_prefix = si_prefix
-        self.shnum = shnum
-        # in the beginning, before we have the real UEB, we can only guess at
-        # the number of segments. But we want to ask for block hashes early.
-        # So if we're asked for which block hashes are needed before we know
-        # numsegs for sure, we return a guess.
-        self._block_hash_tree = IncompleteHashTree(guessed_numsegs)
-        self._know_numsegs = False
-        self._logparent = logparent
-
-    def set_numsegs(self, numsegs):
-        if self._know_numsegs:
-            return
-        self._block_hash_tree = IncompleteHashTree(numsegs)
-        self._know_numsegs = True
-
-    def need_block_hash_root(self):
-        return bool(not self._block_hash_tree[0])
-
-    def set_block_hash_root(self, roothash):
-        assert self._know_numsegs
-        self._block_hash_tree.set_hashes({0: roothash})
-
-    def get_needed_block_hashes(self, segnum):
-        # XXX: include_leaf=True needs thought: how did the old downloader do
-        # it? I think it grabbed *all* block hashes and set them all at once.
-        # Since we want to fetch less data, we either need to fetch the leaf
-        # too, or wait to set the block hashes until we've also received the
-        # block itself, so we can hash it too, and set the chain+leaf all at
-        # the same time.
-        return self._block_hash_tree.needed_hashes(segnum, include_leaf=True)
-
-    def process_block_hashes(self, block_hashes):
-        assert self._know_numsegs
-        # this may raise BadHashError or NotEnoughHashesError
-        self._block_hash_tree.set_hashes(block_hashes)
-
-    def check_block(self, segnum, block):
-        assert self._know_numsegs
-        h = hashutil.block_hash(block)
-        # this may raise BadHashError or NotEnoughHashesError
-        self._block_hash_tree.set_hashes(leaves={segnum: h})
+# local imports
+from allmydata.immutable.downloader.node import DownloadNode
+from allmydata.immutable.downloader.status import DownloadStatus
 
 # all classes are also Services, and the rule is that you don't initiate more
 # work unless self.running
@@ -869,1053 +22,6 @@ class CommonShare:
 # cycles. The primary goal is to decref remote storage BucketReaders when a
 # download is complete.
 
-class SegmentFetcher:
-    """I am responsible for acquiring blocks for a single segment. I will use
-    the Share instances passed to my add_shares() method to locate, retrieve,
-    and validate those blocks. I expect my parent node to call my
-    no_more_shares() method when there are no more shares available. I will
-    call my parent's want_more_shares() method when I want more: I expect to
-    see at least one call to add_shares or no_more_shares afterwards.
-
-    When I have enough validated blocks, I will call my parent's
-    process_blocks() method with a dictionary that maps shnum to blockdata.
-    If I am unable to provide enough blocks, I will call my parent's
-    fetch_failed() method with (self, f). After either of these events, I
-    will shut down and do no further work. My parent can also call my stop()
-    method to have me shut down early."""
-
-    def __init__(self, node, segnum, k):
-        self._node = node # _Node
-        self.segnum = segnum
-        self._k = k
-        self._shares = {} # maps non-dead Share instance to a state, one of
-                          # (AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT).
-                          # State transition map is:
-                          #  AVAILABLE -(send-read)-> PENDING
-                          #  PENDING -(timer)-> OVERDUE
-                          #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
-                          #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
-                          # If a share becomes DEAD, it is removed from the
-                          # dict. If it becomes BADSEGNUM, the whole fetch is
-                          # terminated.
-        self._share_observers = {} # maps Share to Observer2 for active ones
-        self._shnums = DictOfSets() # maps shnum to the shares that provide it
-        self._blocks = {} # maps shnum to validated block data
-        self._no_more_shares = False
-        self._bad_segnum = False
-        self._last_failure = None
-        self._running = True
-
-    def stop(self):
-        log.msg("SegmentFetcher(%s).stop" % self._node._si_prefix,
-                level=log.NOISY, umid="LWyqpg")
-        self._cancel_all_requests()
-        self._running = False
-        self._shares.clear() # let GC work # ??? XXX
-
-
-    # called by our parent _Node
-
-    def add_shares(self, shares):
-        # called when ShareFinder locates a new share, and when a non-initial
-        # segment fetch is started and we already know about shares from the
-        # previous segment
-        for s in shares:
-            self._shares[s] = AVAILABLE
-            self._shnums.add(s._shnum, s)
-        eventually(self.loop)
-
-    def no_more_shares(self):
-        # ShareFinder tells us it's reached the end of its list
-        self._no_more_shares = True
-        eventually(self.loop)
-
-    # internal methods
-
-    def _count_shnums(self, *states):
-        """shnums for which at least one state is in the following list"""
-        shnums = []
-        for shnum,shares in self._shnums.iteritems():
-            matches = [s for s in shares if self._shares.get(s) in states]
-            if matches:
-                shnums.append(shnum)
-        return len(shnums)
-
-    def loop(self):
-        try:
-            # if any exception occurs here, kill the download
-            self._do_loop()
-        except BaseException:
-            self._node.fetch_failed(self, Failure())
-            raise
-
-    def _do_loop(self):
-        k = self._k
-        if not self._running:
-            return
-        if self._bad_segnum:
-            # oops, we were asking for a segment number beyond the end of the
-            # file. This is an error.
-            self.stop()
-            e = BadSegmentNumberError("segnum=%d, numsegs=%d" %
-                                      (self.segnum, self._node.num_segments))
-            f = Failure(e)
-            self._node.fetch_failed(self, f)
-            return
-
-        # are we done?
-        if self._count_shnums(COMPLETE) >= k:
-            # yay!
-            self.stop()
-            self._node.process_blocks(self.segnum, self._blocks)
-            return
-
-        # we may have exhausted everything
-        if (self._no_more_shares and
-            self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) < k):
-            # no more new shares are coming, and the remaining hopeful shares
-            # aren't going to be enough. boo!
-
-            log.msg("share states: %r" % (self._shares,),
-                    level=log.NOISY, umid="0ThykQ")
-            if self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) == 0:
-                format = ("no shares (need %(k)d)."
-                          " Last failure: %(last_failure)s")
-                args = { "k": k,
-                         "last_failure": self._last_failure }
-                error = NoSharesError
-            else:
-                format = ("ran out of shares: %(complete)d complete,"
-                          " %(pending)d pending, %(overdue)d overdue,"
-                          " %(unused)d unused, need %(k)d."
-                          " Last failure: %(last_failure)s")
-                args = {"complete": self._count_shnums(COMPLETE),
-                        "pending": self._count_shnums(PENDING),
-                        "overdue": self._count_shnums(OVERDUE),
-                        # 'unused' should be zero
-                        "unused": self._count_shnums(AVAILABLE),
-                        "k": k,
-                        "last_failure": self._last_failure,
-                        }
-                error = NotEnoughSharesError
-            log.msg(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
-            e = error(format % args)
-            f = Failure(e)
-            self.stop()
-            self._node.fetch_failed(self, f)
-            return
-
-        # nope, not done. Are we "block-hungry" (i.e. do we want to send out
-        # more read requests, or do we think we have enough in flight
-        # already?)
-        while self._count_shnums(PENDING, COMPLETE) < k:
-            # we're hungry.. are there any unused shares?
-            sent = self._send_new_request()
-            if not sent:
-                break
-
-        # ok, now are we "share-hungry" (i.e. do we have enough known shares
-        # to make us happy, or should we ask the ShareFinder to get us more?)
-        if self._count_shnums(AVAILABLE, PENDING, COMPLETE) < k:
-            # we're hungry for more shares
-            self._node.want_more_shares()
-            # that will trigger the ShareFinder to keep looking
-
-    def _find_one(self, shares, state):
-        # TODO could choose fastest
-        for s in shares:
-            if self._shares[s] == state:
-                return s
-        # can never get here, caller has assert in case of code bug
-
-    def _send_new_request(self):
-        for shnum,shares in sorted(self._shnums.iteritems()):
-            states = [self._shares[s] for s in shares]
-            if COMPLETE in states or PENDING in states:
-                # don't send redundant requests
-                continue
-            if AVAILABLE not in states:
-                # no candidates for this shnum, move on
-                continue
-            # here's a candidate. Send a request.
-            s = self._find_one(shares, AVAILABLE)
-            assert s
-            self._shares[s] = PENDING
-            self._share_observers[s] = o = s.get_block(self.segnum)
-            o.subscribe(self._block_request_activity, share=s, shnum=shnum)
-            # TODO: build up a list of candidates, then walk through the
-            # list, sending requests to the most desireable servers,
-            # re-checking our block-hunger each time. For non-initial segment
-            # fetches, this would let us stick with faster servers.
-            return True
-        # nothing was sent: don't call us again until you have more shares to
-        # work with, or one of the existing shares has been declared OVERDUE
-        return False
-
-    def _cancel_all_requests(self):
-        for o in self._share_observers.values():
-            o.cancel()
-        self._share_observers = {}
-
-    def _block_request_activity(self, share, shnum, state, block=None, f=None):
-        # called by Shares, in response to our s.send_request() calls.
-        if not self._running:
-            return
-        log.msg("SegmentFetcher(%s)._block_request_activity:"
-                " Share(sh%d-on-%s) -> %s" %
-                (self._node._si_prefix, shnum, share._peerid_s, state),
-                level=log.NOISY, umid="vilNWA")
-        # COMPLETE, CORRUPT, DEAD, BADSEGNUM are terminal.
-        if state in (COMPLETE, CORRUPT, DEAD, BADSEGNUM):
-            self._share_observers.pop(share, None)
-        if state is COMPLETE:
-            # 'block' is fully validated
-            self._shares[share] = COMPLETE
-            self._blocks[shnum] = block
-        elif state is OVERDUE:
-            self._shares[share] = OVERDUE
-            # OVERDUE is not terminal: it will eventually transition to
-            # COMPLETE, CORRUPT, or DEAD.
-        elif state is CORRUPT:
-            self._shares[share] = CORRUPT
-        elif state is DEAD:
-            del self._shares[share]
-            self._shnums[shnum].remove(share)
-            self._last_failure = f
-        elif state is BADSEGNUM:
-            self._shares[share] = BADSEGNUM # ???
-            self._bad_segnum = True
-        eventually(self.loop)
-
-
-class RequestToken:
-    def __init__(self, peerid):
-        self.peerid = peerid
-
-class ShareFinder:
-    def __init__(self, storage_broker, verifycap, node, download_status,
-                 logparent=None, max_outstanding_requests=10):
-        self.running = True # stopped by Share.stop, from Terminator
-        self.verifycap = verifycap
-        self._started = False
-        self._storage_broker = storage_broker
-        self.share_consumer = self.node = node
-        self.max_outstanding_requests = max_outstanding_requests
-
-        self._hungry = False
-
-        self._commonshares = {} # shnum to CommonShare instance
-        self.undelivered_shares = []
-        self.pending_requests = set()
-
-        self._storage_index = verifycap.storage_index
-        self._si_prefix = base32.b2a_l(self._storage_index[:8], 60)
-        self._node_logparent = logparent
-        self._download_status = download_status
-        self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
-                           si=self._si_prefix,
-                           level=log.NOISY, parent=logparent, umid="2xjj2A")
-
-    def start_finding_servers(self):
-        # don't get servers until somebody uses us: creating the
-        # ImmutableFileNode should not cause work to happen yet. Test case is
-        # test_dirnode, which creates us with storage_broker=None
-        if not self._started:
-            si = self.verifycap.storage_index
-            s = self._storage_broker.get_servers_for_index(si)
-            self._servers = iter(s)
-            self._started = True
-
-    def log(self, *args, **kwargs):
-        if "parent" not in kwargs:
-            kwargs["parent"] = self._lp
-        return log.msg(*args, **kwargs)
-
-    def stop(self):
-        self.running = False
-
-    # called by our parent CiphertextDownloader
-    def hungry(self):
-        self.log(format="ShareFinder[si=%(si)s] hungry",
-                 si=self._si_prefix, level=log.NOISY, umid="NywYaQ")
-        self.start_finding_servers()
-        self._hungry = True
-        eventually(self.loop)
-
-    # internal methods
-    def loop(self):
-        undelivered_s = ",".join(["sh%d@%s" %
-                                  (s._shnum, idlib.shortnodeid_b2a(s._peerid))
-                                  for s in self.undelivered_shares])
-        pending_s = ",".join([idlib.shortnodeid_b2a(rt.peerid)
-                              for rt in self.pending_requests]) # sort?
-        self.log(format="ShareFinder loop: running=%(running)s"
-                 " hungry=%(hungry)s, undelivered=%(undelivered)s,"
-                 " pending=%(pending)s",
-                 running=self.running, hungry=self._hungry,
-                 undelivered=undelivered_s, pending=pending_s,
-                 level=log.NOISY, umid="kRtS4Q")
-        if not self.running:
-            return
-        if not self._hungry:
-            return
-        if self.undelivered_shares:
-            sh = self.undelivered_shares.pop(0)
-            # they will call hungry() again if they want more
-            self._hungry = False
-            self.log(format="delivering Share(shnum=%(shnum)d, server=%(peerid)s)",
-                     shnum=sh._shnum, peerid=sh._peerid_s,
-                     level=log.NOISY, umid="2n1qQw")
-            eventually(self.share_consumer.got_shares, [sh])
-            return
-
-        if len(self.pending_requests) >= self.max_outstanding_requests:
-            # cannot send more requests, must wait for some to retire
-            return
-
-        server = None
-        try:
-            if self._servers:
-                server = self._servers.next()
-        except StopIteration:
-            self._servers = None
-
-        if server:
-            self.send_request(server)
-            # we loop again to get parallel queries. The check above will
-            # prevent us from looping forever.
-            eventually(self.loop)
-            return
-
-        if self.pending_requests:
-            # no server, but there are still requests in flight: maybe one of
-            # them will make progress
-            return
-
-        self.log(format="ShareFinder.loop: no_more_shares, ever",
-                 level=log.UNUSUAL, umid="XjQlzg")
-        # we've run out of servers (so we can't send any more requests), and
-        # we have nothing in flight. No further progress can be made. They
-        # are destined to remain hungry.
-        self.share_consumer.no_more_shares()
-
-    def send_request(self, server):
-        peerid, rref = server
-        req = RequestToken(peerid)
-        self.pending_requests.add(req)
-        lp = self.log(format="sending DYHB to [%(peerid)s]",
-                      peerid=idlib.shortnodeid_b2a(peerid),
-                      level=log.NOISY, umid="Io7pyg")
-        d_ev = self._download_status.add_dyhb_sent(peerid, now())
-        d = rref.callRemote("get_buckets", self._storage_index)
-        d.addBoth(incidentally, self.pending_requests.discard, req)
-        d.addCallbacks(self._got_response, self._got_error,
-                       callbackArgs=(rref.version, peerid, req, d_ev, lp),
-                       errbackArgs=(peerid, req, d_ev, lp))
-        d.addErrback(log.err, format="error in send_request",
-                     level=log.WEIRD, parent=lp, umid="rpdV0w")
-        d.addCallback(incidentally, eventually, self.loop)
-
-    def _got_response(self, buckets, server_version, peerid, req, d_ev, lp):
-        shnums = sorted([shnum for shnum in buckets])
-        d_ev.finished(shnums, now())
-        if buckets:
-            shnums_s = ",".join([str(shnum) for shnum in shnums])
-            self.log(format="got shnums [%(shnums)s] from [%(peerid)s]",
-                     shnums=shnums_s, peerid=idlib.shortnodeid_b2a(peerid),
-                     level=log.NOISY, parent=lp, umid="0fcEZw")
-        else:
-            self.log(format="no shares from [%(peerid)s]",
-                     peerid=idlib.shortnodeid_b2a(peerid),
-                     level=log.NOISY, parent=lp, umid="U7d4JA")
-        if self.node.num_segments is None:
-            best_numsegs = self.node.guessed_num_segments
-        else:
-            best_numsegs = self.node.num_segments
-        for shnum, bucket in buckets.iteritems():
-            if shnum in self._commonshares:
-                cs = self._commonshares[shnum]
-            else:
-                cs = CommonShare(best_numsegs, self._si_prefix, shnum,
-                                 self._node_logparent)
-                # Share._get_satisfaction is responsible for updating
-                # CommonShare.set_numsegs after we know the UEB. Alternatives:
-                #  1: d = self.node.get_num_segments()
-                #     d.addCallback(cs.got_numsegs)
-                #   the problem is that the OneShotObserverList I was using
-                #   inserts an eventual-send between _get_satisfaction's
-                #   _satisfy_UEB and _satisfy_block_hash_tree, and the
-                #   CommonShare didn't get the num_segs message before
-                #   being asked to set block hash values. To resolve this
-                #   would require an immediate ObserverList instead of
-                #   an eventual-send -based one
-                #  2: break _get_satisfaction into Deferred-attached pieces.
-                #     Yuck.
-                self._commonshares[shnum] = cs
-            s = Share(bucket, server_version, self.verifycap, cs, self.node,
-                      self._download_status, peerid, shnum,
-                      self._node_logparent)
-            self.undelivered_shares.append(s)
-
-    def _got_error(self, f, peerid, req, d_ev, lp):
-        d_ev.finished("error", now())
-        self.log(format="got error from [%(peerid)s]",
-                 peerid=idlib.shortnodeid_b2a(peerid), failure=f,
-                 level=log.UNUSUAL, parent=lp, umid="zUKdCw")
-
-
-
-class Segmentation:
-    """I am responsible for a single offset+size read of the file. I handle
-    segmentation: I figure out which segments are necessary, request them
-    (from my CiphertextDownloader) in order, and trim the segments down to
-    match the offset+size span. I use the Producer/Consumer interface to only
-    request one segment at a time.
-    """
-    implements(IPushProducer)
-    def __init__(self, node, offset, size, consumer, read_ev, logparent=None):
-        self._node = node
-        self._hungry = True
-        self._active_segnum = None
-        self._cancel_segment_request = None
-        # these are updated as we deliver data. At any given time, we still
-        # want to download file[offset:offset+size]
-        self._offset = offset
-        self._size = size
-        assert offset+size <= node._verifycap.size
-        self._consumer = consumer
-        self._read_ev = read_ev
-        self._start_pause = None
-        self._lp = logparent
-
-    def start(self):
-        self._alive = True
-        self._deferred = defer.Deferred()
-        self._consumer.registerProducer(self, True)
-        self._maybe_fetch_next()
-        return self._deferred
-
-    def _maybe_fetch_next(self):
-        if not self._alive or not self._hungry:
-            return
-        if self._active_segnum is not None:
-            return
-        self._fetch_next()
-
-    def _fetch_next(self):
-        if self._size == 0:
-            # done!
-            self._alive = False
-            self._hungry = False
-            self._consumer.unregisterProducer()
-            self._deferred.callback(self._consumer)
-            return
-        n = self._node
-        have_actual_segment_size = n.segment_size is not None
-        guess_s = ""
-        if not have_actual_segment_size:
-            guess_s = "probably "
-        segment_size = n.segment_size or n.guessed_segment_size
-        if self._offset == 0:
-            # great! we want segment0 for sure
-            wanted_segnum = 0
-        else:
-            # this might be a guess
-            wanted_segnum = self._offset // segment_size
-        log.msg(format="_fetch_next(offset=%(offset)d) %(guess)swants segnum=%(segnum)d",
-                offset=self._offset, guess=guess_s, segnum=wanted_segnum,
-                level=log.NOISY, parent=self._lp, umid="5WfN0w")
-        self._active_segnum = wanted_segnum
-        d,c = n.get_segment(wanted_segnum, self._lp)
-        self._cancel_segment_request = c
-        d.addBoth(self._request_retired)
-        d.addCallback(self._got_segment, wanted_segnum)
-        if not have_actual_segment_size:
-            # we can retry once
-            d.addErrback(self._retry_bad_segment)
-        d.addErrback(self._error)
-
-    def _request_retired(self, res):
-        self._active_segnum = None
-        self._cancel_segment_request = None
-        return res
-
-    def _got_segment(self, (segment_start,segment,decodetime), wanted_segnum):
-        self._cancel_segment_request = None
-        # we got file[segment_start:segment_start+len(segment)]
-        # we want file[self._offset:self._offset+self._size]
-        log.msg(format="Segmentation got data:"
-                " want [%(wantstart)d-%(wantend)d),"
-                " given [%(segstart)d-%(segend)d), for segnum=%(segnum)d",
-                wantstart=self._offset, wantend=self._offset+self._size,
-                segstart=segment_start, segend=segment_start+len(segment),
-                segnum=wanted_segnum,
-                level=log.OPERATIONAL, parent=self._lp, umid="32dHcg")
-
-        o = overlap(segment_start, len(segment),  self._offset, self._size)
-        # the overlap is file[o[0]:o[0]+o[1]]
-        if not o or o[0] != self._offset:
-            # we didn't get the first byte, so we can't use this segment
-            log.msg("Segmentation handed wrong data:"
-                    " want [%d-%d), given [%d-%d), for segnum=%d,"
-                    " for si=%s"
-                    % (self._offset, self._offset+self._size,
-                       segment_start, segment_start+len(segment),
-                       wanted_segnum, self._node._si_prefix),
-                    level=log.UNUSUAL, parent=self._lp, umid="STlIiA")
-            # we may retry if the segnum we asked was based on a guess
-            raise WrongSegmentError("I was given the wrong data.")
-        offset_in_segment = self._offset - segment_start
-        desired_data = segment[offset_in_segment:offset_in_segment+o[1]]
-
-        self._offset += len(desired_data)
-        self._size -= len(desired_data)
-        self._consumer.write(desired_data)
-        # the consumer might call our .pauseProducing() inside that write()
-        # call, setting self._hungry=False
-        self._read_ev.update(len(desired_data), 0, 0)
-        self._maybe_fetch_next()
-
-    def _retry_bad_segment(self, f):
-        f.trap(WrongSegmentError, BadSegmentNumberError)
-        # we guessed the segnum wrong: either one that doesn't overlap with
-        # the start of our desired region, or one that's beyond the end of
-        # the world. Now that we have the right information, we're allowed to
-        # retry once.
-        assert self._node.segment_size is not None
-        return self._maybe_fetch_next()
-
-    def _error(self, f):
-        log.msg("Error in Segmentation", failure=f,
-                level=log.WEIRD, parent=self._lp, umid="EYlXBg")
-        self._alive = False
-        self._hungry = False
-        self._consumer.unregisterProducer()
-        self._deferred.errback(f)
-
-    def stopProducing(self):
-        self._hungry = False
-        self._alive = False
-        # cancel any outstanding segment request
-        if self._cancel_segment_request:
-            self._cancel_segment_request.cancel()
-            self._cancel_segment_request = None
-    def pauseProducing(self):
-        self._hungry = False
-        self._start_pause = now()
-    def resumeProducing(self):
-        self._hungry = True
-        eventually(self._maybe_fetch_next)
-        if self._start_pause is not None:
-            paused = now() - self._start_pause
-            self._read_ev.update(0, 0, paused)
-            self._start_pause = None
-
-class Cancel:
-    def __init__(self, f):
-        self._f = f
-        self.cancelled = False
-    def cancel(self):
-        if not self.cancelled:
-            self.cancelled = True
-            self._f(self)
-
-class _Node:
-    """Internal class which manages downloads and holds state. External
-    callers use CiphertextFileNode instead."""
-
-    # Share._node points to me
-    def __init__(self, verifycap, storage_broker, secret_holder,
-                 terminator, history, download_status):
-        assert isinstance(verifycap, uri.CHKFileVerifierURI)
-        self._verifycap = verifycap
-        self._storage_broker = storage_broker
-        self._si_prefix = base32.b2a_l(verifycap.storage_index[:8], 60)
-        self.running = True
-        if terminator:
-            terminator.register(self) # calls self.stop() at stopService()
-        # the rules are:
-        # 1: Only send network requests if you're active (self.running is True)
-        # 2: Use TimerService, not reactor.callLater
-        # 3: You can do eventual-sends any time.
-        # These rules should mean that once
-        # stopService()+flushEventualQueue() fires, everything will be done.
-        self._secret_holder = secret_holder
-        self._history = history
-        self._download_status = download_status
-
-        k, N = self._verifycap.needed_shares, self._verifycap.total_shares
-        self.share_hash_tree = IncompleteHashTree(N)
-
-        # we guess the segment size, so Segmentation can pull non-initial
-        # segments in a single roundtrip. This populates
-        # .guessed_segment_size, .guessed_num_segments, and
-        # .ciphertext_hash_tree (with a dummy, to let us guess which hashes
-        # we'll need)
-        self._build_guessed_tables(DEFAULT_MAX_SEGMENT_SIZE)
-
-        # filled in when we parse a valid UEB
-        self.have_UEB = False
-        self.segment_size = None
-        self.tail_segment_size = None
-        self.tail_segment_padded = None
-        self.num_segments = None
-        self.block_size = None
-        self.tail_block_size = None
-
-        # things to track callers that want data
-
-        # _segment_requests can have duplicates
-        self._segment_requests = [] # (segnum, d, cancel_handle)
-        self._active_segment = None # a SegmentFetcher, with .segnum
-
-        # we create one top-level logparent for this _Node, and another one
-        # for each read() call. Segmentation and get_segment() messages are
-        # associated with the read() call, everything else is tied to the
-        # _Node's log entry.
-        lp = log.msg(format="Immutable _Node(%(si)s) created: size=%(size)d,"
-                     " guessed_segsize=%(guessed_segsize)d,"
-                     " guessed_numsegs=%(guessed_numsegs)d",
-                     si=self._si_prefix, size=verifycap.size,
-                     guessed_segsize=self.guessed_segment_size,
-                     guessed_numsegs=self.guessed_num_segments,
-                     level=log.OPERATIONAL, umid="uJ0zAQ")
-        self._lp = lp
-
-        self._sharefinder = ShareFinder(storage_broker, verifycap, self,
-                                        self._download_status, lp)
-        self._shares = set()
-
-    def _build_guessed_tables(self, max_segment_size):
-        size = min(self._verifycap.size, max_segment_size)
-        s = mathutil.next_multiple(size, self._verifycap.needed_shares)
-        self.guessed_segment_size = s
-        r = self._calculate_sizes(self.guessed_segment_size)
-        self.guessed_num_segments = r["num_segments"]
-        # as with CommonShare, our ciphertext_hash_tree is a stub until we
-        # get the real num_segments
-        self.ciphertext_hash_tree = IncompleteHashTree(self.guessed_num_segments)
-
-    def __repr__(self):
-        return "Imm_Node(%s)" % (self._si_prefix,)
-
-    def stop(self):
-        # called by the Terminator at shutdown, mostly for tests
-        if self._active_segment:
-            self._active_segment.stop()
-            self._active_segment = None
-        self._sharefinder.stop()
-
-    # things called by outside callers, via CiphertextFileNode. get_segment()
-    # may also be called by Segmentation.
-
-    def read(self, consumer, offset=0, size=None, read_ev=None):
-        """I am the main entry point, from which FileNode.read() can get
-        data. I feed the consumer with the desired range of ciphertext. I
-        return a Deferred that fires (with the consumer) when the read is
-        finished.
-
-        Note that there is no notion of a 'file pointer': each call to read()
-        uses an independent offset= value."""
-        # for concurrent operations: each gets its own Segmentation manager
-        if size is None:
-            size = self._verifycap.size
-        # clip size so offset+size does not go past EOF
-        size = min(size, self._verifycap.size-offset)
-        if read_ev is None:
-            read_ev = self._download_status.add_read_event(offset, size, now())
-
-        lp = log.msg(format="imm Node(%(si)s).read(%(offset)d, %(size)d)",
-                     si=base32.b2a(self._verifycap.storage_index)[:8],
-                     offset=offset, size=size,
-                     level=log.OPERATIONAL, parent=self._lp, umid="l3j3Ww")
-        if self._history:
-            sp = self._history.stats_provider
-            sp.count("downloader.files_downloaded", 1) # really read() calls
-            sp.count("downloader.bytes_downloaded", size)
-        s = Segmentation(self, offset, size, consumer, read_ev, lp)
-        # this raises an interesting question: what segments to fetch? if
-        # offset=0, always fetch the first segment, and then allow
-        # Segmentation to be responsible for pulling the subsequent ones if
-        # the first wasn't large enough. If offset>0, we're going to need an
-        # extra roundtrip to get the UEB (and therefore the segment size)
-        # before we can figure out which segment to get. TODO: allow the
-        # offset-table-guessing code (which starts by guessing the segsize)
-        # to assist the offset>0 process.
-        d = s.start()
-        def _done(res):
-            read_ev.finished(now())
-            return res
-        d.addBoth(_done)
-        return d
-
-    def get_segment(self, segnum, logparent=None):
-        """Begin downloading a segment. I return a tuple (d, c): 'd' is a
-        Deferred that fires with (offset,data) when the desired segment is
-        available, and c is an object on which c.cancel() can be called to
-        disavow interest in the segment (after which 'd' will never fire).
-
-        You probably need to know the segment size before calling this,
-        unless you want the first few bytes of the file. If you ask for a
-        segment number which turns out to be too large, the Deferred will
-        errback with BadSegmentNumberError.
-
-        The Deferred fires with the offset of the first byte of the data
-        segment, so that you can call get_segment() before knowing the
-        segment size, and still know which data you received.
-
-        The Deferred can also errback with other fatal problems, such as
-        NotEnoughSharesError, NoSharesError, or BadCiphertextHashError.
-        """
-        log.msg(format="imm Node(%(si)s).get_segment(%(segnum)d)",
-                si=base32.b2a(self._verifycap.storage_index)[:8],
-                segnum=segnum,
-                level=log.OPERATIONAL, parent=logparent, umid="UKFjDQ")
-        self._download_status.add_segment_request(segnum, now())
-        d = defer.Deferred()
-        c = Cancel(self._cancel_request)
-        self._segment_requests.append( (segnum, d, c) )
-        self._start_new_segment()
-        return (d, c)
-
-    # things called by the Segmentation object used to transform
-    # arbitrary-sized read() calls into quantized segment fetches
-
-    def _start_new_segment(self):
-        if self._active_segment is None and self._segment_requests:
-            segnum = self._segment_requests[0][0]
-            k = self._verifycap.needed_shares
-            log.msg(format="%(node)s._start_new_segment: segnum=%(segnum)d",
-                    node=repr(self), segnum=segnum,
-                    level=log.NOISY, umid="wAlnHQ")
-            self._active_segment = fetcher = SegmentFetcher(self, segnum, k)
-            active_shares = [s for s in self._shares if s.is_alive()]
-            fetcher.add_shares(active_shares) # this triggers the loop
-
-
-    # called by our child ShareFinder
-    def got_shares(self, shares):
-        self._shares.update(shares)
-        if self._active_segment:
-            self._active_segment.add_shares(shares)
-    def no_more_shares(self):
-        self._no_more_shares = True
-        if self._active_segment:
-            self._active_segment.no_more_shares()
-
-    # things called by our Share instances
-
-    def validate_and_store_UEB(self, UEB_s):
-        log.msg("validate_and_store_UEB",
-                level=log.OPERATIONAL, parent=self._lp, umid="7sTrPw")
-        h = hashutil.uri_extension_hash(UEB_s)
-        if h != self._verifycap.uri_extension_hash:
-            raise BadHashError
-        UEB_dict = uri.unpack_extension(UEB_s)
-        self._parse_and_store_UEB(UEB_dict) # sets self._stuff
-        # TODO: a malformed (but authentic) UEB could throw an assertion in
-        # _parse_and_store_UEB, and we should abandon the download.
-        self.have_UEB = True
-
-    def _parse_and_store_UEB(self, d):
-        # Note: the UEB contains needed_shares and total_shares. These are
-        # redundant and inferior (the filecap contains the authoritative
-        # values). However, because it is possible to encode the same file in
-        # multiple ways, and the encoders might choose (poorly) to use the
-        # same key for both (therefore getting the same SI), we might
-        # encounter shares for both types. The UEB hashes will be different,
-        # however, and we'll disregard the "other" encoding's shares as
-        # corrupted.
-
-        # therefore, we ignore d['total_shares'] and d['needed_shares'].
-
-        log.msg(format="UEB=%(ueb)s, vcap=%(vcap)s",
-                ueb=repr(d), vcap=self._verifycap.to_string(),
-                level=log.NOISY, parent=self._lp, umid="cVqZnA")
-
-        k, N = self._verifycap.needed_shares, self._verifycap.total_shares
-
-        self.segment_size = d['segment_size']
-
-        r = self._calculate_sizes(self.segment_size)
-        self.tail_segment_size = r["tail_segment_size"]
-        self.tail_segment_padded = r["tail_segment_padded"]
-        self.num_segments = r["num_segments"]
-        self.block_size = r["block_size"]
-        self.tail_block_size = r["tail_block_size"]
-        log.msg("actual sizes: %s" % (r,),
-                level=log.NOISY, parent=self._lp, umid="PY6P5Q")
-        if (self.segment_size == self.guessed_segment_size
-            and self.num_segments == self.guessed_num_segments):
-            log.msg("my guess was right!",
-                    level=log.NOISY, parent=self._lp, umid="x340Ow")
-        else:
-            log.msg("my guess was wrong! Extra round trips for me.",
-                    level=log.NOISY, parent=self._lp, umid="tb7RJw")
-
-        # zfec.Decode() instantiation is fast, but still, let's use the same
-        # codec instance for all but the last segment. 3-of-10 takes 15us on
-        # my laptop, 25-of-100 is 900us, 3-of-255 is 97us, 25-of-255 is
-        # 2.5ms, worst-case 254-of-255 is 9.3ms
-        self._codec = CRSDecoder()
-        self._codec.set_params(self.segment_size, k, N)
-
-
-        # Ciphertext hash tree root is mandatory, so that there is at most
-        # one ciphertext that matches this read-cap or verify-cap. The
-        # integrity check on the shares is not sufficient to prevent the
-        # original encoder from creating some shares of file A and other
-        # shares of file B. self.ciphertext_hash_tree was a guess before:
-        # this is where we create it for real.
-        self.ciphertext_hash_tree = IncompleteHashTree(self.num_segments)
-        self.ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
-
-        self.share_hash_tree.set_hashes({0: d['share_root_hash']})
-
-        # Our job is a fast download, not verification, so we ignore any
-        # redundant fields. The Verifier uses a different code path which
-        # does not ignore them.
-
-    def _calculate_sizes(self, segment_size):
-        # segments of ciphertext
-        size = self._verifycap.size
-        k = self._verifycap.needed_shares
-
-        # this assert matches the one in encode.py:127 inside
-        # Encoded._got_all_encoding_parameters, where the UEB is constructed
-        assert segment_size % k == 0
-
-        # the last segment is usually short. We don't store a whole segsize,
-        # but we do pad the segment up to a multiple of k, because the
-        # encoder requires that.
-        tail_segment_size = size % segment_size
-        if tail_segment_size == 0:
-            tail_segment_size = segment_size
-        padded = mathutil.next_multiple(tail_segment_size, k)
-        tail_segment_padded = padded
-
-        num_segments = mathutil.div_ceil(size, segment_size)
-
-        # each segment is turned into N blocks. All but the last are of size
-        # block_size, and the last is of size tail_block_size
-        block_size = segment_size / k
-        tail_block_size = tail_segment_padded / k
-
-        return { "tail_segment_size": tail_segment_size,
-                 "tail_segment_padded": tail_segment_padded,
-                 "num_segments": num_segments,
-                 "block_size": block_size,
-                 "tail_block_size": tail_block_size,
-                 }
-
-
-    def process_share_hashes(self, share_hashes):
-        for hashnum in share_hashes:
-            if hashnum >= len(self.share_hash_tree):
-                # "BadHashError" is normally for e.g. a corrupt block. We
-                # sort of abuse it here to mean a badly numbered hash (which
-                # indicates corruption in the number bytes, rather than in
-                # the data bytes).
-                raise BadHashError("hashnum %d doesn't fit in hashtree(%d)"
-                                   % (hashnum, len(self.share_hash_tree)))
-        self.share_hash_tree.set_hashes(share_hashes)
-
-    def get_needed_ciphertext_hashes(self, segnum):
-        cht = self.ciphertext_hash_tree
-        return cht.needed_hashes(segnum, include_leaf=True)
-    def process_ciphertext_hashes(self, hashes):
-        assert self.num_segments is not None
-        # this may raise BadHashError or NotEnoughHashesError
-        self.ciphertext_hash_tree.set_hashes(hashes)
-
-
-    # called by our child SegmentFetcher
-
-    def want_more_shares(self):
-        self._sharefinder.hungry()
-
-    def fetch_failed(self, sf, f):
-        assert sf is self._active_segment
-        self._active_segment = None
-        # deliver error upwards
-        for (d,c) in self._extract_requests(sf.segnum):
-            eventually(self._deliver, d, c, f)
-
-    def process_blocks(self, segnum, blocks):
-        d = defer.maybeDeferred(self._decode_blocks, segnum, blocks)
-        d.addCallback(self._check_ciphertext_hash, segnum)
-        def _deliver(result):
-            ds = self._download_status
-            if isinstance(result, Failure):
-                ds.add_segment_error(segnum, now())
-            else:
-                (offset, segment, decodetime) = result
-                ds.add_segment_delivery(segnum, now(),
-                                        offset, len(segment), decodetime)
-            log.msg(format="delivering segment(%(segnum)d)",
-                    segnum=segnum,
-                    level=log.OPERATIONAL, parent=self._lp,
-                    umid="j60Ojg")
-            for (d,c) in self._extract_requests(segnum):
-                eventually(self._deliver, d, c, result)
-            self._active_segment = None
-            self._start_new_segment()
-        d.addBoth(_deliver)
-        d.addErrback(lambda f:
-                     log.err("unhandled error during process_blocks",
-                             failure=f, level=log.WEIRD,
-                             parent=self._lp, umid="MkEsCg"))
-
-    def _decode_blocks(self, segnum, blocks):
-        tail = (segnum == self.num_segments-1)
-        codec = self._codec
-        block_size = self.block_size
-        decoded_size = self.segment_size
-        if tail:
-            # account for the padding in the last segment
-            codec = CRSDecoder()
-            k, N = self._verifycap.needed_shares, self._verifycap.total_shares
-            codec.set_params(self.tail_segment_padded, k, N)
-            block_size = self.tail_block_size
-            decoded_size = self.tail_segment_padded
-
-        shares = []
-        shareids = []
-        for (shareid, share) in blocks.iteritems():
-            assert len(share) == block_size
-            shareids.append(shareid)
-            shares.append(share)
-        del blocks
-
-        start = now()
-        d = codec.decode(shares, shareids)   # segment
-        del shares
-        def _process(buffers):
-            decodetime = now() - start
-            segment = "".join(buffers)
-            assert len(segment) == decoded_size
-            del buffers
-            if tail:
-                segment = segment[:self.tail_segment_size]
-            return (segment, decodetime)
-        d.addCallback(_process)
-        return d
-
-    def _check_ciphertext_hash(self, (segment, decodetime), segnum):
-        assert self._active_segment.segnum == segnum
-        assert self.segment_size is not None
-        offset = segnum * self.segment_size
-
-        h = hashutil.crypttext_segment_hash(segment)
-        try:
-            self.ciphertext_hash_tree.set_hashes(leaves={segnum: h})
-            return (offset, segment, decodetime)
-        except (BadHashError, NotEnoughHashesError):
-            format = ("hash failure in ciphertext_hash_tree:"
-                      " segnum=%(segnum)d, SI=%(si)s")
-            log.msg(format=format, segnum=segnum, si=self._si_prefix,
-                    failure=Failure(),
-                    level=log.WEIRD, parent=self._lp, umid="MTwNnw")
-            # this is especially weird, because we made it past the share
-            # hash tree. It implies that we're using the wrong encoding, or
-            # that the uploader deliberately constructed a bad UEB.
-            msg = format % {"segnum": segnum, "si": self._si_prefix}
-            raise BadCiphertextHashError(msg)
-
-    def _deliver(self, d, c, result):
-        # this method exists to handle cancel() that occurs between
-        # _got_segment and _deliver
-        if not c.cancelled:
-            d.callback(result) # might actually be an errback
-
-    def _extract_requests(self, segnum):
-        """Remove matching requests and return their (d,c) tuples so that the
-        caller can retire them."""
-        retire = [(d,c) for (segnum0, d, c) in self._segment_requests
-                  if segnum0 == segnum]
-        self._segment_requests = [t for t in self._segment_requests
-                                  if t[0] != segnum]
-        return retire
-
-    def _cancel_request(self, c):
-        self._segment_requests = [t for t in self._segment_requests
-                                  if t[2] != c]
-        segnums = [segnum for (segnum,d,c) in self._segment_requests]
-        if self._active_segment.segnum not in segnums:
-            self._active_segment.stop()
-            self._active_segment = None
-            self._start_new_segment()
-
-    def check_and_repair(self, monitor, verify=False, add_lease=False):
-        verifycap = self._verifycap
-        storage_index = verifycap.storage_index
-        sb = self._storage_broker
-        servers = sb.get_all_servers()
-        sh = self._secret_holder
-
-        c = Checker(verifycap=verifycap, servers=servers,
-                    verify=verify, add_lease=add_lease, secret_holder=sh,
-                    monitor=monitor)
-        d = c.start()
-        def _maybe_repair(cr):
-            crr = CheckAndRepairResults(storage_index)
-            crr.pre_repair_results = cr
-            if cr.is_healthy():
-                crr.post_repair_results = cr
-                return defer.succeed(crr)
-            else:
-                crr.repair_attempted = True
-                crr.repair_successful = False # until proven successful
-                def _gather_repair_results(ur):
-                    assert IUploadResults.providedBy(ur), ur
-                    # clone the cr (check results) to form the basis of the
-                    # prr (post-repair results)
-                    prr = CheckResults(cr.uri, cr.storage_index)
-                    prr.data = copy.deepcopy(cr.data)
-
-                    sm = prr.data['sharemap']
-                    assert isinstance(sm, DictOfSets), sm
-                    sm.update(ur.sharemap)
-                    servers_responding = set(prr.data['servers-responding'])
-                    servers_responding.union(ur.sharemap.iterkeys())
-                    prr.data['servers-responding'] = list(servers_responding)
-                    prr.data['count-shares-good'] = len(sm)
-                    prr.data['count-good-share-hosts'] = len(sm)
-                    is_healthy = bool(len(sm) >= verifycap.total_shares)
-                    is_recoverable = bool(len(sm) >= verifycap.needed_shares)
-                    prr.set_healthy(is_healthy)
-                    prr.set_recoverable(is_recoverable)
-                    crr.repair_successful = is_healthy
-                    prr.set_needs_rebalancing(len(sm) >= verifycap.total_shares)
-
-                    crr.post_repair_results = prr
-                    return crr
-                def _repair_error(f):
-                    # as with mutable repair, I'm not sure if I want to pass
-                    # through a failure or not. TODO
-                    crr.repair_successful = False
-                    crr.repair_failure = f
-                    return f
-                r = Repairer(storage_broker=sb, secret_holder=sh,
-                             verifycap=verifycap, monitor=monitor)
-                d = r.start()
-                d.addCallbacks(_gather_repair_results, _repair_error)
-                return d
-
-        d.addCallback(_maybe_repair)
-        return d
-
-    def check(self, monitor, verify=False, add_lease=False):
-        verifycap = self._verifycap
-        sb = self._storage_broker
-        servers = sb.get_all_servers()
-        sh = self._secret_holder
-
-        v = Checker(verifycap=verifycap, servers=servers,
-                    verify=verify, add_lease=add_lease, secret_holder=sh,
-                    monitor=monitor)
-        return v.start()
-
 class CiphertextFileNode:
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history, download_status=None):
@@ -1925,8 +31,8 @@ class CiphertextFileNode:
             if history:
                 history.add_download(ds)
             download_status = ds
-        self._node = _Node(verifycap, storage_broker, secret_holder,
-                           terminator, history, download_status)
+        self._node = DownloadNode(verifycap, storage_broker, secret_holder,
+                                  terminator, history, download_status)
 
     def read(self, consumer, offset=0, size=None, read_ev=None):
         """I am the main entry point, from which FileNode.read() can get
@@ -2150,169 +256,3 @@ class ImmutableFileNode:
 # tests to write:
 # * truncated share, so _satisfy_* doesn't get all it wants
 # * slow server
-
-class RequestEvent:
-    def __init__(self, download_status, tag):
-        self._download_status = download_status
-        self._tag = tag
-    def finished(self, received, when):
-        self._download_status.add_request_finished(self._tag, received, when)
-
-class DYHBEvent:
-    def __init__(self, download_status, tag):
-        self._download_status = download_status
-        self._tag = tag
-    def finished(self, shnums, when):
-        self._download_status.add_dyhb_finished(self._tag, shnums, when)
-
-class ReadEvent:
-    def __init__(self, download_status, tag):
-        self._download_status = download_status
-        self._tag = tag
-    def update(self, bytes, decrypttime, pausetime):
-        self._download_status.update_read_event(self._tag, bytes,
-                                                decrypttime, pausetime)
-    def finished(self, finishtime):
-        self._download_status.finish_read_event(self._tag, finishtime)
-
-class DownloadStatus:
-    # There is one DownloadStatus for each CiphertextFileNode. The status
-    # object will keep track of all activity for that node.
-    implements(IDownloadStatus)
-    statusid_counter = itertools.count(0)
-
-    def __init__(self, storage_index, size):
-        self.storage_index = storage_index
-        self.size = size
-        self.counter = self.statusid_counter.next()
-        self.helper = False
-        self.started = None
-        # self.dyhb_requests tracks "do you have a share" requests and
-        # responses. It maps serverid to a tuple of:
-        #  send time
-        #  tuple of response shnums (None if response hasn't arrived, "error")
-        #  response time (None if response hasn't arrived yet)
-        self.dyhb_requests = {}
-
-        # self.requests tracks share-data requests and responses. It maps
-        # serverid to a tuple of:
-        #  shnum,
-        #  start,length,  (of data requested)
-        #  send time
-        #  response length (None if reponse hasn't arrived yet, or "error")
-        #  response time (None if response hasn't arrived)
-        self.requests = {}
-
-        # self.segment_events tracks segment requests and delivery. It is a
-        # list of:
-        #  type ("request", "delivery", "error")
-        #  segment number
-        #  event time
-        #  segment start (file offset of first byte, None except in "delivery")
-        #  segment length (only in "delivery")
-        #  time spent in decode (only in "delivery")
-        self.segment_events = []
-
-        # self.read_events tracks read() requests. It is a list of:
-        #  start,length  (of data requested)
-        #  request time
-        #  finish time (None until finished)
-        #  bytes returned (starts at 0, grows as segments are delivered)
-        #  time spent in decrypt (None for ciphertext-only reads)
-        #  time spent paused
-        self.read_events = []
-
-        self.known_shares = [] # (serverid, shnum)
-        self.problems = []
-
-
-    def add_dyhb_sent(self, serverid, when):
-        r = (when, None, None)
-        if serverid not in self.dyhb_requests:
-            self.dyhb_requests[serverid] = []
-        self.dyhb_requests[serverid].append(r)
-        tag = (serverid, len(self.dyhb_requests[serverid])-1)
-        return DYHBEvent(self, tag)
-
-    def add_dyhb_finished(self, tag, shnums, when):
-        # received="error" on error, else tuple(shnums)
-        (serverid, index) = tag
-        r = self.dyhb_requests[serverid][index]
-        (sent, _, _) = r
-        r = (sent, shnums, when)
-        self.dyhb_requests[serverid][index] = r
-
-    def add_request_sent(self, serverid, shnum, start, length, when):
-        r = (shnum, start, length, when, None, None)
-        if serverid not in self.requests:
-            self.requests[serverid] = []
-        self.requests[serverid].append(r)
-        tag = (serverid, len(self.requests[serverid])-1)
-        return RequestEvent(self, tag)
-
-    def add_request_finished(self, tag, received, when):
-        # received="error" on error, else len(data)
-        (serverid, index) = tag
-        r = self.requests[serverid][index]
-        (shnum, start, length, sent, _, _) = r
-        r = (shnum, start, length, sent, received, when)
-        self.requests[serverid][index] = r
-
-    def add_segment_request(self, segnum, when):
-        if self.started is None:
-            self.started = when
-        r = ("request", segnum, when, None, None, None)
-        self.segment_events.append(r)
-    def add_segment_delivery(self, segnum, when, start, length, decodetime):
-        r = ("delivery", segnum, when, start, length, decodetime)
-        self.segment_events.append(r)
-    def add_segment_error(self, segnum, when):
-        r = ("error", segnum, when, None, None, None)
-        self.segment_events.append(r)
-
-    def add_read_event(self, start, length, when):
-        if self.started is None:
-            self.started = when
-        r = (start, length, when, None, 0, 0, 0)
-        self.read_events.append(r)
-        tag = len(self.read_events)-1
-        return ReadEvent(self, tag)
-    def update_read_event(self, tag, bytes_d, decrypt_d, paused_d):
-        r = self.read_events[tag]
-        (start, length, requesttime, finishtime, bytes, decrypt, paused) = r
-        bytes += bytes_d
-        decrypt += decrypt_d
-        paused += paused_d
-        r = (start, length, requesttime, finishtime, bytes, decrypt, paused)
-        self.read_events[tag] = r
-    def finish_read_event(self, tag, finishtime):
-        r = self.read_events[tag]
-        (start, length, requesttime, _, bytes, decrypt, paused) = r
-        r = (start, length, requesttime, finishtime, bytes, decrypt, paused)
-        self.read_events[tag] = r
-
-    def add_known_share(self, serverid, shnum):
-        self.known_shares.append( (serverid, shnum) )
-
-    def add_problem(self, p):
-        self.problems.append(p)
-
-    # IDownloadStatus methods
-    def get_counter(self):
-        return self.counter
-    def get_storage_index(self):
-        return self.storage_index
-    def get_size(self):
-        return self.size
-    def get_status(self):
-        return "not impl yet" # TODO
-    def get_progress(self):
-        return 0.1 # TODO
-    def using_helper(self):
-        return False
-    def get_active(self):
-        return False # TODO
-    def get_started(self):
-        return self.started
-    def get_results(self):
-        return None # TODO
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
deleted file mode 100755
index d45f5cc..0000000
--- a/src/allmydata/immutable/download2_util.py
+++ /dev/null
@@ -1,73 +0,0 @@
-import weakref
-
-from twisted.application import service
-from foolscap.api import eventually
-
-class Observer2:
-    """A simple class to distribute multiple events to a single subscriber.
-    It accepts arbitrary kwargs, but no posargs."""
-    def __init__(self):
-        self._watcher = None
-        self._undelivered_results = []
-        self._canceler = None
-
-    def set_canceler(self, c, methname):
-        """I will call c.METHNAME(self) when somebody cancels me."""
-        # we use a weakref to avoid creating a cycle between us and the thing
-        # we're observing: they'll be holding a reference to us to compare
-        # against the value we pass to their canceler function. However,
-        # since bound methods are first-class objects (and not kept alive by
-        # the object they're bound to), we can't just stash a weakref to the
-        # bound cancel method. Instead, we must hold a weakref to the actual
-        # object, and obtain its cancel method later.
-        # http://code.activestate.com/recipes/81253-weakmethod/ has an
-        # alternative.
-        self._canceler = (weakref.ref(c), methname)
-
-    def subscribe(self, observer, **watcher_kwargs):
-        self._watcher = (observer, watcher_kwargs)
-        while self._undelivered_results:
-            self._notify(self._undelivered_results.pop(0))
-
-    def notify(self, **result_kwargs):
-        if self._watcher:
-            self._notify(result_kwargs)
-        else:
-            self._undelivered_results.append(result_kwargs)
-
-    def _notify(self, result_kwargs):
-        o, watcher_kwargs = self._watcher
-        kwargs = dict(result_kwargs)
-        kwargs.update(watcher_kwargs)
-        eventually(o, **kwargs)
-
-    def cancel(self):
-        wr,methname = self._canceler
-        o = wr()
-        if o:
-            getattr(o,methname)(self)
-
-
-def incidentally(res, f, *args, **kwargs):
-    """Add me to a Deferred chain like this:
-     d.addBoth(incidentally, func, arg)
-    and I'll behave as if you'd added the following function:
-     def _(res):
-         func(arg)
-         return res
-    This is useful if you want to execute an expression when the Deferred
-    fires, but don't care about its value.
-    """
-    f(*args, **kwargs)
-    return res
-
-
-class Terminator(service.Service):
-    def __init__(self):
-        self._clients = weakref.WeakKeyDictionary()
-    def register(self, c):
-        self._clients[c] = None
-    def stopService(self):
-        for c in self._clients:
-            c.stop()
-        return service.Service.stopService(self)
diff --git a/src/allmydata/immutable/downloader/__init__.py b/src/allmydata/immutable/downloader/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/allmydata/immutable/downloader/common.py b/src/allmydata/immutable/downloader/common.py
new file mode 100644
index 0000000..7364b8d
--- /dev/null
+++ b/src/allmydata/immutable/downloader/common.py
@@ -0,0 +1,11 @@
+
+(AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
+ ("AVAILABLE", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
+
+class BadSegmentNumberError(Exception):
+    pass
+class WrongSegmentError(Exception):
+    pass
+class BadCiphertextHashError(Exception):
+    pass
+
diff --git a/src/allmydata/immutable/downloader/fetcher.py b/src/allmydata/immutable/downloader/fetcher.py
new file mode 100644
index 0000000..2fd987b
--- /dev/null
+++ b/src/allmydata/immutable/downloader/fetcher.py
@@ -0,0 +1,228 @@
+
+from twisted.python.failure import Failure
+from foolscap.api import eventually
+from allmydata.interfaces import NotEnoughSharesError, NoSharesError
+from allmydata.util import log
+from allmydata.util.dictutil import DictOfSets
+from common import AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, \
+     BADSEGNUM, BadSegmentNumberError
+
+class SegmentFetcher:
+    """I am responsible for acquiring blocks for a single segment. I will use
+    the Share instances passed to my add_shares() method to locate, retrieve,
+    and validate those blocks. I expect my parent node to call my
+    no_more_shares() method when there are no more shares available. I will
+    call my parent's want_more_shares() method when I want more: I expect to
+    see at least one call to add_shares or no_more_shares afterwards.
+
+    When I have enough validated blocks, I will call my parent's
+    process_blocks() method with a dictionary that maps shnum to blockdata.
+    If I am unable to provide enough blocks, I will call my parent's
+    fetch_failed() method with (self, f). After either of these events, I
+    will shut down and do no further work. My parent can also call my stop()
+    method to have me shut down early."""
+
+    def __init__(self, node, segnum, k):
+        self._node = node # _Node
+        self.segnum = segnum
+        self._k = k
+        self._shares = {} # maps non-dead Share instance to a state, one of
+                          # (AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT).
+                          # State transition map is:
+                          #  AVAILABLE -(send-read)-> PENDING
+                          #  PENDING -(timer)-> OVERDUE
+                          #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
+                          #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
+                          # If a share becomes DEAD, it is removed from the
+                          # dict. If it becomes BADSEGNUM, the whole fetch is
+                          # terminated.
+        self._share_observers = {} # maps Share to Observer2 for active ones
+        self._shnums = DictOfSets() # maps shnum to the shares that provide it
+        self._blocks = {} # maps shnum to validated block data
+        self._no_more_shares = False
+        self._bad_segnum = False
+        self._last_failure = None
+        self._running = True
+
+    def stop(self):
+        log.msg("SegmentFetcher(%s).stop" % self._node._si_prefix,
+                level=log.NOISY, umid="LWyqpg")
+        self._cancel_all_requests()
+        self._running = False
+        self._shares.clear() # let GC work # ??? XXX
+
+
+    # called by our parent _Node
+
+    def add_shares(self, shares):
+        # called when ShareFinder locates a new share, and when a non-initial
+        # segment fetch is started and we already know about shares from the
+        # previous segment
+        for s in shares:
+            self._shares[s] = AVAILABLE
+            self._shnums.add(s._shnum, s)
+        eventually(self.loop)
+
+    def no_more_shares(self):
+        # ShareFinder tells us it's reached the end of its list
+        self._no_more_shares = True
+        eventually(self.loop)
+
+    # internal methods
+
+    def _count_shnums(self, *states):
+        """shnums for which at least one state is in the following list"""
+        shnums = []
+        for shnum,shares in self._shnums.iteritems():
+            matches = [s for s in shares if self._shares.get(s) in states]
+            if matches:
+                shnums.append(shnum)
+        return len(shnums)
+
+    def loop(self):
+        try:
+            # if any exception occurs here, kill the download
+            self._do_loop()
+        except BaseException:
+            self._node.fetch_failed(self, Failure())
+            raise
+
+    def _do_loop(self):
+        k = self._k
+        if not self._running:
+            return
+        if self._bad_segnum:
+            # oops, we were asking for a segment number beyond the end of the
+            # file. This is an error.
+            self.stop()
+            e = BadSegmentNumberError("segnum=%d, numsegs=%d" %
+                                      (self.segnum, self._node.num_segments))
+            f = Failure(e)
+            self._node.fetch_failed(self, f)
+            return
+
+        # are we done?
+        if self._count_shnums(COMPLETE) >= k:
+            # yay!
+            self.stop()
+            self._node.process_blocks(self.segnum, self._blocks)
+            return
+
+        # we may have exhausted everything
+        if (self._no_more_shares and
+            self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) < k):
+            # no more new shares are coming, and the remaining hopeful shares
+            # aren't going to be enough. boo!
+
+            log.msg("share states: %r" % (self._shares,),
+                    level=log.NOISY, umid="0ThykQ")
+            if self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) == 0:
+                format = ("no shares (need %(k)d)."
+                          " Last failure: %(last_failure)s")
+                args = { "k": k,
+                         "last_failure": self._last_failure }
+                error = NoSharesError
+            else:
+                format = ("ran out of shares: %(complete)d complete,"
+                          " %(pending)d pending, %(overdue)d overdue,"
+                          " %(unused)d unused, need %(k)d."
+                          " Last failure: %(last_failure)s")
+                args = {"complete": self._count_shnums(COMPLETE),
+                        "pending": self._count_shnums(PENDING),
+                        "overdue": self._count_shnums(OVERDUE),
+                        # 'unused' should be zero
+                        "unused": self._count_shnums(AVAILABLE),
+                        "k": k,
+                        "last_failure": self._last_failure,
+                        }
+                error = NotEnoughSharesError
+            log.msg(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
+            e = error(format % args)
+            f = Failure(e)
+            self.stop()
+            self._node.fetch_failed(self, f)
+            return
+
+        # nope, not done. Are we "block-hungry" (i.e. do we want to send out
+        # more read requests, or do we think we have enough in flight
+        # already?)
+        while self._count_shnums(PENDING, COMPLETE) < k:
+            # we're hungry.. are there any unused shares?
+            sent = self._send_new_request()
+            if not sent:
+                break
+
+        # ok, now are we "share-hungry" (i.e. do we have enough known shares
+        # to make us happy, or should we ask the ShareFinder to get us more?)
+        if self._count_shnums(AVAILABLE, PENDING, COMPLETE) < k:
+            # we're hungry for more shares
+            self._node.want_more_shares()
+            # that will trigger the ShareFinder to keep looking
+
+    def _find_one(self, shares, state):
+        # TODO could choose fastest
+        for s in shares:
+            if self._shares[s] == state:
+                return s
+        # can never get here, caller has assert in case of code bug
+
+    def _send_new_request(self):
+        for shnum,shares in sorted(self._shnums.iteritems()):
+            states = [self._shares[s] for s in shares]
+            if COMPLETE in states or PENDING in states:
+                # don't send redundant requests
+                continue
+            if AVAILABLE not in states:
+                # no candidates for this shnum, move on
+                continue
+            # here's a candidate. Send a request.
+            s = self._find_one(shares, AVAILABLE)
+            assert s
+            self._shares[s] = PENDING
+            self._share_observers[s] = o = s.get_block(self.segnum)
+            o.subscribe(self._block_request_activity, share=s, shnum=shnum)
+            # TODO: build up a list of candidates, then walk through the
+            # list, sending requests to the most desireable servers,
+            # re-checking our block-hunger each time. For non-initial segment
+            # fetches, this would let us stick with faster servers.
+            return True
+        # nothing was sent: don't call us again until you have more shares to
+        # work with, or one of the existing shares has been declared OVERDUE
+        return False
+
+    def _cancel_all_requests(self):
+        for o in self._share_observers.values():
+            o.cancel()
+        self._share_observers = {}
+
+    def _block_request_activity(self, share, shnum, state, block=None, f=None):
+        # called by Shares, in response to our s.send_request() calls.
+        if not self._running:
+            return
+        log.msg("SegmentFetcher(%s)._block_request_activity:"
+                " Share(sh%d-on-%s) -> %s" %
+                (self._node._si_prefix, shnum, share._peerid_s, state),
+                level=log.NOISY, umid="vilNWA")
+        # COMPLETE, CORRUPT, DEAD, BADSEGNUM are terminal.
+        if state in (COMPLETE, CORRUPT, DEAD, BADSEGNUM):
+            self._share_observers.pop(share, None)
+        if state is COMPLETE:
+            # 'block' is fully validated
+            self._shares[share] = COMPLETE
+            self._blocks[shnum] = block
+        elif state is OVERDUE:
+            self._shares[share] = OVERDUE
+            # OVERDUE is not terminal: it will eventually transition to
+            # COMPLETE, CORRUPT, or DEAD.
+        elif state is CORRUPT:
+            self._shares[share] = CORRUPT
+        elif state is DEAD:
+            del self._shares[share]
+            self._shnums[shnum].remove(share)
+            self._last_failure = f
+        elif state is BADSEGNUM:
+            self._shares[share] = BADSEGNUM # ???
+            self._bad_segnum = True
+        eventually(self.loop)
+
+
diff --git a/src/allmydata/immutable/downloader/finder.py b/src/allmydata/immutable/downloader/finder.py
new file mode 100644
index 0000000..7cefefa
--- /dev/null
+++ b/src/allmydata/immutable/downloader/finder.py
@@ -0,0 +1,185 @@
+
+import time
+now = time.time
+from foolscap.api import eventually
+from allmydata.util import base32, log, idlib
+
+from share import Share, CommonShare
+from util import incidentally
+
+class RequestToken:
+    def __init__(self, peerid):
+        self.peerid = peerid
+
+class ShareFinder:
+    def __init__(self, storage_broker, verifycap, node, download_status,
+                 logparent=None, max_outstanding_requests=10):
+        self.running = True # stopped by Share.stop, from Terminator
+        self.verifycap = verifycap
+        self._started = False
+        self._storage_broker = storage_broker
+        self.share_consumer = self.node = node
+        self.max_outstanding_requests = max_outstanding_requests
+
+        self._hungry = False
+
+        self._commonshares = {} # shnum to CommonShare instance
+        self.undelivered_shares = []
+        self.pending_requests = set()
+
+        self._storage_index = verifycap.storage_index
+        self._si_prefix = base32.b2a_l(self._storage_index[:8], 60)
+        self._node_logparent = logparent
+        self._download_status = download_status
+        self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
+                           si=self._si_prefix,
+                           level=log.NOISY, parent=logparent, umid="2xjj2A")
+
+    def start_finding_servers(self):
+        # don't get servers until somebody uses us: creating the
+        # ImmutableFileNode should not cause work to happen yet. Test case is
+        # test_dirnode, which creates us with storage_broker=None
+        if not self._started:
+            si = self.verifycap.storage_index
+            s = self._storage_broker.get_servers_for_index(si)
+            self._servers = iter(s)
+            self._started = True
+
+    def log(self, *args, **kwargs):
+        if "parent" not in kwargs:
+            kwargs["parent"] = self._lp
+        return log.msg(*args, **kwargs)
+
+    def stop(self):
+        self.running = False
+
+    # called by our parent CiphertextDownloader
+    def hungry(self):
+        self.log(format="ShareFinder[si=%(si)s] hungry",
+                 si=self._si_prefix, level=log.NOISY, umid="NywYaQ")
+        self.start_finding_servers()
+        self._hungry = True
+        eventually(self.loop)
+
+    # internal methods
+    def loop(self):
+        undelivered_s = ",".join(["sh%d@%s" %
+                                  (s._shnum, idlib.shortnodeid_b2a(s._peerid))
+                                  for s in self.undelivered_shares])
+        pending_s = ",".join([idlib.shortnodeid_b2a(rt.peerid)
+                              for rt in self.pending_requests]) # sort?
+        self.log(format="ShareFinder loop: running=%(running)s"
+                 " hungry=%(hungry)s, undelivered=%(undelivered)s,"
+                 " pending=%(pending)s",
+                 running=self.running, hungry=self._hungry,
+                 undelivered=undelivered_s, pending=pending_s,
+                 level=log.NOISY, umid="kRtS4Q")
+        if not self.running:
+            return
+        if not self._hungry:
+            return
+        if self.undelivered_shares:
+            sh = self.undelivered_shares.pop(0)
+            # they will call hungry() again if they want more
+            self._hungry = False
+            self.log(format="delivering Share(shnum=%(shnum)d, server=%(peerid)s)",
+                     shnum=sh._shnum, peerid=sh._peerid_s,
+                     level=log.NOISY, umid="2n1qQw")
+            eventually(self.share_consumer.got_shares, [sh])
+            return
+
+        if len(self.pending_requests) >= self.max_outstanding_requests:
+            # cannot send more requests, must wait for some to retire
+            return
+
+        server = None
+        try:
+            if self._servers:
+                server = self._servers.next()
+        except StopIteration:
+            self._servers = None
+
+        if server:
+            self.send_request(server)
+            # we loop again to get parallel queries. The check above will
+            # prevent us from looping forever.
+            eventually(self.loop)
+            return
+
+        if self.pending_requests:
+            # no server, but there are still requests in flight: maybe one of
+            # them will make progress
+            return
+
+        self.log(format="ShareFinder.loop: no_more_shares, ever",
+                 level=log.UNUSUAL, umid="XjQlzg")
+        # we've run out of servers (so we can't send any more requests), and
+        # we have nothing in flight. No further progress can be made. They
+        # are destined to remain hungry.
+        self.share_consumer.no_more_shares()
+
+    def send_request(self, server):
+        peerid, rref = server
+        req = RequestToken(peerid)
+        self.pending_requests.add(req)
+        lp = self.log(format="sending DYHB to [%(peerid)s]",
+                      peerid=idlib.shortnodeid_b2a(peerid),
+                      level=log.NOISY, umid="Io7pyg")
+        d_ev = self._download_status.add_dyhb_sent(peerid, now())
+        d = rref.callRemote("get_buckets", self._storage_index)
+        d.addBoth(incidentally, self.pending_requests.discard, req)
+        d.addCallbacks(self._got_response, self._got_error,
+                       callbackArgs=(rref.version, peerid, req, d_ev, lp),
+                       errbackArgs=(peerid, req, d_ev, lp))
+        d.addErrback(log.err, format="error in send_request",
+                     level=log.WEIRD, parent=lp, umid="rpdV0w")
+        d.addCallback(incidentally, eventually, self.loop)
+
+    def _got_response(self, buckets, server_version, peerid, req, d_ev, lp):
+        shnums = sorted([shnum for shnum in buckets])
+        d_ev.finished(shnums, now())
+        if buckets:
+            shnums_s = ",".join([str(shnum) for shnum in shnums])
+            self.log(format="got shnums [%(shnums)s] from [%(peerid)s]",
+                     shnums=shnums_s, peerid=idlib.shortnodeid_b2a(peerid),
+                     level=log.NOISY, parent=lp, umid="0fcEZw")
+        else:
+            self.log(format="no shares from [%(peerid)s]",
+                     peerid=idlib.shortnodeid_b2a(peerid),
+                     level=log.NOISY, parent=lp, umid="U7d4JA")
+        if self.node.num_segments is None:
+            best_numsegs = self.node.guessed_num_segments
+        else:
+            best_numsegs = self.node.num_segments
+        for shnum, bucket in buckets.iteritems():
+            if shnum in self._commonshares:
+                cs = self._commonshares[shnum]
+            else:
+                cs = CommonShare(best_numsegs, self._si_prefix, shnum,
+                                 self._node_logparent)
+                # Share._get_satisfaction is responsible for updating
+                # CommonShare.set_numsegs after we know the UEB. Alternatives:
+                #  1: d = self.node.get_num_segments()
+                #     d.addCallback(cs.got_numsegs)
+                #   the problem is that the OneShotObserverList I was using
+                #   inserts an eventual-send between _get_satisfaction's
+                #   _satisfy_UEB and _satisfy_block_hash_tree, and the
+                #   CommonShare didn't get the num_segs message before
+                #   being asked to set block hash values. To resolve this
+                #   would require an immediate ObserverList instead of
+                #   an eventual-send -based one
+                #  2: break _get_satisfaction into Deferred-attached pieces.
+                #     Yuck.
+                self._commonshares[shnum] = cs
+            s = Share(bucket, server_version, self.verifycap, cs, self.node,
+                      self._download_status, peerid, shnum,
+                      self._node_logparent)
+            self.undelivered_shares.append(s)
+
+    def _got_error(self, f, peerid, req, d_ev, lp):
+        d_ev.finished("error", now())
+        self.log(format="got error from [%(peerid)s]",
+                 peerid=idlib.shortnodeid_b2a(peerid), failure=f,
+                 level=log.UNUSUAL, parent=lp, umid="zUKdCw")
+
+
diff --git a/src/allmydata/immutable/downloader/node.py b/src/allmydata/immutable/downloader/node.py
new file mode 100644
index 0000000..b18c6bc
--- /dev/null
+++ b/src/allmydata/immutable/downloader/node.py
@@ -0,0 +1,528 @@
+
+import copy
+import time
+now = time.time
+from twisted.python.failure import Failure
+from twisted.internet import defer
+from foolscap.api import eventually
+from allmydata import uri
+from allmydata.codec import CRSDecoder
+from allmydata.check_results import CheckResults, CheckAndRepairResults
+from allmydata.util import base32, log, hashutil, mathutil
+from allmydata.util.dictutil import DictOfSets
+from allmydata.interfaces import IUploadResults, DEFAULT_MAX_SEGMENT_SIZE
+from allmydata.hashtree import IncompleteHashTree, BadHashError, \
+     NotEnoughHashesError
+from allmydata.immutable.checker import Checker
+from allmydata.immutable.repairer import Repairer
+
+# local imports
+from finder import ShareFinder
+from fetcher import SegmentFetcher
+from segmentation import Segmentation
+from common import BadCiphertextHashError
+
+class Cancel:
+    def __init__(self, f):
+        self._f = f
+        self.cancelled = False
+    def cancel(self):
+        if not self.cancelled:
+            self.cancelled = True
+            self._f(self)
+
+class DownloadNode:
+    """Internal class which manages downloads and holds state. External
+    callers use CiphertextFileNode instead."""
+
+    # Share._node points to me
+    def __init__(self, verifycap, storage_broker, secret_holder,
+                 terminator, history, download_status):
+        assert isinstance(verifycap, uri.CHKFileVerifierURI)
+        self._verifycap = verifycap
+        self._storage_broker = storage_broker
+        self._si_prefix = base32.b2a_l(verifycap.storage_index[:8], 60)
+        self.running = True
+        if terminator:
+            terminator.register(self) # calls self.stop() at stopService()
+        # the rules are:
+        # 1: Only send network requests if you're active (self.running is True)
+        # 2: Use TimerService, not reactor.callLater
+        # 3: You can do eventual-sends any time.
+        # These rules should mean that once
+        # stopService()+flushEventualQueue() fires, everything will be done.
+        self._secret_holder = secret_holder
+        self._history = history
+        self._download_status = download_status
+
+        k, N = self._verifycap.needed_shares, self._verifycap.total_shares
+        self.share_hash_tree = IncompleteHashTree(N)
+
+        # we guess the segment size, so Segmentation can pull non-initial
+        # segments in a single roundtrip. This populates
+        # .guessed_segment_size, .guessed_num_segments, and
+        # .ciphertext_hash_tree (with a dummy, to let us guess which hashes
+        # we'll need)
+        self._build_guessed_tables(DEFAULT_MAX_SEGMENT_SIZE)
+
+        # filled in when we parse a valid UEB
+        self.have_UEB = False
+        self.segment_size = None
+        self.tail_segment_size = None
+        self.tail_segment_padded = None
+        self.num_segments = None
+        self.block_size = None
+        self.tail_block_size = None
+
+        # things to track callers that want data
+
+        # _segment_requests can have duplicates
+        self._segment_requests = [] # (segnum, d, cancel_handle)
+        self._active_segment = None # a SegmentFetcher, with .segnum
+
+        # we create one top-level logparent for this _Node, and another one
+        # for each read() call. Segmentation and get_segment() messages are
+        # associated with the read() call, everything else is tied to the
+        # _Node's log entry.
+        lp = log.msg(format="Immutable _Node(%(si)s) created: size=%(size)d,"
+                     " guessed_segsize=%(guessed_segsize)d,"
+                     " guessed_numsegs=%(guessed_numsegs)d",
+                     si=self._si_prefix, size=verifycap.size,
+                     guessed_segsize=self.guessed_segment_size,
+                     guessed_numsegs=self.guessed_num_segments,
+                     level=log.OPERATIONAL, umid="uJ0zAQ")
+        self._lp = lp
+
+        self._sharefinder = ShareFinder(storage_broker, verifycap, self,
+                                        self._download_status, lp)
+        self._shares = set()
+
+    def _build_guessed_tables(self, max_segment_size):
+        size = min(self._verifycap.size, max_segment_size)
+        s = mathutil.next_multiple(size, self._verifycap.needed_shares)
+        self.guessed_segment_size = s
+        r = self._calculate_sizes(self.guessed_segment_size)
+        self.guessed_num_segments = r["num_segments"]
+        # as with CommonShare, our ciphertext_hash_tree is a stub until we
+        # get the real num_segments
+        self.ciphertext_hash_tree = IncompleteHashTree(self.guessed_num_segments)
+
+    def __repr__(self):
+        return "Imm_Node(%s)" % (self._si_prefix,)
+
+    def stop(self):
+        # called by the Terminator at shutdown, mostly for tests
+        if self._active_segment:
+            self._active_segment.stop()
+            self._active_segment = None
+        self._sharefinder.stop()
+
+    # things called by outside callers, via CiphertextFileNode. get_segment()
+    # may also be called by Segmentation.
+
+    def read(self, consumer, offset=0, size=None, read_ev=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data. I feed the consumer with the desired range of ciphertext. I
+        return a Deferred that fires (with the consumer) when the read is
+        finished.
+
+        Note that there is no notion of a 'file pointer': each call to read()
+        uses an independent offset= value."""
+        # for concurrent operations: each gets its own Segmentation manager
+        if size is None:
+            size = self._verifycap.size
+        # clip size so offset+size does not go past EOF
+        size = min(size, self._verifycap.size-offset)
+        if read_ev is None:
+            read_ev = self._download_status.add_read_event(offset, size, now())
+
+        lp = log.msg(format="imm Node(%(si)s).read(%(offset)d, %(size)d)",
+                     si=base32.b2a(self._verifycap.storage_index)[:8],
+                     offset=offset, size=size,
+                     level=log.OPERATIONAL, parent=self._lp, umid="l3j3Ww")
+        if self._history:
+            sp = self._history.stats_provider
+            sp.count("downloader.files_downloaded", 1) # really read() calls
+            sp.count("downloader.bytes_downloaded", size)
+        s = Segmentation(self, offset, size, consumer, read_ev, lp)
+        # this raises an interesting question: what segments to fetch? if
+        # offset=0, always fetch the first segment, and then allow
+        # Segmentation to be responsible for pulling the subsequent ones if
+        # the first wasn't large enough. If offset>0, we're going to need an
+        # extra roundtrip to get the UEB (and therefore the segment size)
+        # before we can figure out which segment to get. TODO: allow the
+        # offset-table-guessing code (which starts by guessing the segsize)
+        # to assist the offset>0 process.
+        d = s.start()
+        def _done(res):
+            read_ev.finished(now())
+            return res
+        d.addBoth(_done)
+        return d
+
+    def get_segment(self, segnum, logparent=None):
+        """Begin downloading a segment. I return a tuple (d, c): 'd' is a
+        Deferred that fires with (offset,data) when the desired segment is
+        available, and c is an object on which c.cancel() can be called to
+        disavow interest in the segment (after which 'd' will never fire).
+
+        You probably need to know the segment size before calling this,
+        unless you want the first few bytes of the file. If you ask for a
+        segment number which turns out to be too large, the Deferred will
+        errback with BadSegmentNumberError.
+
+        The Deferred fires with the offset of the first byte of the data
+        segment, so that you can call get_segment() before knowing the
+        segment size, and still know which data you received.
+
+        The Deferred can also errback with other fatal problems, such as
+        NotEnoughSharesError, NoSharesError, or BadCiphertextHashError.
+        """
+        log.msg(format="imm Node(%(si)s).get_segment(%(segnum)d)",
+                si=base32.b2a(self._verifycap.storage_index)[:8],
+                segnum=segnum,
+                level=log.OPERATIONAL, parent=logparent, umid="UKFjDQ")
+        self._download_status.add_segment_request(segnum, now())
+        d = defer.Deferred()
+        c = Cancel(self._cancel_request)
+        self._segment_requests.append( (segnum, d, c) )
+        self._start_new_segment()
+        return (d, c)
+
+    # things called by the Segmentation object used to transform
+    # arbitrary-sized read() calls into quantized segment fetches
+
+    def _start_new_segment(self):
+        if self._active_segment is None and self._segment_requests:
+            segnum = self._segment_requests[0][0]
+            k = self._verifycap.needed_shares
+            log.msg(format="%(node)s._start_new_segment: segnum=%(segnum)d",
+                    node=repr(self), segnum=segnum,
+                    level=log.NOISY, umid="wAlnHQ")
+            self._active_segment = fetcher = SegmentFetcher(self, segnum, k)
+            active_shares = [s for s in self._shares if s.is_alive()]
+            fetcher.add_shares(active_shares) # this triggers the loop
+
+
+    # called by our child ShareFinder
+    def got_shares(self, shares):
+        self._shares.update(shares)
+        if self._active_segment:
+            self._active_segment.add_shares(shares)
+    def no_more_shares(self):
+        self._no_more_shares = True
+        if self._active_segment:
+            self._active_segment.no_more_shares()
+
+    # things called by our Share instances
+
+    def validate_and_store_UEB(self, UEB_s):
+        log.msg("validate_and_store_UEB",
+                level=log.OPERATIONAL, parent=self._lp, umid="7sTrPw")
+        h = hashutil.uri_extension_hash(UEB_s)
+        if h != self._verifycap.uri_extension_hash:
+            raise BadHashError
+        UEB_dict = uri.unpack_extension(UEB_s)
+        self._parse_and_store_UEB(UEB_dict) # sets self._stuff
+        # TODO: a malformed (but authentic) UEB could throw an assertion in
+        # _parse_and_store_UEB, and we should abandon the download.
+        self.have_UEB = True
+
+    def _parse_and_store_UEB(self, d):
+        # Note: the UEB contains needed_shares and total_shares. These are
+        # redundant and inferior (the filecap contains the authoritative
+        # values). However, because it is possible to encode the same file in
+        # multiple ways, and the encoders might choose (poorly) to use the
+        # same key for both (therefore getting the same SI), we might
+        # encounter shares for both types. The UEB hashes will be different,
+        # however, and we'll disregard the "other" encoding's shares as
+        # corrupted.
+
+        # therefore, we ignore d['total_shares'] and d['needed_shares'].
+
+        log.msg(format="UEB=%(ueb)s, vcap=%(vcap)s",
+                ueb=repr(d), vcap=self._verifycap.to_string(),
+                level=log.NOISY, parent=self._lp, umid="cVqZnA")
+
+        k, N = self._verifycap.needed_shares, self._verifycap.total_shares
+
+        self.segment_size = d['segment_size']
+
+        r = self._calculate_sizes(self.segment_size)
+        self.tail_segment_size = r["tail_segment_size"]
+        self.tail_segment_padded = r["tail_segment_padded"]
+        self.num_segments = r["num_segments"]
+        self.block_size = r["block_size"]
+        self.tail_block_size = r["tail_block_size"]
+        log.msg("actual sizes: %s" % (r,),
+                level=log.NOISY, parent=self._lp, umid="PY6P5Q")
+        if (self.segment_size == self.guessed_segment_size
+            and self.num_segments == self.guessed_num_segments):
+            log.msg("my guess was right!",
+                    level=log.NOISY, parent=self._lp, umid="x340Ow")
+        else:
+            log.msg("my guess was wrong! Extra round trips for me.",
+                    level=log.NOISY, parent=self._lp, umid="tb7RJw")
+
+        # zfec.Decode() instantiation is fast, but still, let's use the same
+        # codec instance for all but the last segment. 3-of-10 takes 15us on
+        # my laptop, 25-of-100 is 900us, 3-of-255 is 97us, 25-of-255 is
+        # 2.5ms, worst-case 254-of-255 is 9.3ms
+        self._codec = CRSDecoder()
+        self._codec.set_params(self.segment_size, k, N)
+
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B. self.ciphertext_hash_tree was a guess before:
+        # this is where we create it for real.
+        self.ciphertext_hash_tree = IncompleteHashTree(self.num_segments)
+        self.ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
+
+        self.share_hash_tree.set_hashes({0: d['share_root_hash']})
+
+        # Our job is a fast download, not verification, so we ignore any
+        # redundant fields. The Verifier uses a different code path which
+        # does not ignore them.
+
+    def _calculate_sizes(self, segment_size):
+        # segments of ciphertext
+        size = self._verifycap.size
+        k = self._verifycap.needed_shares
+
+        # this assert matches the one in encode.py:127 inside
+        # Encoded._got_all_encoding_parameters, where the UEB is constructed
+        assert segment_size % k == 0
+
+        # the last segment is usually short. We don't store a whole segsize,
+        # but we do pad the segment up to a multiple of k, because the
+        # encoder requires that.
+        tail_segment_size = size % segment_size
+        if tail_segment_size == 0:
+            tail_segment_size = segment_size
+        padded = mathutil.next_multiple(tail_segment_size, k)
+        tail_segment_padded = padded
+
+        num_segments = mathutil.div_ceil(size, segment_size)
+
+        # each segment is turned into N blocks. All but the last are of size
+        # block_size, and the last is of size tail_block_size
+        block_size = segment_size / k
+        tail_block_size = tail_segment_padded / k
+
+        return { "tail_segment_size": tail_segment_size,
+                 "tail_segment_padded": tail_segment_padded,
+                 "num_segments": num_segments,
+                 "block_size": block_size,
+                 "tail_block_size": tail_block_size,
+                 }
+
+
+    def process_share_hashes(self, share_hashes):
+        for hashnum in share_hashes:
+            if hashnum >= len(self.share_hash_tree):
+                # "BadHashError" is normally for e.g. a corrupt block. We
+                # sort of abuse it here to mean a badly numbered hash (which
+                # indicates corruption in the number bytes, rather than in
+                # the data bytes).
+                raise BadHashError("hashnum %d doesn't fit in hashtree(%d)"
+                                   % (hashnum, len(self.share_hash_tree)))
+        self.share_hash_tree.set_hashes(share_hashes)
+
+    def get_needed_ciphertext_hashes(self, segnum):
+        cht = self.ciphertext_hash_tree
+        return cht.needed_hashes(segnum, include_leaf=True)
+    def process_ciphertext_hashes(self, hashes):
+        assert self.num_segments is not None
+        # this may raise BadHashError or NotEnoughHashesError
+        self.ciphertext_hash_tree.set_hashes(hashes)
+
+
+    # called by our child SegmentFetcher
+
+    def want_more_shares(self):
+        self._sharefinder.hungry()
+
+    def fetch_failed(self, sf, f):
+        assert sf is self._active_segment
+        self._active_segment = None
+        # deliver error upwards
+        for (d,c) in self._extract_requests(sf.segnum):
+            eventually(self._deliver, d, c, f)
+
+    def process_blocks(self, segnum, blocks):
+        d = defer.maybeDeferred(self._decode_blocks, segnum, blocks)
+        d.addCallback(self._check_ciphertext_hash, segnum)
+        def _deliver(result):
+            ds = self._download_status
+            if isinstance(result, Failure):
+                ds.add_segment_error(segnum, now())
+            else:
+                (offset, segment, decodetime) = result
+                ds.add_segment_delivery(segnum, now(),
+                                        offset, len(segment), decodetime)
+            log.msg(format="delivering segment(%(segnum)d)",
+                    segnum=segnum,
+                    level=log.OPERATIONAL, parent=self._lp,
+                    umid="j60Ojg")
+            for (d,c) in self._extract_requests(segnum):
+                eventually(self._deliver, d, c, result)
+            self._active_segment = None
+            self._start_new_segment()
+        d.addBoth(_deliver)
+        d.addErrback(lambda f:
+                     log.err("unhandled error during process_blocks",
+                             failure=f, level=log.WEIRD,
+                             parent=self._lp, umid="MkEsCg"))
+
+    def _decode_blocks(self, segnum, blocks):
+        tail = (segnum == self.num_segments-1)
+        codec = self._codec
+        block_size = self.block_size
+        decoded_size = self.segment_size
+        if tail:
+            # account for the padding in the last segment
+            codec = CRSDecoder()
+            k, N = self._verifycap.needed_shares, self._verifycap.total_shares
+            codec.set_params(self.tail_segment_padded, k, N)
+            block_size = self.tail_block_size
+            decoded_size = self.tail_segment_padded
+
+        shares = []
+        shareids = []
+        for (shareid, share) in blocks.iteritems():
+            assert len(share) == block_size
+            shareids.append(shareid)
+            shares.append(share)
+        del blocks
+
+        start = now()
+        d = codec.decode(shares, shareids)   # segment
+        del shares
+        def _process(buffers):
+            decodetime = now() - start
+            segment = "".join(buffers)
+            assert len(segment) == decoded_size
+            del buffers
+            if tail:
+                segment = segment[:self.tail_segment_size]
+            return (segment, decodetime)
+        d.addCallback(_process)
+        return d
+
+    def _check_ciphertext_hash(self, (segment, decodetime), segnum):
+        assert self._active_segment.segnum == segnum
+        assert self.segment_size is not None
+        offset = segnum * self.segment_size
+
+        h = hashutil.crypttext_segment_hash(segment)
+        try:
+            self.ciphertext_hash_tree.set_hashes(leaves={segnum: h})
+            return (offset, segment, decodetime)
+        except (BadHashError, NotEnoughHashesError):
+            format = ("hash failure in ciphertext_hash_tree:"
+                      " segnum=%(segnum)d, SI=%(si)s")
+            log.msg(format=format, segnum=segnum, si=self._si_prefix,
+                    failure=Failure(),
+                    level=log.WEIRD, parent=self._lp, umid="MTwNnw")
+            # this is especially weird, because we made it past the share
+            # hash tree. It implies that we're using the wrong encoding, or
+            # that the uploader deliberately constructed a bad UEB.
+            msg = format % {"segnum": segnum, "si": self._si_prefix}
+            raise BadCiphertextHashError(msg)
+
+    def _deliver(self, d, c, result):
+        # this method exists to handle cancel() that occurs between
+        # _got_segment and _deliver
+        if not c.cancelled:
+            d.callback(result) # might actually be an errback
+
+    def _extract_requests(self, segnum):
+        """Remove matching requests and return their (d,c) tuples so that the
+        caller can retire them."""
+        retire = [(d,c) for (segnum0, d, c) in self._segment_requests
+                  if segnum0 == segnum]
+        self._segment_requests = [t for t in self._segment_requests
+                                  if t[0] != segnum]
+        return retire
+
+    def _cancel_request(self, c):
+        self._segment_requests = [t for t in self._segment_requests
+                                  if t[2] != c]
+        segnums = [segnum for (segnum,d,c) in self._segment_requests]
+        if self._active_segment.segnum not in segnums:
+            self._active_segment.stop()
+            self._active_segment = None
+            self._start_new_segment()
+
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        verifycap = self._verifycap
+        storage_index = verifycap.storage_index
+        sb = self._storage_broker
+        servers = sb.get_all_servers()
+        sh = self._secret_holder
+
+        c = Checker(verifycap=verifycap, servers=servers,
+                    verify=verify, add_lease=add_lease, secret_holder=sh,
+                    monitor=monitor)
+        d = c.start()
+        def _maybe_repair(cr):
+            crr = CheckAndRepairResults(storage_index)
+            crr.pre_repair_results = cr
+            if cr.is_healthy():
+                crr.post_repair_results = cr
+                return defer.succeed(crr)
+            else:
+                crr.repair_attempted = True
+                crr.repair_successful = False # until proven successful
+                def _gather_repair_results(ur):
+                    assert IUploadResults.providedBy(ur), ur
+                    # clone the cr (check results) to form the basis of the
+                    # prr (post-repair results)
+                    prr = CheckResults(cr.uri, cr.storage_index)
+                    prr.data = copy.deepcopy(cr.data)
+
+                    sm = prr.data['sharemap']
+                    assert isinstance(sm, DictOfSets), sm
+                    sm.update(ur.sharemap)
+                    servers_responding = set(prr.data['servers-responding'])
+                    servers_responding.union(ur.sharemap.iterkeys())
+                    prr.data['servers-responding'] = list(servers_responding)
+                    prr.data['count-shares-good'] = len(sm)
+                    prr.data['count-good-share-hosts'] = len(sm)
+                    is_healthy = bool(len(sm) >= verifycap.total_shares)
+                    is_recoverable = bool(len(sm) >= verifycap.needed_shares)
+                    prr.set_healthy(is_healthy)
+                    prr.set_recoverable(is_recoverable)
+                    crr.repair_successful = is_healthy
+                    prr.set_needs_rebalancing(len(sm) >= verifycap.total_shares)
+
+                    crr.post_repair_results = prr
+                    return crr
+                def _repair_error(f):
+                    # as with mutable repair, I'm not sure if I want to pass
+                    # through a failure or not. TODO
+                    crr.repair_successful = False
+                    crr.repair_failure = f
+                    return f
+                r = Repairer(storage_broker=sb, secret_holder=sh,
+                             verifycap=verifycap, monitor=monitor)
+                d = r.start()
+                d.addCallbacks(_gather_repair_results, _repair_error)
+                return d
+
+        d.addCallback(_maybe_repair)
+        return d
+
+    def check(self, monitor, verify=False, add_lease=False):
+        verifycap = self._verifycap
+        sb = self._storage_broker
+        servers = sb.get_all_servers()
+        sh = self._secret_holder
+
+        v = Checker(verifycap=verifycap, servers=servers,
+                    verify=verify, add_lease=add_lease, secret_holder=sh,
+                    monitor=monitor)
+        return v.start()
diff --git a/src/allmydata/immutable/downloader/segmentation.py b/src/allmydata/immutable/downloader/segmentation.py
new file mode 100644
index 0000000..adc138e
--- /dev/null
+++ b/src/allmydata/immutable/downloader/segmentation.py
@@ -0,0 +1,157 @@
+
+import time
+now = time.time
+from zope.interface import implements
+from twisted.internet import defer
+from twisted.internet.interfaces import IPushProducer
+from foolscap.api import eventually
+from allmydata.util import log
+from allmydata.util.spans import overlap
+
+from common import BadSegmentNumberError, WrongSegmentError
+
+class Segmentation:
+    """I am responsible for a single offset+size read of the file. I handle
+    segmentation: I figure out which segments are necessary, request them
+    (from my CiphertextDownloader) in order, and trim the segments down to
+    match the offset+size span. I use the Producer/Consumer interface to only
+    request one segment at a time.
+    """
+    implements(IPushProducer)
+    def __init__(self, node, offset, size, consumer, read_ev, logparent=None):
+        self._node = node
+        self._hungry = True
+        self._active_segnum = None
+        self._cancel_segment_request = None
+        # these are updated as we deliver data. At any given time, we still
+        # want to download file[offset:offset+size]
+        self._offset = offset
+        self._size = size
+        assert offset+size <= node._verifycap.size
+        self._consumer = consumer
+        self._read_ev = read_ev
+        self._start_pause = None
+        self._lp = logparent
+
+    def start(self):
+        self._alive = True
+        self._deferred = defer.Deferred()
+        self._consumer.registerProducer(self, True)
+        self._maybe_fetch_next()
+        return self._deferred
+
+    def _maybe_fetch_next(self):
+        if not self._alive or not self._hungry:
+            return
+        if self._active_segnum is not None:
+            return
+        self._fetch_next()
+
+    def _fetch_next(self):
+        if self._size == 0:
+            # done!
+            self._alive = False
+            self._hungry = False
+            self._consumer.unregisterProducer()
+            self._deferred.callback(self._consumer)
+            return
+        n = self._node
+        have_actual_segment_size = n.segment_size is not None
+        guess_s = ""
+        if not have_actual_segment_size:
+            guess_s = "probably "
+        segment_size = n.segment_size or n.guessed_segment_size
+        if self._offset == 0:
+            # great! we want segment0 for sure
+            wanted_segnum = 0
+        else:
+            # this might be a guess
+            wanted_segnum = self._offset // segment_size
+        log.msg(format="_fetch_next(offset=%(offset)d) %(guess)swants segnum=%(segnum)d",
+                offset=self._offset, guess=guess_s, segnum=wanted_segnum,
+                level=log.NOISY, parent=self._lp, umid="5WfN0w")
+        self._active_segnum = wanted_segnum
+        d,c = n.get_segment(wanted_segnum, self._lp)
+        self._cancel_segment_request = c
+        d.addBoth(self._request_retired)
+        d.addCallback(self._got_segment, wanted_segnum)
+        if not have_actual_segment_size:
+            # we can retry once
+            d.addErrback(self._retry_bad_segment)
+        d.addErrback(self._error)
+
+    def _request_retired(self, res):
+        self._active_segnum = None
+        self._cancel_segment_request = None
+        return res
+
+    def _got_segment(self, (segment_start,segment,decodetime), wanted_segnum):
+        self._cancel_segment_request = None
+        # we got file[segment_start:segment_start+len(segment)]
+        # we want file[self._offset:self._offset+self._size]
+        log.msg(format="Segmentation got data:"
+                " want [%(wantstart)d-%(wantend)d),"
+                " given [%(segstart)d-%(segend)d), for segnum=%(segnum)d",
+                wantstart=self._offset, wantend=self._offset+self._size,
+                segstart=segment_start, segend=segment_start+len(segment),
+                segnum=wanted_segnum,
+                level=log.OPERATIONAL, parent=self._lp, umid="32dHcg")
+
+        o = overlap(segment_start, len(segment),  self._offset, self._size)
+        # the overlap is file[o[0]:o[0]+o[1]]
+        if not o or o[0] != self._offset:
+            # we didn't get the first byte, so we can't use this segment
+            log.msg("Segmentation handed wrong data:"
+                    " want [%d-%d), given [%d-%d), for segnum=%d,"
+                    " for si=%s"
+                    % (self._offset, self._offset+self._size,
+                       segment_start, segment_start+len(segment),
+                       wanted_segnum, self._node._si_prefix),
+                    level=log.UNUSUAL, parent=self._lp, umid="STlIiA")
+            # we may retry if the segnum we asked was based on a guess
+            raise WrongSegmentError("I was given the wrong data.")
+        offset_in_segment = self._offset - segment_start
+        desired_data = segment[offset_in_segment:offset_in_segment+o[1]]
+
+        self._offset += len(desired_data)
+        self._size -= len(desired_data)
+        self._consumer.write(desired_data)
+        # the consumer might call our .pauseProducing() inside that write()
+        # call, setting self._hungry=False
+        self._read_ev.update(len(desired_data), 0, 0)
+        self._maybe_fetch_next()
+
+    def _retry_bad_segment(self, f):
+        f.trap(WrongSegmentError, BadSegmentNumberError)
+        # we guessed the segnum wrong: either one that doesn't overlap with
+        # the start of our desired region, or one that's beyond the end of
+        # the world. Now that we have the right information, we're allowed to
+        # retry once.
+        assert self._node.segment_size is not None
+        return self._maybe_fetch_next()
+
+    def _error(self, f):
+        log.msg("Error in Segmentation", failure=f,
+                level=log.WEIRD, parent=self._lp, umid="EYlXBg")
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
+
+    def stopProducing(self):
+        self._hungry = False
+        self._alive = False
+        # cancel any outstanding segment request
+        if self._cancel_segment_request:
+            self._cancel_segment_request.cancel()
+            self._cancel_segment_request = None
+    def pauseProducing(self):
+        self._hungry = False
+        self._start_pause = now()
+    def resumeProducing(self):
+        self._hungry = True
+        eventually(self._maybe_fetch_next)
+        if self._start_pause is not None:
+            paused = now() - self._start_pause
+            self._read_ev.update(0, 0, paused)
+            self._start_pause = None
diff --git a/src/allmydata/immutable/downloader/share.py b/src/allmydata/immutable/downloader/share.py
new file mode 100644
index 0000000..c4dbd73
--- /dev/null
+++ b/src/allmydata/immutable/downloader/share.py
@@ -0,0 +1,840 @@
+
+import struct
+import time
+now = time.time
+
+from twisted.python.failure import Failure
+from foolscap.api import eventually
+from allmydata.util import base32, log, hashutil, mathutil
+from allmydata.util.spans import Spans, DataSpans
+from allmydata.interfaces import HASH_SIZE
+from allmydata.hashtree import IncompleteHashTree, BadHashError, \
+     NotEnoughHashesError
+
+from allmydata.immutable.layout import make_write_bucket_proxy
+from util import Observer2
+from common import COMPLETE, CORRUPT, DEAD, BADSEGNUM
+
+
+class LayoutInvalid(Exception):
+    pass
+class DataUnavailable(Exception):
+    pass
+
+class Share:
+    """I represent a single instance of a single share (e.g. I reference the
+    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
+    I am associated with a CommonShare that remembers data that is held in
+    common among e.g. SI=abcde/shnum2 across all servers. I am also
+    associated with a CiphertextFileNode for e.g. SI=abcde (all shares, all
+    servers).
+    """
+    # this is a specific implementation of IShare for tahoe's native storage
+    # servers. A different backend would use a different class.
+
+    def __init__(self, rref, server_version, verifycap, commonshare, node,
+                 download_status, peerid, shnum, logparent):
+        self._rref = rref
+        self._server_version = server_version
+        self._node = node # holds share_hash_tree and UEB
+        self.actual_segment_size = node.segment_size # might still be None
+        # XXX change node.guessed_segment_size to
+        # node.best_guess_segment_size(), which should give us the real ones
+        # if known, else its guess.
+        self._guess_offsets(verifycap, node.guessed_segment_size)
+        self.actual_offsets = None
+        self._UEB_length = None
+        self._commonshare = commonshare # holds block_hash_tree
+        self._download_status = download_status
+        self._peerid = peerid
+        self._peerid_s = base32.b2a(peerid)[:5]
+        self._storage_index = verifycap.storage_index
+        self._si_prefix = base32.b2a(verifycap.storage_index)[:8]
+        self._shnum = shnum
+        # self._alive becomes False upon fatal corruption or server error
+        self._alive = True
+        self._lp = log.msg(format="%(share)s created", share=repr(self),
+                           level=log.NOISY, parent=logparent, umid="P7hv2w")
+
+        self._pending = Spans() # request sent but no response received yet
+        self._received = DataSpans() # ACK response received, with data
+        self._unavailable = Spans() # NAK response received, no data
+
+        # any given byte of the share can be in one of four states:
+        #  in: _wanted, _requested, _received
+        #      FALSE    FALSE       FALSE : don't care about it at all
+        #      TRUE     FALSE       FALSE : want it, haven't yet asked for it
+        #      TRUE     TRUE        FALSE : request is in-flight
+        #                                   or didn't get it
+        #      FALSE    TRUE        TRUE  : got it, haven't used it yet
+        #      FALSE    TRUE        FALSE : got it and used it
+        #      FALSE    FALSE       FALSE : block consumed, ready to ask again
+        #
+        # when we request data and get a NAK, we leave it in _requested
+        # to remind ourself to not ask for it again. We don't explicitly
+        # remove it from anything (maybe this should change).
+        #
+        # We retain the hashtrees in the Node, so we leave those spans in
+        # _requested (and never ask for them again, as long as the Node is
+        # alive). But we don't retain data blocks (too big), so when we
+        # consume a data block, we remove it from _requested, so a later
+        # download can re-fetch it.
+
+        self._requested_blocks = [] # (segnum, set(observer2..))
+        ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
+        self._overrun_ok = ver["tolerates-immutable-read-overrun"]
+        # If _overrun_ok and we guess the offsets correctly, we can get
+        # everything in one RTT. If _overrun_ok and we guess wrong, we might
+        # need two RTT (but we could get lucky and do it in one). If overrun
+        # is *not* ok (tahoe-1.3.0 or earlier), we need four RTT: 1=version,
+        # 2=offset table, 3=UEB_length and everything else (hashes, block),
+        # 4=UEB.
+
+        self.had_corruption = False # for unit tests
+
+    def __repr__(self):
+        return "Share(sh%d-on-%s)" % (self._shnum, self._peerid_s)
+
+    def is_alive(self):
+        # XXX: reconsider. If the share sees a single error, should it remain
+        # dead for all time? Or should the next segment try again? This DEAD
+        # state is stored elsewhere too (SegmentFetcher per-share states?)
+        # and needs to be consistent. We clear _alive in self._fail(), which
+        # is called upon a network error, or layout failure, or hash failure
+        # in the UEB or a hash tree. We do not _fail() for a hash failure in
+        # a block, but of course we still tell our callers about
+        # state=CORRUPT so they'll find a different share.
+        return self._alive
+
+    def _guess_offsets(self, verifycap, guessed_segment_size):
+        self.guessed_segment_size = guessed_segment_size
+        size = verifycap.size
+        k = verifycap.needed_shares
+        N = verifycap.total_shares
+        r = self._node._calculate_sizes(guessed_segment_size)
+        # num_segments, block_size/tail_block_size
+        # guessed_segment_size/tail_segment_size/tail_segment_padded
+        share_size = mathutil.div_ceil(size, k)
+        # share_size is the amount of block data that will be put into each
+        # share, summed over all segments. It does not include hashes, the
+        # UEB, or other overhead.
+
+        # use the upload-side code to get this as accurate as possible
+        ht = IncompleteHashTree(N)
+        num_share_hashes = len(ht.needed_hashes(0, include_leaf=True))
+        wbp = make_write_bucket_proxy(None, share_size, r["block_size"],
+                                      r["num_segments"], num_share_hashes, 0,
+                                      None)
+        self._fieldsize = wbp.fieldsize
+        self._fieldstruct = wbp.fieldstruct
+        self.guessed_offsets = wbp._offsets
+
+    # called by our client, the SegmentFetcher
+    def get_block(self, segnum):
+        """Add a block number to the list of requests. This will eventually
+        result in a fetch of the data necessary to validate the block, then
+        the block itself. The fetch order is generally
+        first-come-first-served, but requests may be answered out-of-order if
+        data becomes available sooner.
+
+        I return an Observer2, which has two uses. The first is to call
+        o.subscribe(), which gives me a place to send state changes and
+        eventually the data block. The second is o.cancel(), which removes
+        the request (if it is still active).
+
+        I will distribute the following events through my Observer2:
+         - state=OVERDUE: ?? I believe I should have had an answer by now.
+                          You may want to ask another share instead.
+         - state=BADSEGNUM: the segnum you asked for is too large. I must
+                            fetch a valid UEB before I can determine this,
+                            so the notification is asynchronous
+         - state=COMPLETE, block=data: here is a valid block
+         - state=CORRUPT: this share contains corrupted data
+         - state=DEAD, f=Failure: the server reported an error, this share
+                                  is unusable
+        """
+        log.msg("%s.get_block(%d)" % (repr(self), segnum),
+                level=log.NOISY, parent=self._lp, umid="RTo9MQ")
+        assert segnum >= 0
+        o = Observer2()
+        o.set_canceler(self, "_cancel_block_request")
+        for i,(segnum0,observers) in enumerate(self._requested_blocks):
+            if segnum0 == segnum:
+                observers.add(o)
+                break
+        else:
+            self._requested_blocks.append( (segnum, set([o])) )
+        eventually(self.loop)
+        return o
+
+    def _cancel_block_request(self, o):
+        new_requests = []
+        for e in self._requested_blocks:
+            (segnum0, observers) = e
+            observers.discard(o)
+            if observers:
+                new_requests.append(e)
+        self._requested_blocks = new_requests
+
+    # internal methods
+    def _active_segnum_and_observers(self):
+        if self._requested_blocks:
+            # we only retrieve information for one segment at a time, to
+            # minimize alacrity (first come, first served)
+            return self._requested_blocks[0]
+        return None, []
+
+    def loop(self):
+        try:
+            # if any exceptions occur here, kill the download
+            log.msg("%s.loop, reqs=[%s], pending=%s, received=%s,"
+                    " unavailable=%s" %
+                    (repr(self),
+                     ",".join([str(req[0]) for req in self._requested_blocks]),
+                     self._pending.dump(), self._received.dump(),
+                     self._unavailable.dump() ),
+                    level=log.NOISY, parent=self._lp, umid="BaL1zw")
+            self._do_loop()
+            # all exception cases call self._fail(), which clears self._alive
+        except (BadHashError, NotEnoughHashesError, LayoutInvalid), e:
+            # Abandon this share. We do this if we see corruption in the
+            # offset table, the UEB, or a hash tree. We don't abandon the
+            # whole share if we see corruption in a data block (we abandon
+            # just the one block, and still try to get data from other blocks
+            # on the same server). In theory, we could get good data from a
+            # share with a corrupt UEB (by first getting the UEB from some
+            # other share), or corrupt hash trees, but the logic to decide
+            # when this is safe is non-trivial. So for now, give up at the
+            # first sign of corruption.
+            #
+            # _satisfy_*() code which detects corruption should first call
+            # self._signal_corruption(), and then raise the exception.
+            log.msg(format="corruption detected in %(share)s",
+                    share=repr(self),
+                    level=log.UNUSUAL, parent=self._lp, umid="gWspVw")
+            self._fail(Failure(e), log.UNUSUAL)
+        except DataUnavailable, e:
+            # Abandon this share.
+            log.msg(format="need data that will never be available"
+                    " from %s: pending=%s, received=%s, unavailable=%s" %
+                    (repr(self),
+                     self._pending.dump(), self._received.dump(),
+                     self._unavailable.dump() ),
+                    level=log.UNUSUAL, parent=self._lp, umid="F7yJnQ")
+            self._fail(Failure(e), log.UNUSUAL)
+        except BaseException:
+            self._fail(Failure())
+            raise
+        log.msg("%s.loop done, reqs=[%s], pending=%s, received=%s,"
+                " unavailable=%s" %
+                (repr(self),
+                 ",".join([str(req[0]) for req in self._requested_blocks]),
+                 self._pending.dump(), self._received.dump(),
+                 self._unavailable.dump() ),
+                level=log.NOISY, parent=self._lp, umid="9lRaRA")
+
+    def _do_loop(self):
+        # we are (eventually) called after all state transitions:
+        #  new segments added to self._requested_blocks
+        #  new data received from servers (responses to our read() calls)
+        #  impatience timer fires (server appears slow)
+        if not self._alive:
+            return
+
+        # First, consume all of the information that we currently have, for
+        # all the segments people currently want.
+        while self._get_satisfaction():
+            pass
+
+        # When we get no satisfaction (from the data we've received so far),
+        # we determine what data we desire (to satisfy more requests). The
+        # number of segments is finite, so I can't get no satisfaction
+        # forever.
+        wanted, needed = self._desire()
+
+        # Finally, send out requests for whatever we need (desire minus
+        # have). You can't always get what you want, but if you try
+        # sometimes, you just might find, you get what you need.
+        self._send_requests(wanted + needed)
+
+        # and sometimes you can't even get what you need
+        disappointment = needed & self._unavailable
+        if len(disappointment):
+            self.had_corruption = True
+            raise DataUnavailable("need %s but will never get it" %
+                                  disappointment.dump())
+
+    def _get_satisfaction(self):
+        # return True if we retired a data block, and should therefore be
+        # called again. Return False if we don't retire a data block (even if
+        # we do retire some other data, like hash chains).
+
+        if self.actual_offsets is None:
+            if not self._satisfy_offsets():
+                # can't even look at anything without the offset table
+                return False
+
+        if not self._node.have_UEB:
+            if not self._satisfy_UEB():
+                # can't check any hashes without the UEB
+                return False
+        self.actual_segment_size = self._node.segment_size # might be updated
+        assert self.actual_segment_size is not None
+
+        # knowing the UEB means knowing num_segments. Despite the redundancy,
+        # this is the best place to set this. CommonShare.set_numsegs will
+        # ignore duplicate calls.
+        assert self._node.num_segments is not None
+        cs = self._commonshare
+        cs.set_numsegs(self._node.num_segments)
+
+        segnum, observers = self._active_segnum_and_observers()
+        # if segnum is None, we don't really need to do anything (we have no
+        # outstanding readers right now), but we'll fill in the bits that
+        # aren't tied to any particular segment.
+
+        if segnum is not None and segnum >= self._node.num_segments:
+            for o in observers:
+                o.notify(state=BADSEGNUM)
+            self._requested_blocks.pop(0)
+            return True
+
+        if self._node.share_hash_tree.needed_hashes(self._shnum):
+            if not self._satisfy_share_hash_tree():
+                # can't check block_hash_tree without a root
+                return False
+
+        if cs.need_block_hash_root():
+            block_hash_root = self._node.share_hash_tree.get_leaf(self._shnum)
+            cs.set_block_hash_root(block_hash_root)
+
+        if segnum is None:
+            return False # we don't want any particular segment right now
+
+        # block_hash_tree
+        needed_hashes = self._commonshare.get_needed_block_hashes(segnum)
+        if needed_hashes:
+            if not self._satisfy_block_hash_tree(needed_hashes):
+                # can't check block without block_hash_tree
+                return False
+
+        # ciphertext_hash_tree
+        needed_hashes = self._node.get_needed_ciphertext_hashes(segnum)
+        if needed_hashes:
+            if not self._satisfy_ciphertext_hash_tree(needed_hashes):
+                # can't check decoded blocks without ciphertext_hash_tree
+                return False
+
+        # data blocks
+        return self._satisfy_data_block(segnum, observers)
+
+    def _satisfy_offsets(self):
+        version_s = self._received.get(0, 4)
+        if version_s is None:
+            return False
+        (version,) = struct.unpack(">L", version_s)
+        if version == 1:
+            table_start = 0x0c
+            self._fieldsize = 0x4
+            self._fieldstruct = "L"
+        elif version == 2:
+            table_start = 0x14
+            self._fieldsize = 0x8
+            self._fieldstruct = "Q"
+        else:
+            self.had_corruption = True
+            raise LayoutInvalid("unknown version %d (I understand 1 and 2)"
+                                % version)
+        offset_table_size = 6 * self._fieldsize
+        table_s = self._received.pop(table_start, offset_table_size)
+        if table_s is None:
+            return False
+        fields = struct.unpack(">"+6*self._fieldstruct, table_s)
+        offsets = {}
+        for i,field in enumerate(['data',
+                                  'plaintext_hash_tree', # UNUSED
+                                  'crypttext_hash_tree',
+                                  'block_hashes',
+                                  'share_hashes',
+                                  'uri_extension',
+                                  ] ):
+            offsets[field] = fields[i]
+        self.actual_offsets = offsets
+        log.msg("actual offsets: data=%d, plaintext_hash_tree=%d, crypttext_hash_tree=%d, block_hashes=%d, share_hashes=%d, uri_extension=%d" % tuple(fields))
+        self._received.remove(0, 4) # don't need this anymore
+
+        # validate the offsets a bit
+        share_hashes_size = offsets["uri_extension"] - offsets["share_hashes"]
+        if share_hashes_size < 0 or share_hashes_size % (2+HASH_SIZE) != 0:
+            # the share hash chain is stored as (hashnum,hash) pairs
+            self.had_corruption = True
+            raise LayoutInvalid("share hashes malformed -- should be a"
+                                " multiple of %d bytes -- not %d" %
+                                (2+HASH_SIZE, share_hashes_size))
+        block_hashes_size = offsets["share_hashes"] - offsets["block_hashes"]
+        if block_hashes_size < 0 or block_hashes_size % (HASH_SIZE) != 0:
+            # the block hash tree is stored as a list of hashes
+            self.had_corruption = True
+            raise LayoutInvalid("block hashes malformed -- should be a"
+                                " multiple of %d bytes -- not %d" %
+                                (HASH_SIZE, block_hashes_size))
+        # we only look at 'crypttext_hash_tree' if the UEB says we're
+        # actually using it. Same with 'plaintext_hash_tree'. This gives us
+        # some wiggle room: a place to stash data for later extensions.
+
+        return True
+
+    def _satisfy_UEB(self):
+        o = self.actual_offsets
+        fsize = self._fieldsize
+        UEB_length_s = self._received.get(o["uri_extension"], fsize)
+        if not UEB_length_s:
+            return False
+        (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
+        UEB_s = self._received.pop(o["uri_extension"]+fsize, UEB_length)
+        if not UEB_s:
+            return False
+        self._received.remove(o["uri_extension"], fsize)
+        try:
+            self._node.validate_and_store_UEB(UEB_s)
+            return True
+        except (LayoutInvalid, BadHashError), e:
+            # TODO: if this UEB was bad, we'll keep trying to validate it
+            # over and over again. Only log.err on the first one, or better
+            # yet skip all but the first
+            f = Failure(e)
+            self._signal_corruption(f, o["uri_extension"], fsize+UEB_length)
+            self.had_corruption = True
+            raise
+
+    def _satisfy_share_hash_tree(self):
+        # the share hash chain is stored as (hashnum,hash) tuples, so you
+        # can't fetch just the pieces you need, because you don't know
+        # exactly where they are. So fetch everything, and parse the results
+        # later.
+        o = self.actual_offsets
+        hashlen = o["uri_extension"] - o["share_hashes"]
+        assert hashlen % (2+HASH_SIZE) == 0
+        hashdata = self._received.get(o["share_hashes"], hashlen)
+        if not hashdata:
+            return False
+        share_hashes = {}
+        for i in range(0, hashlen, 2+HASH_SIZE):
+            (hashnum,) = struct.unpack(">H", hashdata[i:i+2])
+            hashvalue = hashdata[i+2:i+2+HASH_SIZE]
+            share_hashes[hashnum] = hashvalue
+        try:
+            self._node.process_share_hashes(share_hashes)
+            # adds to self._node.share_hash_tree
+        except (BadHashError, NotEnoughHashesError), e:
+            f = Failure(e)
+            self._signal_corruption(f, o["share_hashes"], hashlen)
+            self.had_corruption = True
+            raise
+        self._received.remove(o["share_hashes"], hashlen)
+        return True
+
+    def _signal_corruption(self, f, start, offset):
+        # there was corruption somewhere in the given range
+        reason = "corruption in share[%d-%d): %s" % (start, start+offset,
+                                                     str(f.value))
+        self._rref.callRemoteOnly("advise_corrupt_share", reason)
+
+    def _satisfy_block_hash_tree(self, needed_hashes):
+        o_bh = self.actual_offsets["block_hashes"]
+        block_hashes = {}
+        for hashnum in needed_hashes:
+            hashdata = self._received.get(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
+            if hashdata:
+                block_hashes[hashnum] = hashdata
+            else:
+                return False # missing some hashes
+        # note that we don't submit any hashes to the block_hash_tree until
+        # we've gotten them all, because the hash tree will throw an
+        # exception if we only give it a partial set (which it therefore
+        # cannot validate)
+        try:
+            self._commonshare.process_block_hashes(block_hashes)
+        except (BadHashError, NotEnoughHashesError), e:
+            f = Failure(e)
+            hashnums = ",".join([str(n) for n in sorted(block_hashes.keys())])
+            log.msg(format="hash failure in block_hashes=(%(hashnums)s),"
+                    " from %(share)s",
+                    hashnums=hashnums, shnum=self._shnum, share=repr(self),
+                    failure=f, level=log.WEIRD, parent=self._lp, umid="yNyFdA")
+            hsize = max(0, max(needed_hashes)) * HASH_SIZE
+            self._signal_corruption(f, o_bh, hsize)
+            self.had_corruption = True
+            raise
+        for hashnum in needed_hashes:
+            self._received.remove(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
+        return True
+
+    def _satisfy_ciphertext_hash_tree(self, needed_hashes):
+        start = self.actual_offsets["crypttext_hash_tree"]
+        hashes = {}
+        for hashnum in needed_hashes:
+            hashdata = self._received.get(start+hashnum*HASH_SIZE, HASH_SIZE)
+            if hashdata:
+                hashes[hashnum] = hashdata
+            else:
+                return False # missing some hashes
+        # we don't submit any hashes to the ciphertext_hash_tree until we've
+        # gotten them all
+        try:
+            self._node.process_ciphertext_hashes(hashes)
+        except (BadHashError, NotEnoughHashesError), e:
+            f = Failure(e)
+            hashnums = ",".join([str(n) for n in sorted(hashes.keys())])
+            log.msg(format="hash failure in ciphertext_hashes=(%(hashnums)s),"
+                    " from %(share)s",
+                    hashnums=hashnums, share=repr(self), failure=f,
+                    level=log.WEIRD, parent=self._lp, umid="iZI0TA")
+            hsize = max(0, max(needed_hashes))*HASH_SIZE
+            self._signal_corruption(f, start, hsize)
+            self.had_corruption = True
+            raise
+        for hashnum in needed_hashes:
+            self._received.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
+        return True
+
+    def _satisfy_data_block(self, segnum, observers):
+        tail = (segnum == self._node.num_segments-1)
+        datastart = self.actual_offsets["data"]
+        blockstart = datastart + segnum * self._node.block_size
+        blocklen = self._node.block_size
+        if tail:
+            blocklen = self._node.tail_block_size
+
+        block = self._received.pop(blockstart, blocklen)
+        if not block:
+            log.msg("no data for block %s (want [%d:+%d])" % (repr(self),
+                                                              blockstart, blocklen))
+            return False
+        log.msg(format="%(share)s._satisfy_data_block [%(start)d:+%(length)d]",
+                share=repr(self), start=blockstart, length=blocklen,
+                level=log.NOISY, parent=self._lp, umid="uTDNZg")
+        # this block is being retired, either as COMPLETE or CORRUPT, since
+        # no further data reads will help
+        assert self._requested_blocks[0][0] == segnum
+        try:
+            self._commonshare.check_block(segnum, block)
+            # hurrah, we have a valid block. Deliver it.
+            for o in observers:
+                # goes to SegmentFetcher._block_request_activity
+                o.notify(state=COMPLETE, block=block)
+        except (BadHashError, NotEnoughHashesError), e:
+            # rats, we have a corrupt block. Notify our clients that they
+            # need to look elsewhere, and advise the server. Unlike
+            # corruption in other parts of the share, this doesn't cause us
+            # to abandon the whole share.
+            f = Failure(e)
+            log.msg(format="hash failure in block %(segnum)d, from %(share)s",
+                    segnum=segnum, share=repr(self), failure=f,
+                    level=log.WEIRD, parent=self._lp, umid="mZjkqA")
+            for o in observers:
+                o.notify(state=CORRUPT)
+            self._signal_corruption(f, blockstart, blocklen)
+            self.had_corruption = True
+        # in either case, we've retired this block
+        self._requested_blocks.pop(0)
+        # popping the request keeps us from turning around and wanting the
+        # block again right away
+        return True # got satisfaction
+
+    def _desire(self):
+        segnum, observers = self._active_segnum_and_observers() # maybe None
+
+        # 'want_it' is for data we merely want: we know that we don't really
+        # need it. This includes speculative reads, like the first 1KB of the
+        # share (for the offset table) and the first 2KB of the UEB.
+        #
+        # 'need_it' is for data that, if we have the real offset table, we'll
+        # need. If we are only guessing at the offset table, it's merely
+        # wanted. (The share is abandoned if we can't get data that we really
+        # need).
+        #
+        # 'gotta_gotta_have_it' is for data that we absolutely need,
+        # independent of whether we're still guessing about the offset table:
+        # the version number and the offset table itself.
+        #
+        # Mr. Popeil, I'm in trouble, need your assistance on the double. Aww..
+
+        desire = Spans(), Spans(), Spans()
+        (want_it, need_it, gotta_gotta_have_it) = desire
+
+        self.actual_segment_size = self._node.segment_size # might be updated
+        o = self.actual_offsets or self.guessed_offsets
+        segsize = self.actual_segment_size or self.guessed_segment_size
+        r = self._node._calculate_sizes(segsize)
+
+        if not self.actual_offsets:
+            # all _desire functions add bits to the three desire[] spans
+            self._desire_offsets(desire)
+
+        # we can use guessed offsets as long as this server tolerates
+        # overrun. Otherwise, we must wait for the offsets to arrive before
+        # we try to read anything else.
+        if self.actual_offsets or self._overrun_ok:
+            if not self._node.have_UEB:
+                self._desire_UEB(desire, o)
+            # They might ask for a segment that doesn't look right.
+            # _satisfy() will catch+reject bad segnums once we know the UEB
+            # (and therefore segsize and numsegs), so we'll only fail this
+            # test if we're still guessing. We want to avoid asking the
+            # hashtrees for needed_hashes() for bad segnums. So don't enter
+            # _desire_hashes or _desire_data unless the segnum looks
+            # reasonable.
+            if segnum < r["num_segments"]:
+                # XXX somehow we're getting here for sh5. we don't yet know
+                # the actual_segment_size, we're still working off the guess.
+                # the ciphertext_hash_tree has been corrected, but the
+                # commonshare._block_hash_tree is still in the guessed state.
+                self._desire_share_hashes(desire, o)
+                if segnum is not None:
+                    self._desire_block_hashes(desire, o, segnum)
+                    self._desire_data(desire, o, r, segnum, segsize)
+            else:
+                log.msg("_desire: segnum(%d) looks wrong (numsegs=%d)"
+                        % (segnum, r["num_segments"]),
+                        level=log.UNUSUAL, parent=self._lp, umid="tuYRQQ")
+
+        log.msg("end _desire: want_it=%s need_it=%s gotta=%s"
+                % (want_it.dump(), need_it.dump(), gotta_gotta_have_it.dump()))
+        if self.actual_offsets:
+            return (want_it, need_it+gotta_gotta_have_it)
+        else:
+            return (want_it+need_it, gotta_gotta_have_it)
+
+    def _desire_offsets(self, desire):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+        if self._overrun_ok:
+            # easy! this includes version number, sizes, and offsets
+            want_it.add(0, 1024)
+            return
+
+        # v1 has an offset table that lives [0x0,0x24). v2 lives [0x0,0x44).
+        # To be conservative, only request the data that we know lives there,
+        # even if that means more roundtrips.
+
+        gotta_gotta_have_it.add(0, 4)  # version number, always safe
+        version_s = self._received.get(0, 4)
+        if not version_s:
+            return
+        (version,) = struct.unpack(">L", version_s)
+        # The code in _satisfy_offsets will have checked this version
+        # already. There is no code path to get this far with version>2.
+        assert 1 <= version <= 2, "can't get here, version=%d" % version
+        if version == 1:
+            table_start = 0x0c
+            fieldsize = 0x4
+        elif version == 2:
+            table_start = 0x14
+            fieldsize = 0x8
+        offset_table_size = 6 * fieldsize
+        gotta_gotta_have_it.add(table_start, offset_table_size)
+
+    def _desire_UEB(self, desire, o):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+
+        # UEB data is stored as (length,data).
+        if self._overrun_ok:
+            # We can pre-fetch 2kb, which should probably cover it. If it
+            # turns out to be larger, we'll come back here later with a known
+            # length and fetch the rest.
+            want_it.add(o["uri_extension"], 2048)
+            # now, while that is probably enough to fetch the whole UEB, it
+            # might not be, so we need to do the next few steps as well. In
+            # most cases, the following steps will not actually add anything
+            # to need_it
+
+        need_it.add(o["uri_extension"], self._fieldsize)
+        # only use a length if we're sure it's correct, otherwise we'll
+        # probably fetch a huge number
+        if not self.actual_offsets:
+            return
+        UEB_length_s = self._received.get(o["uri_extension"], self._fieldsize)
+        if UEB_length_s:
+            (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
+            # we know the length, so make sure we grab everything
+            need_it.add(o["uri_extension"]+self._fieldsize, UEB_length)
+
+    def _desire_share_hashes(self, desire, o):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+
+        if self._node.share_hash_tree.needed_hashes(self._shnum):
+            hashlen = o["uri_extension"] - o["share_hashes"]
+            need_it.add(o["share_hashes"], hashlen)
+
+    def _desire_block_hashes(self, desire, o, segnum):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+
+        # block hash chain
+        for hashnum in self._commonshare.get_needed_block_hashes(segnum):
+            need_it.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+
+        # ciphertext hash chain
+        for hashnum in self._node.get_needed_ciphertext_hashes(segnum):
+            need_it.add(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
+
+    def _desire_data(self, desire, o, r, segnum, segsize):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+        tail = (segnum == r["num_segments"]-1)
+        datastart = o["data"]
+        blockstart = datastart + segnum * r["block_size"]
+        blocklen = r["block_size"]
+        if tail:
+            blocklen = r["tail_block_size"]
+        need_it.add(blockstart, blocklen)
+
+    def _send_requests(self, desired):
+        ask = desired - self._pending - self._received.get_spans()
+        log.msg("%s._send_requests, desired=%s, pending=%s, ask=%s" %
+                (repr(self), desired.dump(), self._pending.dump(), ask.dump()),
+                level=log.NOISY, parent=self._lp, umid="E94CVA")
+        # XXX At one time, this code distinguished between data blocks and
+        # hashes, and made sure to send (small) requests for hashes before
+        # sending (big) requests for blocks. The idea was to make sure that
+        # all hashes arrive before the blocks, so the blocks can be consumed
+        # and released in a single turn. I removed this for simplicity.
+        # Reconsider the removal: maybe bring it back.
+        ds = self._download_status
+
+        for (start, length) in ask:
+            # TODO: quantize to reasonably-large blocks
+            self._pending.add(start, length)
+            lp = log.msg(format="%(share)s._send_request"
+                         " [%(start)d:+%(length)d]",
+                         share=repr(self),
+                         start=start, length=length,
+                         level=log.NOISY, parent=self._lp, umid="sgVAyA")
+            req_ev = ds.add_request_sent(self._peerid, self._shnum,
+                                         start, length, now())
+            d = self._send_request(start, length)
+            d.addCallback(self._got_data, start, length, req_ev, lp)
+            d.addErrback(self._got_error, start, length, req_ev, lp)
+            d.addCallback(self._trigger_loop)
+            d.addErrback(lambda f:
+                         log.err(format="unhandled error during send_request",
+                                 failure=f, parent=self._lp,
+                                 level=log.WEIRD, umid="qZu0wg"))
+
+    def _send_request(self, start, length):
+        return self._rref.callRemote("read", start, length)
+
+    def _got_data(self, data, start, length, req_ev, lp):
+        req_ev.finished(len(data), now())
+        if not self._alive:
+            return
+        log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
+                share=repr(self), start=start, length=length, datalen=len(data),
+                level=log.NOISY, parent=lp, umid="5Qn6VQ")
+        self._pending.remove(start, length)
+        self._received.add(start, data)
+
+        # if we ask for [a:c], and we get back [a:b] (b<c), that means we're
+        # never going to get [b:c]. If we really need that data, this block
+        # will never complete. The easiest way to get into this situation is
+        # to hit a share with a corrupted offset table, or one that's somehow
+        # been truncated. On the other hand, when overrun_ok is true, we ask
+        # for data beyond the end of the share all the time (it saves some
+        # RTT when we don't know the length of the share ahead of time). So
+        # not every asked-for-but-not-received byte is fatal.
+        if len(data) < length:
+            self._unavailable.add(start+len(data), length-len(data))
+
+        # XXX if table corruption causes our sections to overlap, then one
+        # consumer (i.e. block hash tree) will pop/remove the data that
+        # another consumer (i.e. block data) mistakenly thinks it needs. It
+        # won't ask for that data again, because the span is in
+        # self._requested. But that span won't be in self._unavailable
+        # because we got it back from the server. TODO: handle this properly
+        # (raise DataUnavailable). Then add sanity-checking
+        # no-overlaps-allowed tests to the offset-table unpacking code to
+        # catch this earlier. XXX
+
+        # accumulate a wanted/needed span (not as self._x, but passed into
+        # desire* functions). manage a pending/in-flight list. when the
+        # requests are sent out, empty/discard the wanted/needed span and
+        # populate/augment the pending list. when the responses come back,
+        # augment either received+data or unavailable.
+
+        # if a corrupt offset table results in double-usage, we'll send
+        # double requests.
+
+        # the wanted/needed span is only "wanted" for the first pass. Once
+        # the offset table arrives, it's all "needed".
+
+    def _got_error(self, f, start, length, req_ev, lp):
+        req_ev.finished("error", now())
+        log.msg(format="error requesting %(start)d+%(length)d"
+                " from %(server)s for si %(si)s",
+                start=start, length=length,
+                server=self._peerid_s, si=self._si_prefix,
+                failure=f, parent=lp, level=log.UNUSUAL, umid="BZgAJw")
+        # retire our observers, assuming we won't be able to make any
+        # further progress
+        self._fail(f, log.UNUSUAL)
+
+    def _trigger_loop(self, res):
+        if self._alive:
+            eventually(self.loop)
+        return res
+
+    def _fail(self, f, level=log.WEIRD):
+        log.msg(format="abandoning %(share)s",
+                share=repr(self), failure=f,
+                level=level, parent=self._lp, umid="JKM2Og")
+        self._alive = False
+        for (segnum, observers) in self._requested_blocks:
+            for o in observers:
+                o.notify(state=DEAD, f=f)
+
+
+class CommonShare:
+    """I hold data that is common across all instances of a single share,
+    like sh2 on both servers A and B. This is just the block hash tree.
+    """
+    def __init__(self, guessed_numsegs, si_prefix, shnum, logparent):
+        self.si_prefix = si_prefix
+        self.shnum = shnum
+        # in the beginning, before we have the real UEB, we can only guess at
+        # the number of segments. But we want to ask for block hashes early.
+        # So if we're asked for which block hashes are needed before we know
+        # numsegs for sure, we return a guess.
+        self._block_hash_tree = IncompleteHashTree(guessed_numsegs)
+        self._know_numsegs = False
+        self._logparent = logparent
+
+    def set_numsegs(self, numsegs):
+        if self._know_numsegs:
+            return
+        self._block_hash_tree = IncompleteHashTree(numsegs)
+        self._know_numsegs = True
+
+    def need_block_hash_root(self):
+        return bool(not self._block_hash_tree[0])
+
+    def set_block_hash_root(self, roothash):
+        assert self._know_numsegs
+        self._block_hash_tree.set_hashes({0: roothash})
+
+    def get_needed_block_hashes(self, segnum):
+        # XXX: include_leaf=True needs thought: how did the old downloader do
+        # it? I think it grabbed *all* block hashes and set them all at once.
+        # Since we want to fetch less data, we either need to fetch the leaf
+        # too, or wait to set the block hashes until we've also received the
+        # block itself, so we can hash it too, and set the chain+leaf all at
+        # the same time.
+        return self._block_hash_tree.needed_hashes(segnum, include_leaf=True)
+
+    def process_block_hashes(self, block_hashes):
+        assert self._know_numsegs
+        # this may raise BadHashError or NotEnoughHashesError
+        self._block_hash_tree.set_hashes(block_hashes)
+
+    def check_block(self, segnum, block):
+        assert self._know_numsegs
+        h = hashutil.block_hash(block)
+        # this may raise BadHashError or NotEnoughHashesError
+        self._block_hash_tree.set_hashes(leaves={segnum: h})
diff --git a/src/allmydata/immutable/downloader/status.py b/src/allmydata/immutable/downloader/status.py
new file mode 100644
index 0000000..5d60db0
--- /dev/null
+++ b/src/allmydata/immutable/downloader/status.py
@@ -0,0 +1,170 @@
+
+import itertools
+from zope.interface import implements
+from allmydata.interfaces import IDownloadStatus
+
+class RequestEvent:
+    def __init__(self, download_status, tag):
+        self._download_status = download_status
+        self._tag = tag
+    def finished(self, received, when):
+        self._download_status.add_request_finished(self._tag, received, when)
+
+class DYHBEvent:
+    def __init__(self, download_status, tag):
+        self._download_status = download_status
+        self._tag = tag
+    def finished(self, shnums, when):
+        self._download_status.add_dyhb_finished(self._tag, shnums, when)
+
+class ReadEvent:
+    def __init__(self, download_status, tag):
+        self._download_status = download_status
+        self._tag = tag
+    def update(self, bytes, decrypttime, pausetime):
+        self._download_status.update_read_event(self._tag, bytes,
+                                                decrypttime, pausetime)
+    def finished(self, finishtime):
+        self._download_status.finish_read_event(self._tag, finishtime)
+
+class DownloadStatus:
+    # There is one DownloadStatus for each CiphertextFileNode. The status
+    # object will keep track of all activity for that node.
+    implements(IDownloadStatus)
+    statusid_counter = itertools.count(0)
+
+    def __init__(self, storage_index, size):
+        self.storage_index = storage_index
+        self.size = size
+        self.counter = self.statusid_counter.next()
+        self.helper = False
+        self.started = None
+        # self.dyhb_requests tracks "do you have a share" requests and
+        # responses. It maps serverid to a tuple of:
+        #  send time
+        #  tuple of response shnums (None if response hasn't arrived, "error")
+        #  response time (None if response hasn't arrived yet)
+        self.dyhb_requests = {}
+
+        # self.requests tracks share-data requests and responses. It maps
+        # serverid to a tuple of:
+        #  shnum,
+        #  start,length,  (of data requested)
+        #  send time
+        #  response length (None if reponse hasn't arrived yet, or "error")
+        #  response time (None if response hasn't arrived)
+        self.requests = {}
+
+        # self.segment_events tracks segment requests and delivery. It is a
+        # list of:
+        #  type ("request", "delivery", "error")
+        #  segment number
+        #  event time
+        #  segment start (file offset of first byte, None except in "delivery")
+        #  segment length (only in "delivery")
+        #  time spent in decode (only in "delivery")
+        self.segment_events = []
+
+        # self.read_events tracks read() requests. It is a list of:
+        #  start,length  (of data requested)
+        #  request time
+        #  finish time (None until finished)
+        #  bytes returned (starts at 0, grows as segments are delivered)
+        #  time spent in decrypt (None for ciphertext-only reads)
+        #  time spent paused
+        self.read_events = []
+
+        self.known_shares = [] # (serverid, shnum)
+        self.problems = []
+
+
+    def add_dyhb_sent(self, serverid, when):
+        r = (when, None, None)
+        if serverid not in self.dyhb_requests:
+            self.dyhb_requests[serverid] = []
+        self.dyhb_requests[serverid].append(r)
+        tag = (serverid, len(self.dyhb_requests[serverid])-1)
+        return DYHBEvent(self, tag)
+
+    def add_dyhb_finished(self, tag, shnums, when):
+        # received="error" on error, else tuple(shnums)
+        (serverid, index) = tag
+        r = self.dyhb_requests[serverid][index]
+        (sent, _, _) = r
+        r = (sent, shnums, when)
+        self.dyhb_requests[serverid][index] = r
+
+    def add_request_sent(self, serverid, shnum, start, length, when):
+        r = (shnum, start, length, when, None, None)
+        if serverid not in self.requests:
+            self.requests[serverid] = []
+        self.requests[serverid].append(r)
+        tag = (serverid, len(self.requests[serverid])-1)
+        return RequestEvent(self, tag)
+
+    def add_request_finished(self, tag, received, when):
+        # received="error" on error, else len(data)
+        (serverid, index) = tag
+        r = self.requests[serverid][index]
+        (shnum, start, length, sent, _, _) = r
+        r = (shnum, start, length, sent, received, when)
+        self.requests[serverid][index] = r
+
+    def add_segment_request(self, segnum, when):
+        if self.started is None:
+            self.started = when
+        r = ("request", segnum, when, None, None, None)
+        self.segment_events.append(r)
+    def add_segment_delivery(self, segnum, when, start, length, decodetime):
+        r = ("delivery", segnum, when, start, length, decodetime)
+        self.segment_events.append(r)
+    def add_segment_error(self, segnum, when):
+        r = ("error", segnum, when, None, None, None)
+        self.segment_events.append(r)
+
+    def add_read_event(self, start, length, when):
+        if self.started is None:
+            self.started = when
+        r = (start, length, when, None, 0, 0, 0)
+        self.read_events.append(r)
+        tag = len(self.read_events)-1
+        return ReadEvent(self, tag)
+    def update_read_event(self, tag, bytes_d, decrypt_d, paused_d):
+        r = self.read_events[tag]
+        (start, length, requesttime, finishtime, bytes, decrypt, paused) = r
+        bytes += bytes_d
+        decrypt += decrypt_d
+        paused += paused_d
+        r = (start, length, requesttime, finishtime, bytes, decrypt, paused)
+        self.read_events[tag] = r
+    def finish_read_event(self, tag, finishtime):
+        r = self.read_events[tag]
+        (start, length, requesttime, _, bytes, decrypt, paused) = r
+        r = (start, length, requesttime, finishtime, bytes, decrypt, paused)
+        self.read_events[tag] = r
+
+    def add_known_share(self, serverid, shnum):
+        self.known_shares.append( (serverid, shnum) )
+
+    def add_problem(self, p):
+        self.problems.append(p)
+
+    # IDownloadStatus methods
+    def get_counter(self):
+        return self.counter
+    def get_storage_index(self):
+        return self.storage_index
+    def get_size(self):
+        return self.size
+    def get_status(self):
+        return "not impl yet" # TODO
+    def get_progress(self):
+        return 0.1 # TODO
+    def using_helper(self):
+        return False
+    def get_active(self):
+        return False # TODO
+    def get_started(self):
+        return self.started
+    def get_results(self):
+        return None # TODO
diff --git a/src/allmydata/immutable/downloader/util.py b/src/allmydata/immutable/downloader/util.py
new file mode 100644
index 0000000..d45f5cc
--- /dev/null
+++ b/src/allmydata/immutable/downloader/util.py
@@ -0,0 +1,73 @@
+import weakref
+
+from twisted.application import service
+from foolscap.api import eventually
+
+class Observer2:
+    """A simple class to distribute multiple events to a single subscriber.
+    It accepts arbitrary kwargs, but no posargs."""
+    def __init__(self):
+        self._watcher = None
+        self._undelivered_results = []
+        self._canceler = None
+
+    def set_canceler(self, c, methname):
+        """I will call c.METHNAME(self) when somebody cancels me."""
+        # we use a weakref to avoid creating a cycle between us and the thing
+        # we're observing: they'll be holding a reference to us to compare
+        # against the value we pass to their canceler function. However,
+        # since bound methods are first-class objects (and not kept alive by
+        # the object they're bound to), we can't just stash a weakref to the
+        # bound cancel method. Instead, we must hold a weakref to the actual
+        # object, and obtain its cancel method later.
+        # http://code.activestate.com/recipes/81253-weakmethod/ has an
+        # alternative.
+        self._canceler = (weakref.ref(c), methname)
+
+    def subscribe(self, observer, **watcher_kwargs):
+        self._watcher = (observer, watcher_kwargs)
+        while self._undelivered_results:
+            self._notify(self._undelivered_results.pop(0))
+
+    def notify(self, **result_kwargs):
+        if self._watcher:
+            self._notify(result_kwargs)
+        else:
+            self._undelivered_results.append(result_kwargs)
+
+    def _notify(self, result_kwargs):
+        o, watcher_kwargs = self._watcher
+        kwargs = dict(result_kwargs)
+        kwargs.update(watcher_kwargs)
+        eventually(o, **kwargs)
+
+    def cancel(self):
+        wr,methname = self._canceler
+        o = wr()
+        if o:
+            getattr(o,methname)(self)
+
+
+def incidentally(res, f, *args, **kwargs):
+    """Add me to a Deferred chain like this:
+     d.addBoth(incidentally, func, arg)
+    and I'll behave as if you'd added the following function:
+     def _(res):
+         func(arg)
+         return res
+    This is useful if you want to execute an expression when the Deferred
+    fires, but don't care about its value.
+    """
+    f(*args, **kwargs)
+    return res
+
+
+class Terminator(service.Service):
+    def __init__(self):
+        self._clients = weakref.WeakKeyDictionary()
+    def register(self, c):
+        self._clients[c] = None
+    def stopService(self):
+        for c in self._clients:
+            c.stop()
+        return service.Service.stopService(self)
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index 35c646c..95fae2f 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -14,8 +14,8 @@ from allmydata.immutable import upload, layout
 from allmydata.test.no_network import GridTestMixin
 from allmydata.test.common import ShouldFailMixin
 from allmydata.interfaces import NotEnoughSharesError, NoSharesError
-from allmydata.immutable.download2 import BadSegmentNumberError, \
-     WrongSegmentError, BadCiphertextHashError
+from allmydata.immutable.downloader.common import BadSegmentNumberError, \
+     BadCiphertextHashError
 from allmydata.codec import CRSDecoder
 from foolscap.eventual import fireEventually, flushEventualQueue
 

commit 2f4352e590298183d13f09c3434fd7288a56b9c5
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 09:54:22 2010 -0700

    add __eq__ and friends to ImmutableFileNode. Not sure I want them there.
---
 src/allmydata/immutable/download2.py |   15 +++++++++++++++
 1 files changed, 15 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 7406a95..ec284b3 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -2016,6 +2016,21 @@ class ImmutableFileNode:
         self.u = filecap
         self._readkey = filecap.key
 
+    # TODO: I'm not sure about this.. what's the use case for node==node? If
+    # we keep it here, we should also put this on CiphertextFileNode
+    def __hash__(self):
+        return self.u.__hash__()
+    def __eq__(self, other):
+        if isinstance(other, ImmutableFileNode):
+            return self.u.__eq__(other.u)
+        else:
+            return False
+    def __ne__(self, other):
+        if isinstance(other, ImmutableFileNode):
+            return self.u.__eq__(other.u)
+        else:
+            return True
+
     def read(self, consumer, offset=0, size=None):
         actual_size = size
         if actual_size == None:

commit a83e85c5b4ee7ef63338ba7a9776d018bc1842ff
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 09:25:29 2010 -0700

    fix remote call to advise_corrupt_share: the signature is different for
    immutable buckets
---
 src/allmydata/test/test_filenode.py |    8 ++++----
 1 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/src/allmydata/test/test_filenode.py b/src/allmydata/test/test_filenode.py
index 5f3feaa..19d6f5e 100644
--- a/src/allmydata/test/test_filenode.py
+++ b/src/allmydata/test/test_filenode.py
@@ -2,7 +2,8 @@
 from twisted.trial import unittest
 from allmydata import uri, client
 from allmydata.monitor import Monitor
-from allmydata.immutable.filenode import ImmutableFileNode, LiteralFileNode
+from allmydata.immutable.filenode import LiteralFileNode
+from allmydata.immutable.download2 import ImmutableFileNode
 from allmydata.mutable.filenode import MutableFileNode
 from allmydata.util import hashutil, cachedir
 from allmydata.util.consumer import download_to_data
@@ -30,9 +31,8 @@ class Node(unittest.TestCase):
                            needed_shares=3,
                            total_shares=10,
                            size=1000)
-        cf = cachedir.CacheFile("none")
-        fn1 = ImmutableFileNode(u, None, None, None, None, cf)
-        fn2 = ImmutableFileNode(u, None, None, None, None, cf)
+        fn1 = ImmutableFileNode(u, None, None, None, None)
+        fn2 = ImmutableFileNode(u, None, None, None, None)
         self.failUnlessEqual(fn1, fn2)
         self.failIfEqual(fn1, "I am not a filenode")
         self.failIfEqual(fn1, NotANode())

commit 480349915caa9897cc35064736601be1fa3cf26a
Author: Brian Warner <warner@lothar.com>
Date:   Fri May 21 09:20:07 2010 -0700

    fix remote call to advise_corrupt_share: the signature is different for
    immutable buckets
---
 src/allmydata/immutable/download2.py |    3 +--
 1 files changed, 1 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 5af7718..7406a95 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -460,8 +460,7 @@ class Share:
         # there was corruption somewhere in the given range
         reason = "corruption in share[%d-%d): %s" % (start, start+offset,
                                                      str(f.value))
-        self._rref.callRemoteOnly("advise_corrupt_share", "immutable",
-                                  self._storage_index, self._shnum, reason)
+        self._rref.callRemoteOnly("advise_corrupt_share", reason)
 
     def _satisfy_block_hash_tree(self, needed_hashes):
         o_bh = self.actual_offsets["block_hashes"]

commit d9f19630cc3b195cef2112cfc141a51725476e60
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 15 17:56:53 2010 -0700

    add CiphertextFileNode to nodemaker, read() gets you ciphertext, duh
---
 src/allmydata/nodemaker.py          |    7 ++++++-
 src/allmydata/test/test_download.py |   16 ++++++++++++++++
 2 files changed, 22 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/nodemaker.py b/src/allmydata/nodemaker.py
index 8389301..033c34a 100644
--- a/src/allmydata/nodemaker.py
+++ b/src/allmydata/nodemaker.py
@@ -1,7 +1,7 @@
 import weakref
 from zope.interface import implements
 from allmydata.immutable.filenode import LiteralFileNode
-from allmydata.immutable.download2 import ImmutableFileNode
+from allmydata.immutable.download2 import ImmutableFileNode, CiphertextFileNode
 from allmydata.immutable.upload import Data
 from allmydata.mutable.filenode import MutableFileNode
 from allmydata.dirnode import DirectoryNode, pack_children
@@ -33,6 +33,9 @@ class NodeMaker:
     def _create_immutable(self, cap):
         return ImmutableFileNode(cap, self.storage_broker, self.secret_holder,
                                  self.terminator, self.history)
+    def _create_immutable_verifier(self, cap):
+        return CiphertextFileNode(cap, self.storage_broker, self.secret_holder,
+                                  self.terminator, self.history)
     def _create_mutable(self, cap):
         n = MutableFileNode(self.storage_broker, self.secret_holder,
                             self.default_encoding_parameters,
@@ -75,6 +78,8 @@ class NodeMaker:
             return self._create_lit(cap)
         if isinstance(cap, uri.CHKFileURI):
             return self._create_immutable(cap)
+        if isinstance(cap, uri.CHKFileVerifierURI):
+            return self._create_immutable_verifier(cap)
         if isinstance(cap, (uri.ReadonlySSKFileURI, uri.WriteableSSKFileURI)):
             return self._create_mutable(cap)
         if isinstance(cap, (uri.DirectoryURI,
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index c44e08a..35c646c 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -618,6 +618,22 @@ class DownloadTest(_Base, unittest.TestCase):
         d = self.download_immutable()
         return d
 
+    def test_verifycap(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+
+        n = self.c0.create_node_from_uri(immutable_uri)
+        vcap = n.get_verify_cap().to_string()
+        vn = self.c0.create_node_from_uri(vcap)
+        d = download_to_data(vn)
+        def _got_ciphertext(ciphertext):
+            self.failUnlessEqual(len(ciphertext), len(plaintext))
+            self.failIfEqual(ciphertext, plaintext)
+        d.addCallback(_got_ciphertext)
+        return d
+
 class BrokenDecoder(CRSDecoder):
     def decode(self, shares, shareids):
         d = CRSDecoder.decode(self, shares, shareids)

commit 0e0c7870109e448907d9debb380b55731ddc1359
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:42:13 2010 -0700

    misc/sizes.py: update, we now use SHA256 (not SHA1), large-file overhead
    grows to 0.5%
---
 misc/simulators/sizes.py |   16 ++++++++--------
 1 files changed, 8 insertions(+), 8 deletions(-)

diff --git a/misc/simulators/sizes.py b/misc/simulators/sizes.py
index d9c230a..7910946 100644
--- a/misc/simulators/sizes.py
+++ b/misc/simulators/sizes.py
@@ -60,22 +60,22 @@ class Sizes:
             self.block_arity = 0
             self.block_tree_depth = 0
             self.block_overhead = 0
-            self.bytes_until_some_data = 20 + share_size
+            self.bytes_until_some_data = 32 + share_size
             self.share_storage_overhead = 0
             self.share_transmission_overhead = 0
 
         elif mode == "beta":
             # k=num_blocks, d=1
-            # each block has a 20-byte hash
+            # each block has a 32-byte hash
             self.block_arity = num_blocks
             self.block_tree_depth = 1
-            self.block_overhead = 20
+            self.block_overhead = 32
             # the share has a list of hashes, one for each block
             self.share_storage_overhead = (self.block_overhead *
                                            num_blocks)
             # we can get away with not sending the hash of the share that
             # we're sending in full, once
-            self.share_transmission_overhead = self.share_storage_overhead - 20
+            self.share_transmission_overhead = self.share_storage_overhead - 32
             # we must get the whole list (so it can be validated) before
             # any data can be validated
             self.bytes_until_some_data = (self.share_transmission_overhead +
@@ -89,7 +89,7 @@ class Sizes:
             # to make things easier, we make the pessimistic assumption that
             # we have to store hashes for all the empty places in the tree
             # (when the number of shares is not an exact exponent of k)
-            self.block_overhead = 20
+            self.block_overhead = 32
             # the block hashes are organized into a k-ary tree, which
             # means storing (and eventually transmitting) more hashes. This
             # count includes all the low-level share hashes and the root.
@@ -98,18 +98,18 @@ class Sizes:
             #print "num_leaves", num_leaves
             #print "hash_nodes", hash_nodes
             # the storage overhead is this
-            self.share_storage_overhead = 20 * (hash_nodes - 1)
+            self.share_storage_overhead = 32 * (hash_nodes - 1)
             # the transmission overhead is smaller: if we actually transmit
             # every block, we don't have to transmit 1/k of the
             # lowest-level block hashes, and we don't have to transmit the
             # root because it was already sent with the share-level hash tree
-            self.share_transmission_overhead = 20 * (hash_nodes
+            self.share_transmission_overhead = 32 * (hash_nodes
                                                      - 1 # the root
                                                      - num_leaves / k)
             # we must get a full sibling hash chain before we can validate
             # any data
             sibling_length = d * (k-1)
-            self.bytes_until_some_data = 20 * sibling_length + block_size
+            self.bytes_until_some_data = 32 * sibling_length + block_size
             
             
 

commit 6dafe7e1600a4893ab9f6c9f96e8977bc06f5983
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:40:17 2010 -0700

    ShareFinder: loop until we have max_outstanding_requests in flight, to
    parallelize DHYB queries instead of running them sequentially.
---
 src/allmydata/immutable/download2.py |    4 ++++
 1 files changed, 4 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index fcb5800..5af7718 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1169,6 +1169,7 @@ class ShareFinder:
                      level=log.NOISY, umid="2n1qQw")
             eventually(self.share_consumer.got_shares, [sh])
             return
+
         if len(self.pending_requests) >= self.max_outstanding_requests:
             # cannot send more requests, must wait for some to retire
             return
@@ -1182,6 +1183,9 @@ class ShareFinder:
 
         if server:
             self.send_request(server)
+            # we loop again to get parallel queries. The check above will
+            # prevent us from looping forever.
+            eventually(self.loop)
             return
 
         if self.pending_requests:

commit bae06a7fa9b00408da858dfd5629a971b62d0045
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:38:53 2010 -0700

    cosmetic improvements around ciphertext_hash_tree
---
 src/allmydata/immutable/download2.py |    4 ++--
 1 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index d31c2d1..fcb5800 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1459,7 +1459,6 @@ class _Node:
         self.num_segments = None
         self.block_size = None
         self.tail_block_size = None
-        #self.ciphertext_hash_tree = None # size depends on num_segments
 
         # things to track callers that want data
 
@@ -1663,7 +1662,8 @@ class _Node:
         # one ciphertext that matches this read-cap or verify-cap. The
         # integrity check on the shares is not sufficient to prevent the
         # original encoder from creating some shares of file A and other
-        # shares of file B.
+        # shares of file B. self.ciphertext_hash_tree was a guess before:
+        # this is where we create it for real.
         self.ciphertext_hash_tree = IncompleteHashTree(self.num_segments)
         self.ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
 

commit bba608651ba04b95ef01c5c2ad2ea19da9f1b922
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:38:28 2010 -0700

    tolerate history=None, which occurs in some unit tests
---
 src/allmydata/immutable/download2.py |   13 ++++++++-----
 1 files changed, 8 insertions(+), 5 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index fc73e73..d31c2d1 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1527,9 +1527,10 @@ class _Node:
                      si=base32.b2a(self._verifycap.storage_index)[:8],
                      offset=offset, size=size,
                      level=log.OPERATIONAL, parent=self._lp, umid="l3j3Ww")
-        sp = self._history.stats_provider
-        sp.count("downloader.files_downloaded", 1) # really read() calls
-        sp.count("downloader.bytes_downloaded", size)
+        if self._history:
+            sp = self._history.stats_provider
+            sp.count("downloader.files_downloaded", 1) # really read() calls
+            sp.count("downloader.bytes_downloaded", size)
         s = Segmentation(self, offset, size, consumer, read_ev, lp)
         # this raises an interesting question: what segments to fetch? if
         # offset=0, always fetch the first segment, and then allow
@@ -1918,7 +1919,8 @@ class CiphertextFileNode:
         assert isinstance(verifycap, uri.CHKFileVerifierURI)
         if download_status is None:
             ds = DownloadStatus(verifycap.storage_index, verifycap.size)
-            history.add_download(ds)
+            if history:
+                history.add_download(ds)
             download_status = ds
         self._node = _Node(verifycap, storage_broker, secret_holder,
                            terminator, history, download_status)
@@ -2002,7 +2004,8 @@ class ImmutableFileNode:
         assert isinstance(filecap, uri.CHKFileURI)
         verifycap = filecap.get_verify_cap()
         ds = DownloadStatus(verifycap.storage_index, verifycap.size)
-        history.add_download(ds)
+        if history:
+            history.add_download(ds)
         self._download_status = ds
         self._cnode = CiphertextFileNode(verifycap, storage_broker,
                                          secret_holder, terminator, history, ds)

commit d9a4e1ee5dd54459313a5f1f6edfd20f9809b596
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:38:02 2010 -0700

    don't ask for bytes that we already have. This removes a read from the case
    where we receive the 1024B offset-table overread and then need some hashes
    from within that range.
---
 src/allmydata/immutable/download2.py |    5 ++++-
 1 files changed, 4 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index b7eecc0..fc73e73 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -306,6 +306,7 @@ class Share:
         # knowing the UEB means knowing num_segments. Despite the redundancy,
         # this is the best place to set this. CommonShare.set_numsegs will
         # ignore duplicate calls.
+        assert self._node.num_segments is not None
         cs = self._commonshare
         cs.set_numsegs(self._node.num_segments)
 
@@ -579,6 +580,8 @@ class Share:
         # 'gotta_gotta_have_it' is for data that we absolutely need,
         # independent of whether we're still guessing about the offset table:
         # the version number and the offset table itself.
+        #
+        # Mr. Popeil, I'm in trouble, need your assistance on the double. Aww..
 
         desire = Spans(), Spans(), Spans()
         (want_it, need_it, gotta_gotta_have_it) = desire
@@ -708,7 +711,7 @@ class Share:
         need_it.add(blockstart, blocklen)
 
     def _send_requests(self, desired):
-        ask = desired - self._pending
+        ask = desired - self._pending - self._received.get_spans()
         log.msg("%s._send_requests, desired=%s, pending=%s, ask=%s" %
                 (repr(self), desired.dump(), self._pending.dump(), ask.dump()),
                 level=log.NOISY, parent=self._lp, umid="E94CVA")

commit 717f2f2e2d8131d36d28144570cbe9d31b43df6d
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:06:57 2010 -0700

    test_immutable: wrap to 80cols. No code changes.
---
 src/allmydata/test/test_immutable.py |   44 ++++++++++++++++++++--------------
 1 files changed, 26 insertions(+), 18 deletions(-)

diff --git a/src/allmydata/test/test_immutable.py b/src/allmydata/test/test_immutable.py
index dfc8356..63fb7ca 100644
--- a/src/allmydata/test/test_immutable.py
+++ b/src/allmydata/test/test_immutable.py
@@ -18,8 +18,9 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
             return res
         d.addCallback(_stash_it)
 
-        # The following process of deleting 8 of the shares and asserting that you can't
-        # download it is more to test this test code than to test the Tahoe code...
+        # The following process of deleting 8 of the shares and asserting
+        # that you can't download it is more to test this test code than to
+        # test the Tahoe code...
         def _then_delete_8(unused=None):
             self.replace_shares(stash[0], storage_index=self.uri.get_storage_index())
             for i in range(8):
@@ -42,21 +43,23 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
         return d
 
     def test_download(self):
-        """ Basic download.  (This functionality is more or less already tested by test code in
-        other modules, but this module is also going to test some more specific things about
-        immutable download.)
+        """ Basic download. (This functionality is more or less already
+        tested by test code in other modules, but this module is also going
+        to test some more specific things about immutable download.)
         """
         d = defer.succeed(None)
         before_download_reads = self._count_reads()
         def _after_download(unused=None):
             after_download_reads = self._count_reads()
-            self.failIf(after_download_reads-before_download_reads > 27, (after_download_reads, before_download_reads))
+            self.failIf(after_download_reads-before_download_reads > 27,
+                        (after_download_reads, before_download_reads))
         d.addCallback(self._download_and_check_plaintext)
         d.addCallback(_after_download)
         return d
 
     def test_download_from_only_3_remaining_shares(self):
-        """ Test download after 7 random shares (of the 10) have been removed. """
+        """ Test download after 7 random shares (of the 10) have been
+        removed."""
         d = defer.succeed(None)
         def _then_delete_7(unused=None):
             for i in range(7):
@@ -71,7 +74,8 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
         return d
 
     def test_download_from_only_3_shares_with_good_crypttext_hash(self):
-        """ Test download after 7 random shares (of the 10) have had their crypttext hash tree corrupted. """
+        """ Test download after 7 random shares (of the 10) have had their
+        crypttext hash tree corrupted."""
         d = defer.succeed(None)
         def _then_corrupt_7(unused=None):
             shnums = range(10)
@@ -95,9 +99,10 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
         return d
 
     def test_download_abort_if_too_many_corrupted_shares(self):
-        """ Test that download gives up quickly when it realizes there aren't enough uncorrupted
-        shares out there. It should be able to tell because the corruption occurs in the
-        sharedata version number, which it checks first."""
+        """Test that download gives up quickly when it realizes there aren't
+        enough uncorrupted shares out there. It should be able to tell
+        because the corruption occurs in the sharedata version number, which
+        it checks first."""
         d = defer.succeed(None)
         def _then_corrupt_8(unused=None):
             shnums = range(10)
@@ -121,17 +126,20 @@ class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase)
 
         def _after_attempt(unused=None):
             after_download_reads = self._count_reads()
-            # To pass this test, you are required to give up before reading all of the share
-            # data.  Actually, we could give up sooner than 45 reads, but currently our download
-            # code does 45 reads.  This test then serves as a "performance regression detector"
-            # -- if you change download code so that it takes *more* reads, then this test will
-            # fail.
-            self.failIf(after_download_reads-before_download_reads > 45, (after_download_reads, before_download_reads))
+            # To pass this test, you are required to give up before reading
+            # all of the share data. Actually, we could give up sooner than
+            # 45 reads, but currently our download code does 45 reads. This
+            # test then serves as a "performance regression detector" -- if
+            # you change download code so that it takes *more* reads, then
+            # this test will fail.
+            self.failIf(after_download_reads-before_download_reads > 45,
+                        (after_download_reads, before_download_reads))
         d.addCallback(_after_attempt)
         return d
 
 
-# XXX extend these tests to show bad behavior of various kinds from servers: raising exception from each remove_foo() method, for example
+# XXX extend these tests to show bad behavior of various kinds from servers:
+# raising exception from each remove_foo() method, for example
 
 # XXX test disconnect DeadReferenceError from get_buckets and get_block_whatsit
 

commit 9625bf14f3c84f43f6e0a2f5773ca8f3016d314b
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 18:05:59 2010 -0700

    test_immutable: remove number-of-reads assertion from
    test_download_abort_if_too_many_corrupted_shares, since the new downloader
    pipelines reads to speed up the overall download.
---
 src/allmydata/test/test_immutable.py |   37 ++++++++-------------------------
 1 files changed, 9 insertions(+), 28 deletions(-)

diff --git a/src/allmydata/test/test_immutable.py b/src/allmydata/test/test_immutable.py
index a430db2..dfc8356 100644
--- a/src/allmydata/test/test_immutable.py
+++ b/src/allmydata/test/test_immutable.py
@@ -5,7 +5,7 @@ from twisted.internet import defer
 from twisted.trial import unittest
 import random
 
-class Test(common.ShareManglingMixin, unittest.TestCase):
+class Test(common.ShareManglingMixin, common.ShouldFailMixin, unittest.TestCase):
     def test_test_code(self):
         # The following process of stashing the shares, running
         # replace_shares, and asserting that the new set of shares equals the
@@ -84,33 +84,14 @@ class Test(common.ShareManglingMixin, unittest.TestCase):
         return d
 
     def test_download_abort_if_too_many_missing_shares(self):
-        """ Test that download gives up quickly when it realizes there aren't enough shares out
-        there."""
-        d = defer.succeed(None)
-        def _then_delete_8(unused=None):
-            for i in range(8):
-                self._delete_a_share()
-        d.addCallback(_then_delete_8)
-
-        before_download_reads = self._count_reads()
-        def _attempt_to_download(unused=None):
-            d2 = download_to_data(self.n)
-
-            def _callb(res):
-                self.fail("Should have gotten an error from attempt to download, not %r" % (res,))
-            def _errb(f):
-                self.failUnless(f.check(NotEnoughSharesError))
-            d2.addCallbacks(_callb, _errb)
-            return d2
-
-        d.addCallback(_attempt_to_download)
-
-        def _after_attempt(unused=None):
-            after_download_reads = self._count_reads()
-            # To pass this test, you are required to give up before actually trying to read any
-            # share data.
-            self.failIf(after_download_reads-before_download_reads > 0, (after_download_reads, before_download_reads))
-        d.addCallback(_after_attempt)
+        """ Test that download gives up quickly when it realizes there aren't
+        enough shares out there."""
+        for i in range(8):
+            self._delete_a_share()
+        d = self.shouldFail(NotEnoughSharesError, "delete 8", None,
+                            download_to_data, self.n)
+        # the new downloader pipelines a bunch of read requests in parallel,
+        # so don't bother asserting anything about the number of reads
         return d
 
     def test_download_abort_if_too_many_corrupted_shares(self):

commit ae7ec2b4686fe6fcd729e592e5c63bb013d4bfb1
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 09:52:32 2010 -0700

    test_web: make it work
---
 src/allmydata/test/test_web.py |   11 +++++++----
 1 files changed, 7 insertions(+), 4 deletions(-)

diff --git a/src/allmydata/test/test_web.py b/src/allmydata/test/test_web.py
index 28b2b1f..679fb14 100644
--- a/src/allmydata/test/test_web.py
+++ b/src/allmydata/test/test_web.py
@@ -4169,7 +4169,7 @@ class Grid(GridTestMixin, WebErrorMixin, unittest.TestCase, ShouldFailMixin):
                    "no servers were connected, but it might also indicate "
                    "severe corruption. You should perform a filecheck on "
                    "this object to learn more. The full error message is: "
-                   "Failed to get enough shareholders: have 0, need 3")
+                   "no shares (need 3). Last failure: None")
             self.failUnlessEqual(exp, body)
         d.addCallback(_check_zero_shares)
 
@@ -4181,13 +4181,16 @@ class Grid(GridTestMixin, WebErrorMixin, unittest.TestCase, ShouldFailMixin):
         def _check_one_share(body):
             self.failIf("<html>" in body, body)
             body = " ".join(body.strip().split())
-            exp = ("NotEnoughSharesError: This indicates that some "
+            msg = ("NotEnoughSharesError: This indicates that some "
                    "servers were unavailable, or that shares have been "
                    "lost to server departure, hard drive failure, or disk "
                    "corruption. You should perform a filecheck on "
                    "this object to learn more. The full error message is:"
-                   " Failed to get enough shareholders: have 1, need 3")
-            self.failUnlessEqual(exp, body)
+                   " ran out of shares: %d complete, %d pending, 0 overdue,"
+                   " 0 unused, need 3. Last failure: None")
+            msg1 = msg % (1, 0)
+            msg2 = msg % (0, 1)
+            self.failUnless(body == msg1 or body == msg2, body)
         d.addCallback(_check_one_share)
 
         d.addCallback(lambda ignored:

commit f3587c9c31c4bb6cdb4a99443c279d0aa42b7ef5
Author: Brian Warner <warner@lothar.com>
Date:   Thu May 13 09:49:28 2010 -0700

    re-enable test_system's checking of downloadstatus, don't make web/status
    display event tables for LITs
---
 src/allmydata/test/test_system.py |    2 +-
 src/allmydata/web/status.py       |    2 ++
 2 files changed, 3 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/test/test_system.py b/src/allmydata/test/test_system.py
index 560ed6e..3c0f143 100644
--- a/src/allmydata/test/test_system.py
+++ b/src/allmydata/test/test_system.py
@@ -1176,7 +1176,7 @@ class SystemTest(SystemTestMixin, unittest.TestCase):
         d.addCallback(_got_status)
         def _got_up(res):
             return self.GET("status/down-%d" % self._down_status)
-        #d.addCallback(_got_up)
+        d.addCallback(_got_up)
         def _got_down(res):
             return self.GET("status/mapupdate-%d" % self._update_status)
         d.addCallback(_got_down)
diff --git a/src/allmydata/web/status.py b/src/allmydata/web/status.py
index 6b8c45c..c3a55d7 100644
--- a/src/allmydata/web/status.py
+++ b/src/allmydata/web/status.py
@@ -397,6 +397,8 @@ class DownloadStatusPage(DownloadResultsRendererMixin, rend.Page):
         return simplejson.dumps(data, indent=1) + "\n"
 
     def render_events(self, ctx, data):
+        if not self.download_status.storage_index:
+            return
         srt = self.short_relative_time
         l = T.ul()
 

commit 89001e7b54c7546a86be7a2d2f6ad991a5dac46f
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 12 10:33:55 2010 -0700

    simplify retry-when-segnum-is-wrong handling: instead of switching on
    were-we-guessing afterwards, conditionally addErrback the retry code up
    front. Removes some code and removes some tests of that code.
---
 src/allmydata/immutable/download2.py |   53 +++++++-----------
 src/allmydata/test/test_download.py  |   96 +---------------------------------
 2 files changed, 22 insertions(+), 127 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index f510bbb..b7eecc0 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -34,7 +34,7 @@ from repairer import Repairer
 KiB = 1024
 class BadSegmentNumberError(Exception):
     pass
-class BadSegmentError(Exception):
+class WrongSegmentError(Exception):
     pass
 class BadCiphertextHashError(Exception):
     pass
@@ -1323,9 +1323,10 @@ class Segmentation:
         d,c = n.get_segment(wanted_segnum, self._lp)
         self._cancel_segment_request = c
         d.addBoth(self._request_retired)
-        d.addCallback(self._got_segment, have_actual_segment_size,
-                      wanted_segnum)
-        d.addErrback(self._retry_bad_segment, have_actual_segment_size)
+        d.addCallback(self._got_segment, wanted_segnum)
+        if not have_actual_segment_size:
+            # we can retry once
+            d.addErrback(self._retry_bad_segment)
         d.addErrback(self._error)
 
     def _request_retired(self, res):
@@ -1333,8 +1334,7 @@ class Segmentation:
         self._cancel_segment_request = None
         return res
 
-    def _got_segment(self, (segment_start,segment,decodetime),
-                     had_actual_segment_size, wanted_segnum):
+    def _got_segment(self, (segment_start,segment,decodetime), wanted_segnum):
         self._cancel_segment_request = None
         # we got file[segment_start:segment_start+len(segment)]
         # we want file[self._offset:self._offset+self._size]
@@ -1350,23 +1350,15 @@ class Segmentation:
         # the overlap is file[o[0]:o[0]+o[1]]
         if not o or o[0] != self._offset:
             # we didn't get the first byte, so we can't use this segment
-            if had_actual_segment_size:
-                # and we should have gotten it right. This is big problem.
-                log.msg("Segmentation handed wrong data (but we knew better):"
-                        " want [%d-%d), given [%d-%d), for segnum=%d,"
-                        " for si=%s"
-                        % (self._offset, self._offset+self._size,
-                           segment_start, segment_start+len(segment),
-                           wanted_segnum, self._node._si_prefix),
-                        level=log.WEIRD, parent=self._lp, umid="STlIiA")
-                raise BadSegmentError("Despite knowing the segment size,"
-                                      " I was given the wrong data."
-                                      " I cannot cope.")
-            # we've wasted some bandwidth, but now we can grab the right one,
-            # because we should know the segsize by now.
-            assert self._node.segment_size is not None
-            self._maybe_fetch_next()
-            return
+            log.msg("Segmentation handed wrong data:"
+                    " want [%d-%d), given [%d-%d), for segnum=%d,"
+                    " for si=%s"
+                    % (self._offset, self._offset+self._size,
+                       segment_start, segment_start+len(segment),
+                       wanted_segnum, self._node._si_prefix),
+                    level=log.UNUSUAL, parent=self._lp, umid="STlIiA")
+            # we may retry if the segnum we asked was based on a guess
+            raise WrongSegmentError("I was given the wrong data.")
         offset_in_segment = self._offset - segment_start
         desired_data = segment[offset_in_segment:offset_in_segment+o[1]]
 
@@ -1378,15 +1370,12 @@ class Segmentation:
         self._read_ev.update(len(desired_data), 0, 0)
         self._maybe_fetch_next()
 
-    def _retry_bad_segment(self, f, had_actual_segment_size):
-        f.trap(BadSegmentNumberError) # guessed way wrong, off the end
-        if had_actual_segment_size:
-            # but we should have known better, so this is a real error. This
-            # indicates a code bug.
-            log.msg("Segmentation retried and failed with wrong segnum",
-                    level=log.WEIRD, parent=self._lp, umid="6Hd0ZA")
-            return f
-        # we didn't know better: try again with more information
+    def _retry_bad_segment(self, f):
+        f.trap(WrongSegmentError, BadSegmentNumberError)
+        # we guessed the segnum wrong: either one that doesn't overlap with
+        # the start of our desired region, or one that's beyond the end of
+        # the world. Now that we have the right information, we're allowed to
+        # retry once.
         assert self._node.segment_size is not None
         return self._maybe_fetch_next()
 
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index 79b3cbc..c44e08a 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -15,7 +15,7 @@ from allmydata.test.no_network import GridTestMixin
 from allmydata.test.common import ShouldFailMixin
 from allmydata.interfaces import NotEnoughSharesError, NoSharesError
 from allmydata.immutable.download2 import BadSegmentNumberError, \
-     BadSegmentError, BadCiphertextHashError
+     WrongSegmentError, BadCiphertextHashError
 from allmydata.codec import CRSDecoder
 from foolscap.eventual import fireEventually, flushEventualQueue
 
@@ -480,100 +480,6 @@ class DownloadTest(_Base, unittest.TestCase):
                             _try_download)
         return d
 
-    def test_download_bad_segment_retry(self):
-        self.basedir = self.mktemp()
-        self.set_up_grid()
-        self.c0 = self.g.clients[0]
-        self.load_shares()
-        # damage the Node so that it always gets the wrong segment number
-        n = self.c0.create_node_from_uri(immutable_uri)
-        d = download_to_data(n)
-        def _got_data(data):
-            self.failUnlessEqual(data, plaintext)
-        d.addCallback(_got_data)
-        def _cause_damage(data):
-            # we leave n.num_segments alone, but break n_segment_size
-            log.msg("test_download_bad_segment_retry causing damage")
-            n._cnode._node.segment_size = 72 # 5 segs
-        d.addCallback(_cause_damage)
-        def _try_download():
-            con = MemoryConsumer()
-            d = n.read(con, 150, 20) # seg2
-            return d
-        d.addCallback(lambda ign:
-                      self.shouldFail(BadSegmentNumberError, "badseg",
-                                      "segnum=2, numsegs=1",
-                                      _try_download))
-        return d
-
-    def test_download_bad_segment_retry2(self):
-        self.basedir = self.mktemp()
-        self.set_up_grid()
-        self.c0 = self.g.clients[0]
-
-        # damage the Node so that it always gets a wrong (but still valid)
-        # segment number. We upload a file with 6 segments, then force a
-        # segsize which causes our attempt to get seg1 to actually fetch
-        # something else.
-
-        u = upload.Data(plaintext, None)
-        u.max_segment_size = 60 # 6 segs
-        d = self.c0.upload(u)
-        def _uploaded(ur):
-            n = self.c0.create_node_from_uri(ur.uri)
-            n._cnode._node._build_guessed_tables(u.max_segment_size)
-            d = download_to_data(n)
-            def _got_data(data):
-                self.failUnlessEqual(data, plaintext)
-            d.addCallback(_got_data)
-            def _cause_damage1(data):
-                # we leave n.num_segments alone, but break n_segment_size
-                log.msg("test_download_bad_segment_retry2 causing damage")
-                n._cnode._node.segment_size = 90
-                # start the download, so Segmentation 1: gets the wrong size
-                # and 2: thinks it has the right size. But then fix the
-                # segsize before any blocks actually show up (since leaving
-                # segment_size broken would also break FEC decode).
-                con = MemoryConsumer()
-                # file[60:70] really lives in seg1[0:10]. But if we think the
-                # segsize is 90, we'll think it lives in seg0[60:70], so
-                # we'll ask for that, get seg0 (i.e. file[0:60]), notice the
-                # incomplete overlap, and hit the retry code. Because we
-                # tricked Segmentation into thinking that it wasn't guessing
-                # about the segsize, it will trigger the BadSegmentError
-                # failsafe on the first pass.
-                d = n.read(con, 60, 10)
-                n._cnode._node.segment_size = u.max_segment_size
-                def _continue():
-                    return d
-                return self.shouldFail(BadSegmentError, "badseg1",
-                                       "I cannot cope",
-                                       _continue)
-            d.addCallback(_cause_damage1)
-            def _cause_damage2(data):
-                # same thing, but set a segment_size which causes partial
-                # overlap instead of zero overlap
-                log.msg("test_download_bad_segment_retry2 causing damage2")
-                n._cnode._node.segment_size = 57
-                con = MemoryConsumer()
-                # file[57:67] really lives in seg0[57:60]+seg1[0:3]. But if
-                # we think the segsize is 57, we'll think it lives in
-                # seg1[0:10], so we'll ask for that, get seg1 (i.e.
-                # file[60:120]), and the overlap will start 3 bytes into the
-                # data we want. Since we don't yet have the first byte of the
-                # range, we'll hit the retry code.
-                d = n.read(con, 57, 10)
-                n._cnode._node.segment_size = u.max_segment_size
-                def _continue():
-                    return d
-                return self.shouldFail(BadSegmentError, "badseg2",
-                                       "I cannot cope",
-                                       _continue)
-            d.addCallback(_cause_damage2)
-            return d
-        d.addCallback(_uploaded)
-        return d
-
     def test_download_segment_terminate(self):
         self.basedir = self.mktemp()
         self.set_up_grid()

commit e471a6e02dfa7ee944e3db562de6ecbbb347cb36
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 12 08:50:42 2010 -0700

    add download-status data (event timestamps), basic web renderer
---
 src/allmydata/immutable/download2.py    |  292 +++++++++++++++++++++++++++----
 src/allmydata/test/test_download.py     |    2 +-
 src/allmydata/web/download-status.xhtml |    1 +
 src/allmydata/web/status.py             |  141 +++++++++++++++-
 src/allmydata/web/tahoe.css             |   12 ++-
 5 files changed, 412 insertions(+), 36 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 6e561c7..f510bbb 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -2,6 +2,9 @@
 import binascii
 import struct
 import copy
+import time
+import itertools
+now = time.time
 from zope.interface import implements
 from twisted.python.failure import Failure
 from twisted.internet import defer
@@ -9,7 +12,8 @@ from twisted.internet.interfaces import IPushProducer, IConsumer
 
 from foolscap.api import eventually
 from allmydata.interfaces import IImmutableFileNode, IUploadResults, \
-     NotEnoughSharesError, NoSharesError, HASH_SIZE, DEFAULT_MAX_SEGMENT_SIZE
+     IDownloadStatus, HASH_SIZE, DEFAULT_MAX_SEGMENT_SIZE, \
+     NotEnoughSharesError, NoSharesError
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 from allmydata.util import base32, log, hashutil, mathutil, idlib
@@ -51,7 +55,7 @@ class Share:
     # servers. A different backend would use a different class.
 
     def __init__(self, rref, server_version, verifycap, commonshare, node,
-                 peerid, shnum, logparent):
+                 download_status, peerid, shnum, logparent):
         self._rref = rref
         self._server_version = server_version
         self._node = node # holds share_hash_tree and UEB
@@ -63,6 +67,7 @@ class Share:
         self.actual_offsets = None
         self._UEB_length = None
         self._commonshare = commonshare # holds block_hash_tree
+        self._download_status = download_status
         self._peerid = peerid
         self._peerid_s = base32.b2a(peerid)[:5]
         self._storage_index = verifycap.storage_index
@@ -713,6 +718,7 @@ class Share:
         # all hashes arrive before the blocks, so the blocks can be consumed
         # and released in a single turn. I removed this for simplicity.
         # Reconsider the removal: maybe bring it back.
+        ds = self._download_status
 
         for (start, length) in ask:
             # TODO: quantize to reasonably-large blocks
@@ -722,9 +728,11 @@ class Share:
                          share=repr(self),
                          start=start, length=length,
                          level=log.NOISY, parent=self._lp, umid="sgVAyA")
+            req_ev = ds.add_request_sent(self._peerid, self._shnum,
+                                         start, length, now())
             d = self._send_request(start, length)
-            d.addCallback(self._got_data, start, length, lp)
-            d.addErrback(self._got_error, start, length, lp)
+            d.addCallback(self._got_data, start, length, req_ev, lp)
+            d.addErrback(self._got_error, start, length, req_ev, lp)
             d.addCallback(self._trigger_loop)
             d.addErrback(lambda f:
                          log.err(format="unhandled error during send_request",
@@ -734,7 +742,8 @@ class Share:
     def _send_request(self, start, length):
         return self._rref.callRemote("read", start, length)
 
-    def _got_data(self, data, start, length, lp):
+    def _got_data(self, data, start, length, req_ev, lp):
+        req_ev.finished(len(data), now())
         if not self._alive:
             return
         log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
@@ -776,7 +785,8 @@ class Share:
         # the wanted/needed span is only "wanted" for the first pass. Once
         # the offset table arrives, it's all "needed".
 
-    def _got_error(self, f, start, length, lp):
+    def _got_error(self, f, start, length, req_ev, lp):
+        req_ev.finished("error", now())
         log.msg(format="error requesting %(start)d+%(length)d"
                 " from %(server)s for si %(si)s",
                 start=start, length=length,
@@ -1017,7 +1027,7 @@ class SegmentFetcher:
         # can never get here, caller has assert in case of code bug
 
     def _send_new_request(self):
-        for shnum,shares in self._shnums.iteritems():
+        for shnum,shares in sorted(self._shnums.iteritems()):
             states = [self._shares[s] for s in shares]
             if COMPLETE in states or PENDING in states:
                 # don't send redundant requests
@@ -1081,8 +1091,8 @@ class RequestToken:
         self.peerid = peerid
 
 class ShareFinder:
-    def __init__(self, storage_broker, verifycap, node, logparent=None,
-                 max_outstanding_requests=10):
+    def __init__(self, storage_broker, verifycap, node, download_status,
+                 logparent=None, max_outstanding_requests=10):
         self.running = True # stopped by Share.stop, from Terminator
         self.verifycap = verifycap
         self._started = False
@@ -1099,6 +1109,7 @@ class ShareFinder:
         self._storage_index = verifycap.storage_index
         self._si_prefix = base32.b2a_l(self._storage_index[:8], 60)
         self._node_logparent = logparent
+        self._download_status = download_status
         self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
                            si=self._si_prefix,
                            level=log.NOISY, parent=logparent, umid="2xjj2A")
@@ -1189,18 +1200,21 @@ class ShareFinder:
         lp = self.log(format="sending DYHB to [%(peerid)s]",
                       peerid=idlib.shortnodeid_b2a(peerid),
                       level=log.NOISY, umid="Io7pyg")
+        d_ev = self._download_status.add_dyhb_sent(peerid, now())
         d = rref.callRemote("get_buckets", self._storage_index)
         d.addBoth(incidentally, self.pending_requests.discard, req)
         d.addCallbacks(self._got_response, self._got_error,
-                       callbackArgs=(rref.version, peerid, req, lp),
-                       errbackArgs=(peerid, req, lp))
+                       callbackArgs=(rref.version, peerid, req, d_ev, lp),
+                       errbackArgs=(peerid, req, d_ev, lp))
         d.addErrback(log.err, format="error in send_request",
                      level=log.WEIRD, parent=lp, umid="rpdV0w")
         d.addCallback(incidentally, eventually, self.loop)
 
-    def _got_response(self, buckets, server_version, peerid, req, lp):
+    def _got_response(self, buckets, server_version, peerid, req, d_ev, lp):
+        shnums = sorted([shnum for shnum in buckets])
+        d_ev.finished(shnums, now())
         if buckets:
-            shnums_s = ",".join([str(shnum) for shnum in buckets])
+            shnums_s = ",".join([str(shnum) for shnum in shnums])
             self.log(format="got shnums [%(shnums)s] from [%(peerid)s]",
                      shnums=shnums_s, peerid=idlib.shortnodeid_b2a(peerid),
                      level=log.NOISY, parent=lp, umid="0fcEZw")
@@ -1233,10 +1247,12 @@ class ShareFinder:
                 #     Yuck.
                 self._commonshares[shnum] = cs
             s = Share(bucket, server_version, self.verifycap, cs, self.node,
-                      peerid, shnum, self._node_logparent)
+                      self._download_status, peerid, shnum,
+                      self._node_logparent)
             self.undelivered_shares.append(s)
 
-    def _got_error(self, f, peerid, req, lp):
+    def _got_error(self, f, peerid, req, d_ev, lp):
+        d_ev.finished("error", now())
         self.log(format="got error from [%(peerid)s]",
                  peerid=idlib.shortnodeid_b2a(peerid), failure=f,
                  level=log.UNUSUAL, parent=lp, umid="zUKdCw")
@@ -1251,7 +1267,7 @@ class Segmentation:
     request one segment at a time.
     """
     implements(IPushProducer)
-    def __init__(self, node, offset, size, consumer, logparent=None):
+    def __init__(self, node, offset, size, consumer, read_ev, logparent=None):
         self._node = node
         self._hungry = True
         self._active_segnum = None
@@ -1262,6 +1278,8 @@ class Segmentation:
         self._size = size
         assert offset+size <= node._verifycap.size
         self._consumer = consumer
+        self._read_ev = read_ev
+        self._start_pause = None
         self._lp = logparent
 
     def start(self):
@@ -1315,8 +1333,8 @@ class Segmentation:
         self._cancel_segment_request = None
         return res
 
-    def _got_segment(self, (segment_start,segment), had_actual_segment_size,
-                     wanted_segnum):
+    def _got_segment(self, (segment_start,segment,decodetime),
+                     had_actual_segment_size, wanted_segnum):
         self._cancel_segment_request = None
         # we got file[segment_start:segment_start+len(segment)]
         # we want file[self._offset:self._offset+self._size]
@@ -1357,6 +1375,7 @@ class Segmentation:
         self._consumer.write(desired_data)
         # the consumer might call our .pauseProducing() inside that write()
         # call, setting self._hungry=False
+        self._read_ev.update(len(desired_data), 0, 0)
         self._maybe_fetch_next()
 
     def _retry_bad_segment(self, f, had_actual_segment_size):
@@ -1388,9 +1407,14 @@ class Segmentation:
             self._cancel_segment_request = None
     def pauseProducing(self):
         self._hungry = False
+        self._start_pause = now()
     def resumeProducing(self):
         self._hungry = True
         eventually(self._maybe_fetch_next)
+        if self._start_pause is not None:
+            paused = now() - self._start_pause
+            self._read_ev.update(0, 0, paused)
+            self._start_pause = None
 
 class Cancel:
     def __init__(self, f):
@@ -1407,7 +1431,7 @@ class _Node:
 
     # Share._node points to me
     def __init__(self, verifycap, storage_broker, secret_holder,
-                 terminator, history):
+                 terminator, history, download_status):
         assert isinstance(verifycap, uri.CHKFileVerifierURI)
         self._verifycap = verifycap
         self._storage_broker = storage_broker
@@ -1423,6 +1447,7 @@ class _Node:
         # stopService()+flushEventualQueue() fires, everything will be done.
         self._secret_holder = secret_holder
         self._history = history
+        self._download_status = download_status
 
         k, N = self._verifycap.needed_shares, self._verifycap.total_shares
         self.share_hash_tree = IncompleteHashTree(N)
@@ -1463,7 +1488,8 @@ class _Node:
                      level=log.OPERATIONAL, umid="uJ0zAQ")
         self._lp = lp
 
-        self._sharefinder = ShareFinder(storage_broker, verifycap, self, lp)
+        self._sharefinder = ShareFinder(storage_broker, verifycap, self,
+                                        self._download_status, lp)
         self._shares = set()
 
     def _build_guessed_tables(self, max_segment_size):
@@ -1489,7 +1515,7 @@ class _Node:
     # things called by outside callers, via CiphertextFileNode. get_segment()
     # may also be called by Segmentation.
 
-    def read(self, consumer, offset=0, size=None):
+    def read(self, consumer, offset=0, size=None, read_ev=None):
         """I am the main entry point, from which FileNode.read() can get
         data. I feed the consumer with the desired range of ciphertext. I
         return a Deferred that fires (with the consumer) when the read is
@@ -1502,6 +1528,9 @@ class _Node:
             size = self._verifycap.size
         # clip size so offset+size does not go past EOF
         size = min(size, self._verifycap.size-offset)
+        if read_ev is None:
+            read_ev = self._download_status.add_read_event(offset, size, now())
+
         lp = log.msg(format="imm Node(%(si)s).read(%(offset)d, %(size)d)",
                      si=base32.b2a(self._verifycap.storage_index)[:8],
                      offset=offset, size=size,
@@ -1509,7 +1538,7 @@ class _Node:
         sp = self._history.stats_provider
         sp.count("downloader.files_downloaded", 1) # really read() calls
         sp.count("downloader.bytes_downloaded", size)
-        s = Segmentation(self, offset, size, consumer, lp)
+        s = Segmentation(self, offset, size, consumer, read_ev, lp)
         # this raises an interesting question: what segments to fetch? if
         # offset=0, always fetch the first segment, and then allow
         # Segmentation to be responsible for pulling the subsequent ones if
@@ -1519,6 +1548,10 @@ class _Node:
         # offset-table-guessing code (which starts by guessing the segsize)
         # to assist the offset>0 process.
         d = s.start()
+        def _done(res):
+            read_ev.finished(now())
+            return res
+        d.addBoth(_done)
         return d
 
     def get_segment(self, segnum, logparent=None):
@@ -1543,6 +1576,7 @@ class _Node:
                 si=base32.b2a(self._verifycap.storage_index)[:8],
                 segnum=segnum,
                 level=log.OPERATIONAL, parent=logparent, umid="UKFjDQ")
+        self._download_status.add_segment_request(segnum, now())
         d = defer.Deferred()
         c = Cancel(self._cancel_request)
         self._segment_requests.append( (segnum, d, c) )
@@ -1715,6 +1749,13 @@ class _Node:
         d = defer.maybeDeferred(self._decode_blocks, segnum, blocks)
         d.addCallback(self._check_ciphertext_hash, segnum)
         def _deliver(result):
+            ds = self._download_status
+            if isinstance(result, Failure):
+                ds.add_segment_error(segnum, now())
+            else:
+                (offset, segment, decodetime) = result
+                ds.add_segment_delivery(segnum, now(),
+                                        offset, len(segment), decodetime)
             log.msg(format="delivering segment(%(segnum)d)",
                     segnum=segnum,
                     level=log.OPERATIONAL, parent=self._lp,
@@ -1750,19 +1791,21 @@ class _Node:
             shares.append(share)
         del blocks
 
+        start = now()
         d = codec.decode(shares, shareids)   # segment
         del shares
         def _process(buffers):
+            decodetime = now() - start
             segment = "".join(buffers)
             assert len(segment) == decoded_size
             del buffers
             if tail:
                 segment = segment[:self.tail_segment_size]
-            return segment
+            return (segment, decodetime)
         d.addCallback(_process)
         return d
 
-    def _check_ciphertext_hash(self, segment, segnum):
+    def _check_ciphertext_hash(self, (segment, decodetime), segnum):
         assert self._active_segment.segnum == segnum
         assert self.segment_size is not None
         offset = segnum * self.segment_size
@@ -1770,7 +1813,7 @@ class _Node:
         h = hashutil.crypttext_segment_hash(segment)
         try:
             self.ciphertext_hash_tree.set_hashes(leaves={segnum: h})
-            return (offset, segment)
+            return (offset, segment, decodetime)
         except (BadHashError, NotEnoughHashesError):
             format = ("hash failure in ciphertext_hash_tree:"
                       " segnum=%(segnum)d, SI=%(si)s")
@@ -1879,17 +1922,21 @@ class _Node:
 
 class CiphertextFileNode:
     def __init__(self, verifycap, storage_broker, secret_holder,
-                 terminator, history):
+                 terminator, history, download_status=None):
         assert isinstance(verifycap, uri.CHKFileVerifierURI)
+        if download_status is None:
+            ds = DownloadStatus(verifycap.storage_index, verifycap.size)
+            history.add_download(ds)
+            download_status = ds
         self._node = _Node(verifycap, storage_broker, secret_holder,
-                           terminator, history)
+                           terminator, history, download_status)
 
-    def read(self, consumer, offset=0, size=None):
+    def read(self, consumer, offset=0, size=None, read_ev=None):
         """I am the main entry point, from which FileNode.read() can get
         data. I feed the consumer with the desired range of ciphertext. I
         return a Deferred that fires (with the consumer) when the read is
         finished."""
-        return self._node.read(consumer, offset, size)
+        return self._node.read(consumer, offset, size, read_ev)
 
     def get_segment(self, segnum):
         """Begin downloading a segment. I return a tuple (d, c): 'd' is a
@@ -1925,8 +1972,9 @@ class DecryptingConsumer:
     real consumer."""
     implements(IConsumer)
 
-    def __init__(self, consumer, readkey, offset):
+    def __init__(self, consumer, readkey, offset, read_event):
         self._consumer = consumer
+        self._read_event = read_event
         # TODO: pycryptopp CTR-mode needs random-access operations: I want
         # either a=AES(readkey, offset) or better yet both of:
         #  a=AES(readkey, offset=0)
@@ -1947,7 +1995,10 @@ class DecryptingConsumer:
     def unregisterProducer(self):
         self._consumer.unregisterProducer()
     def write(self, ciphertext):
+        started = now()
         plaintext = self._decryptor.process(ciphertext)
+        elapsed = now() - started
+        self._read_event.update(0, elapsed, 0)
         self._consumer.write(plaintext)
 
 class ImmutableFileNode:
@@ -1958,15 +2009,24 @@ class ImmutableFileNode:
                  history):
         assert isinstance(filecap, uri.CHKFileURI)
         verifycap = filecap.get_verify_cap()
+        ds = DownloadStatus(verifycap.storage_index, verifycap.size)
+        history.add_download(ds)
+        self._download_status = ds
         self._cnode = CiphertextFileNode(verifycap, storage_broker,
-                                         secret_holder, terminator, history)
+                                         secret_holder, terminator, history, ds)
         assert isinstance(filecap, uri.CHKFileURI)
         self.u = filecap
         self._readkey = filecap.key
 
     def read(self, consumer, offset=0, size=None):
-        decryptor = DecryptingConsumer(consumer, self._readkey, offset)
-        d = self._cnode.read(decryptor, offset, size)
+        actual_size = size
+        if actual_size == None:
+            actual_size = self.u.size
+        actual_size = actual_size - offset
+        read_ev = self._download_status.add_read_event(offset,actual_size,
+                                                       now())
+        decryptor = DecryptingConsumer(consumer, self._readkey, offset, read_ev)
+        d = self._cnode.read(decryptor, offset, size, read_ev)
         d.addCallback(lambda dc: consumer)
         return d
 
@@ -2077,3 +2137,169 @@ class ImmutableFileNode:
 # tests to write:
 # * truncated share, so _satisfy_* doesn't get all it wants
 # * slow server
+
+class RequestEvent:
+    def __init__(self, download_status, tag):
+        self._download_status = download_status
+        self._tag = tag
+    def finished(self, received, when):
+        self._download_status.add_request_finished(self._tag, received, when)
+
+class DYHBEvent:
+    def __init__(self, download_status, tag):
+        self._download_status = download_status
+        self._tag = tag
+    def finished(self, shnums, when):
+        self._download_status.add_dyhb_finished(self._tag, shnums, when)
+
+class ReadEvent:
+    def __init__(self, download_status, tag):
+        self._download_status = download_status
+        self._tag = tag
+    def update(self, bytes, decrypttime, pausetime):
+        self._download_status.update_read_event(self._tag, bytes,
+                                                decrypttime, pausetime)
+    def finished(self, finishtime):
+        self._download_status.finish_read_event(self._tag, finishtime)
+
+class DownloadStatus:
+    # There is one DownloadStatus for each CiphertextFileNode. The status
+    # object will keep track of all activity for that node.
+    implements(IDownloadStatus)
+    statusid_counter = itertools.count(0)
+
+    def __init__(self, storage_index, size):
+        self.storage_index = storage_index
+        self.size = size
+        self.counter = self.statusid_counter.next()
+        self.helper = False
+        self.started = None
+        # self.dyhb_requests tracks "do you have a share" requests and
+        # responses. It maps serverid to a tuple of:
+        #  send time
+        #  tuple of response shnums (None if response hasn't arrived, "error")
+        #  response time (None if response hasn't arrived yet)
+        self.dyhb_requests = {}
+
+        # self.requests tracks share-data requests and responses. It maps
+        # serverid to a tuple of:
+        #  shnum,
+        #  start,length,  (of data requested)
+        #  send time
+        #  response length (None if reponse hasn't arrived yet, or "error")
+        #  response time (None if response hasn't arrived)
+        self.requests = {}
+
+        # self.segment_events tracks segment requests and delivery. It is a
+        # list of:
+        #  type ("request", "delivery", "error")
+        #  segment number
+        #  event time
+        #  segment start (file offset of first byte, None except in "delivery")
+        #  segment length (only in "delivery")
+        #  time spent in decode (only in "delivery")
+        self.segment_events = []
+
+        # self.read_events tracks read() requests. It is a list of:
+        #  start,length  (of data requested)
+        #  request time
+        #  finish time (None until finished)
+        #  bytes returned (starts at 0, grows as segments are delivered)
+        #  time spent in decrypt (None for ciphertext-only reads)
+        #  time spent paused
+        self.read_events = []
+
+        self.known_shares = [] # (serverid, shnum)
+        self.problems = []
+
+
+    def add_dyhb_sent(self, serverid, when):
+        r = (when, None, None)
+        if serverid not in self.dyhb_requests:
+            self.dyhb_requests[serverid] = []
+        self.dyhb_requests[serverid].append(r)
+        tag = (serverid, len(self.dyhb_requests[serverid])-1)
+        return DYHBEvent(self, tag)
+
+    def add_dyhb_finished(self, tag, shnums, when):
+        # received="error" on error, else tuple(shnums)
+        (serverid, index) = tag
+        r = self.dyhb_requests[serverid][index]
+        (sent, _, _) = r
+        r = (sent, shnums, when)
+        self.dyhb_requests[serverid][index] = r
+
+    def add_request_sent(self, serverid, shnum, start, length, when):
+        r = (shnum, start, length, when, None, None)
+        if serverid not in self.requests:
+            self.requests[serverid] = []
+        self.requests[serverid].append(r)
+        tag = (serverid, len(self.requests[serverid])-1)
+        return RequestEvent(self, tag)
+
+    def add_request_finished(self, tag, received, when):
+        # received="error" on error, else len(data)
+        (serverid, index) = tag
+        r = self.requests[serverid][index]
+        (shnum, start, length, sent, _, _) = r
+        r = (shnum, start, length, sent, received, when)
+        self.requests[serverid][index] = r
+
+    def add_segment_request(self, segnum, when):
+        if self.started is None:
+            self.started = when
+        r = ("request", segnum, when, None, None, None)
+        self.segment_events.append(r)
+    def add_segment_delivery(self, segnum, when, start, length, decodetime):
+        r = ("delivery", segnum, when, start, length, decodetime)
+        self.segment_events.append(r)
+    def add_segment_error(self, segnum, when):
+        r = ("error", segnum, when, None, None, None)
+        self.segment_events.append(r)
+
+    def add_read_event(self, start, length, when):
+        if self.started is None:
+            self.started = when
+        r = (start, length, when, None, 0, 0, 0)
+        self.read_events.append(r)
+        tag = len(self.read_events)-1
+        return ReadEvent(self, tag)
+    def update_read_event(self, tag, bytes_d, decrypt_d, paused_d):
+        r = self.read_events[tag]
+        (start, length, requesttime, finishtime, bytes, decrypt, paused) = r
+        bytes += bytes_d
+        decrypt += decrypt_d
+        paused += paused_d
+        r = (start, length, requesttime, finishtime, bytes, decrypt, paused)
+        self.read_events[tag] = r
+    def finish_read_event(self, tag, finishtime):
+        r = self.read_events[tag]
+        (start, length, requesttime, _, bytes, decrypt, paused) = r
+        r = (start, length, requesttime, finishtime, bytes, decrypt, paused)
+        self.read_events[tag] = r
+
+    def add_known_share(self, serverid, shnum):
+        self.known_shares.append( (serverid, shnum) )
+
+    def add_problem(self, p):
+        self.problems.append(p)
+
+    # IDownloadStatus methods
+    def get_counter(self):
+        return self.counter
+    def get_storage_index(self):
+        return self.storage_index
+    def get_size(self):
+        return self.size
+    def get_status(self):
+        return "not impl yet" # TODO
+    def get_progress(self):
+        return 0.1 # TODO
+    def using_helper(self):
+        return False
+    def get_active(self):
+        return False # TODO
+    def get_started(self):
+        return self.started
+    def get_results(self):
+        return None # TODO
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index cfdf935..79b3cbc 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -441,7 +441,7 @@ class DownloadTest(_Base, unittest.TestCase):
         n = self.c0.create_node_from_uri(immutable_uri)
         cn = n._cnode
         (d,c) = cn.get_segment(0)
-        def _got_segment((offset,data)):
+        def _got_segment((offset,data,decodetime)):
             self.failUnlessEqual(offset, 0)
             self.failUnlessEqual(len(data), len(plaintext))
         d.addCallback(_got_segment)
diff --git a/src/allmydata/web/download-status.xhtml b/src/allmydata/web/download-status.xhtml
index 77342ba..30abfca 100644
--- a/src/allmydata/web/download-status.xhtml
+++ b/src/allmydata/web/download-status.xhtml
@@ -18,6 +18,7 @@
   <li>Status: <span n:render="status"/></li>
 </ul>
 
+<div n:render="events"></div>
 
 <div n:render="results">
   <h2>Download Results</h2>
diff --git a/src/allmydata/web/status.py b/src/allmydata/web/status.py
index e4241a3..6b8c45c 100644
--- a/src/allmydata/web/status.py
+++ b/src/allmydata/web/status.py
@@ -358,6 +358,145 @@ class DownloadStatusPage(DownloadResultsRendererMixin, rend.Page):
     def download_results(self):
         return defer.maybeDeferred(self.download_status.get_results)
 
+    def relative_time(self, t):
+        if t is None:
+            return t
+        if self.download_status.started is not None:
+            return t - self.download_status.started
+        return t
+    def short_relative_time(self, t):
+        t = self.relative_time(t)
+        if t is None:
+            return ""
+        return "+%.6fs" % t
+
+    def renderHTTP(self, ctx):
+        req = inevow.IRequest(ctx)
+        t = get_arg(req, "t")
+        if t == "json":
+            return self.json(req)
+        return rend.Page.renderHTTP(self, ctx)
+
+    def json(self, req):
+        req.setHeader("content-type", "text/plain")
+        data = {}
+        dyhb_events = []
+        for serverid,requests in self.download_status.dyhb_requests.iteritems():
+            for req in requests:
+                dyhb_events.append( (base32.b2a(serverid),) + req )
+        dyhb_events.sort(key=lambda req: req[1])
+        data["dyhb"] = dyhb_events
+        request_events = []
+        for serverid,requests in self.download_status.requests.iteritems():
+            for req in requests:
+                request_events.append( (base32.b2a(serverid),) + req )
+        request_events.sort(key=lambda req: (req[4],req[1]))
+        data["requests"] = request_events
+        data["segment"] = self.download_status.segment_events
+        data["read"] = self.download_status.read_events
+        return simplejson.dumps(data, indent=1) + "\n"
+
+    def render_events(self, ctx, data):
+        srt = self.short_relative_time
+        l = T.ul()
+
+        t = T.table(class_="status-download-events")
+        t[T.tr[T.td["serverid"], T.td["sent"], T.td["received"],
+               T.td["shnums"], T.td["RTT"]]]
+        dyhb_events = []
+        for serverid,requests in self.download_status.dyhb_requests.iteritems():
+            for req in requests:
+                dyhb_events.append( (serverid,) + req )
+        dyhb_events.sort(key=lambda req: req[1])
+        for d_ev in dyhb_events:
+            (serverid, sent, shnums, received) = d_ev
+            serverid_s = idlib.shortnodeid_b2a(serverid)
+            rtt = received - sent
+            t[T.tr(style="background: %s" % self.color(serverid))[
+                [T.td[serverid_s], T.td[srt(sent)], T.td[srt(received)],
+                 T.td[",".join([str(shnum) for shnum in shnums])],
+                 T.td[self.render_time(None, rtt)],
+                 ]]]
+        l["DYHB Requests:", t]
+
+        t = T.table(class_="status-download-events")
+        t[T.tr[T.td["range"], T.td["start"], T.td["finish"], T.td["got"],
+               T.td["time"], T.td["decrypttime"], T.td["pausedtime"],
+               T.td["speed"]]]
+        for r_ev in self.download_status.read_events:
+            (start, length, requesttime, finishtime, bytes, decrypt, paused) = r_ev
+            print r_ev
+            if finishtime is not None:
+                rtt = finishtime - requesttime - paused
+                speed = self.render_rate(None, 1.0 * bytes / rtt)
+                rtt = self.render_time(None, rtt)
+                decrypt = self.render_time(None, decrypt)
+                paused = self.render_time(None, paused)
+            else:
+                speed, rtt, decrypt, paused = "","","",""
+            t[T.tr[T.td["[%d:+%d]" % (start, length)],
+                   T.td[srt(requesttime)], T.td[srt(finishtime)],
+                   T.td[bytes], T.td[rtt], T.td[decrypt], T.td[paused],
+                   T.td[speed],
+                   ]]
+        l["Read Events:", t]
+
+        t = T.table(class_="status-download-events")
+        t[T.tr[T.td["type"], T.td["segnum"], T.td["when"], T.td["range"],
+               T.td["decodetime"], T.td["segtime"], T.td["speed"]]]
+        reqtime = (None, None)
+        for s_ev in self.download_status.segment_events:
+            (etype, segnum, when, segstart, seglen, decodetime) = s_ev
+            if etype == "request":
+                t[T.tr[T.td["request"], T.td["seg%d" % segnum],
+                       T.td[srt(when)]]]
+                reqtime = (segnum, when)
+            elif etype == "delivery":
+                if reqtime[0] == segnum:
+                    segtime = when - reqtime[1]
+                    speed = self.render_rate(None, 1.0 * seglen / segtime)
+                    segtime = self.render_time(None, segtime)
+                else:
+                    segtime, speed = "", ""
+                t[T.tr[T.td["delivery"], T.td["seg%d" % segnum],
+                       T.td[srt(when)],
+                       T.td["[%d:+%d]" % (segstart, seglen)],
+                       T.td[self.render_time(None,decodetime)],
+                       T.td[segtime], T.td[speed]]]
+            elif etype == "error":
+                t[T.tr[T.td["error"], T.td["seg%d" % segnum]]]
+        l["Segment Events:", t]
+
+        t = T.table(border="1")
+        t[T.tr[T.td["serverid"], T.td["shnum"], T.td["range"],
+               T.td["txtime"], T.td["rxtime"], T.td["received"], T.td["RTT"]]]
+        reqtime = (None, None)
+        request_events = []
+        for serverid,requests in self.download_status.requests.iteritems():
+            for req in requests:
+                request_events.append( (serverid,) + req )
+        request_events.sort(key=lambda req: (req[4],req[1]))
+        for r_ev in request_events:
+            (peerid, shnum, start, length, sent, receivedlen, received) = r_ev
+            rtt = None
+            if received is not None:
+                rtt = received - sent
+            peerid_s = idlib.shortnodeid_b2a(peerid)
+            t[T.tr(style="background: %s" % self.color(peerid))[
+                T.td[peerid_s], T.td[shnum],
+                T.td["[%d:+%d]" % (start, length)],
+                T.td[srt(sent)], T.td[srt(received)], T.td[receivedlen],
+                T.td[self.render_time(None, rtt)],
+                ]]
+        l["Requests:", t]
+
+        return l
+
+    def color(self, peerid):
+        def m(c):
+            return min(ord(c) / 2 + 0x80, 0xff)
+        return "#%02x%02x%02x" % (m(peerid[0]), m(peerid[1]), m(peerid[2]))
+
     def render_results(self, ctx, data):
         d = self.download_results()
         def _got_results(results):
@@ -371,7 +510,7 @@ class DownloadStatusPage(DownloadResultsRendererMixin, rend.Page):
         TIME_FORMAT = "%H:%M:%S %d-%b-%Y"
         started_s = time.strftime(TIME_FORMAT,
                                   time.localtime(data.get_started()))
-        return started_s
+        return started_s + " (%s)" % data.get_started()
 
     def render_si(self, ctx, data):
         si_s = base32.b2a_or_none(data.get_storage_index())
diff --git a/src/allmydata/web/tahoe.css b/src/allmydata/web/tahoe.css
index 9e0dc2b..a862966 100644
--- a/src/allmydata/web/tahoe.css
+++ b/src/allmydata/web/tahoe.css
@@ -134,4 +134,14 @@ table.tahoe-directory {
   display: table-cell;
   text-align: center;
   padding: 0 1em;
-}
\ No newline at end of file
+}
+
+/* recent upload/download status pages */
+
+table.status-download-events {
+  border: 1px solid #aaa;
+}
+table.status-download-events td {
+  border: 1px solid #a00;
+  padding: 2px
+}

commit 9d507f297fe1499ca1a31c8d4bca2f846c13514a
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 22:36:33 2010 -0700

    remove some comments about work that's now been finished
---
 src/allmydata/immutable/download2.py |   18 ++----------------
 1 files changed, 2 insertions(+), 16 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index f0d98fe..6e561c7 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -2048,22 +2048,10 @@ class ImmutableFileNode:
 #   CommonShare[shnum]
 #   Share[shnum,server]
 
-# TODO: when we learn numsegs, any get_segment() calls for bad blocknumbers
-# should be failed with BadSegmentNumberError. But should this be the
-# responsibility of CiphertextFileNode, or SegmentFetcher? The knowledge will
-# first appear when a Share receives a valid UEB and calls
-# CiphertextFileNode.validate_UEB, then _parse_UEB. The SegmentFetcher is
-# expecting to hear from the Share, via the _block_request_activity observer.
-
-# make it the responsibility of the SegmentFetcher. Each Share that gets a
-# valid UEB will tell the SegmentFetcher BADSEGNUM (instead of COMPLETE or
-# CORRUPT). The SegmentFetcher it then responsible for shutting down, and
-# informing its parent (the CiphertextFileNode) of the BadSegmentNumberError,
-# which is then passed to the client of get_segment().
-
 
 # TODO: if offset table is corrupt, attacker could cause us to fetch whole
-# (large) share
+# (large) share. But only from that one server, and they could throw lots of
+# data at our connection anyways.
 
 # log budget: when downloading at 1MBps (i.e. 8 segments-per-second), 10
 # log.OPERATIONAL per second, 100 log.NOISY per second. With k=3, that's 3
@@ -2088,6 +2076,4 @@ class ImmutableFileNode:
 
 # tests to write:
 # * truncated share, so _satisfy_* doesn't get all it wants
-# * v2 share, exercise large-offset-table code
 # * slow server
-# * hash failures of all sorts

commit 8fbe659f9d669e7bb374cd350bc8491aed1b0519
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 20:42:49 2010 -0700

    stop parsing the flat crypttext_hash: we don't use it anyways
---
 src/allmydata/immutable/download2.py |   12 ------------
 1 files changed, 0 insertions(+), 12 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index fee2e70..f0d98fe 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1443,7 +1443,6 @@ class _Node:
         self.block_size = None
         self.tail_block_size = None
         #self.ciphertext_hash_tree = None # size depends on num_segments
-        self.ciphertext_hash = None # flat hash, optional
 
         # things to track callers that want data
 
@@ -1643,17 +1642,6 @@ class _Node:
 
         self.share_hash_tree.set_hashes({0: d['share_root_hash']})
 
-        # crypttext_hash is optional. We only pull this from the first UEB
-        # that we see.
-        if 'crypttext_hash' in d:
-            if len(d["crypttext_hash"]) == hashutil.CRYPTO_VAL_SIZE:
-                self.ciphertext_hash = d['crypttext_hash']
-            else:
-                log.msg("ignoring bad-length UEB[crypttext_hash], "
-                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
-                                                   hashutil.CRYPTO_VAL_SIZE),
-                        level=log.WEIRD, parent=self._lp, umid="oZkGLA")
-
         # Our job is a fast download, not verification, so we ignore any
         # redundant fields. The Verifier uses a different code path which
         # does not ignore them.

commit beb53bd2bbea9922ff5de63f0e76997903ffbc9f
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 20:12:08 2010 -0700

    more tests: bad ciphertext hash tree. Remove some unreachable lines.
---
 src/allmydata/immutable/download2.py |    3 +-
 src/allmydata/test/test_download.py  |   69 +++++++++++++++++++++++++++++++++-
 2 files changed, 70 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index d3e61f4..fee2e70 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1014,7 +1014,7 @@ class SegmentFetcher:
         for s in shares:
             if self._shares[s] == state:
                 return s
-        raise IndexError("shouldn't get here")
+        # can never get here, caller has assert in case of code bug
 
     def _send_new_request(self):
         for shnum,shares in self._shnums.iteritems():
@@ -1027,6 +1027,7 @@ class SegmentFetcher:
                 continue
             # here's a candidate. Send a request.
             s = self._find_one(shares, AVAILABLE)
+            assert s
             self._shares[s] = PENDING
             self._share_observers[s] = o = s.get_block(self.segnum)
             o.subscribe(self._block_request_activity, share=s, shnum=shnum)
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index 660fe82..cfdf935 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -14,7 +14,9 @@ from allmydata.immutable import upload, layout
 from allmydata.test.no_network import GridTestMixin
 from allmydata.test.common import ShouldFailMixin
 from allmydata.interfaces import NotEnoughSharesError, NoSharesError
-from allmydata.immutable.download2 import BadSegmentError, BadSegmentNumberError
+from allmydata.immutable.download2 import BadSegmentNumberError, \
+     BadSegmentError, BadCiphertextHashError
+from allmydata.codec import CRSDecoder
 from foolscap.eventual import fireEventually, flushEventualQueue
 
 plaintext = "This is a moderate-sized file.\n" * 10
@@ -600,6 +602,60 @@ class DownloadTest(_Base, unittest.TestCase):
         con = MemoryConsumer()
         d = n.read(con)
         con.producer.stopProducing()
+        # d should never fire
+        del d
+
+    def test_download_segment_bad_ciphertext_hash(self):
+        # The crypttext_hash_tree asserts the integrity of the decoded
+        # ciphertext, and exists to detect two sorts of problems. The first
+        # is a bug in zfec decode. The second is the "two-sided t-shirt"
+        # attack (found by Christian Grothoff), in which a malicious uploader
+        # creates two sets of shares (one for file A, second for file B),
+        # uploads a combination of them (shares 0-4 of A, 5-9 of B), and then
+        # builds an otherwise normal UEB around those shares: their goal is
+        # to give their victim a filecap which sometimes downloads the good A
+        # contents, and sometimes the bad B contents, depending upon which
+        # servers/shares they can get to. Having a hash of the ciphertext
+        # forces them to commit to exactly one version. (Christian's prize
+        # for finding this problem was a t-shirt with two sides: the shares
+        # of file A on the front, B on the back).
+
+        # creating a set of shares with this property is too hard, although
+        # it'd be nice to do so and confirm our fix. (it requires a lot of
+        # tampering with the uploader). So instead, we just damage the
+        # decoder. The tail decoder is rebuilt each time, so we need to use a
+        # file with multiple segments.
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 60 # 6 segs
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            n._cnode._node._build_guessed_tables(u.max_segment_size)
+
+            d = download_to_data(n)
+            def _break_codec(data):
+                # the codec isn't created until the UEB is retrieved
+                node = n._cnode._node
+                vcap = node._verifycap
+                k, N = vcap.needed_shares, vcap.total_shares
+                bad_codec = BrokenDecoder()
+                bad_codec.set_params(node.segment_size, k, N)
+                node._codec = bad_codec
+            d.addCallback(_break_codec)
+            # now try to download it again. The broken codec will provide
+            # ciphertext that fails the hash test.
+            d.addCallback(lambda ign:
+                          self.shouldFail(BadCiphertextHashError, "badhash",
+                                          "hash failure in "
+                                          "ciphertext_hash_tree: segnum=0",
+                                          download_to_data, n))
+            return d
+        d.addCallback(_uploaded)
+        return d
 
     def OFFtest_download_segment_XXX(self):
         self.basedir = self.mktemp()
@@ -656,6 +712,17 @@ class DownloadTest(_Base, unittest.TestCase):
         d = self.download_immutable()
         return d
 
+class BrokenDecoder(CRSDecoder):
+    def decode(self, shares, shareids):
+        d = CRSDecoder.decode(self, shares, shareids)
+        def _decoded(buffers):
+            def _corruptor(s, which):
+                return s[:which] + chr(ord(s[which])^0x01) + s[which+1:]
+            buffers[0] = _corruptor(buffers[0], 0) # flip lsb of first byte
+            return buffers
+        d.addCallback(_decoded)
+        return d
+
 class Corruption(_Base, unittest.TestCase):
 
     def test_each_byte(self):

commit c56706aa47dcbf477a84ec4135a23032063aba41
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 19:26:46 2010 -0700

    clean up asserts, adding tests, slowly converging on complete coverage
---
 src/allmydata/immutable/download2.py |   17 ++-
 src/allmydata/immutable/layout.py    |    4 +
 src/allmydata/test/test_download.py  |  221 +++++++++++++++++++++++++++++++++-
 3 files changed, 230 insertions(+), 12 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 7252451..d3e61f4 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -637,15 +637,15 @@ class Share:
         if not version_s:
             return
         (version,) = struct.unpack(">L", version_s)
+        # The code in _satisfy_offsets will have checked this version
+        # already. There is no code path to get this far with version>2.
+        assert 1 <= version <= 2, "can't get here, version=%d" % version
         if version == 1:
             table_start = 0x0c
             fieldsize = 0x4
         elif version == 2:
             table_start = 0x14
             fieldsize = 0x8
-        else:
-            raise LayoutInvalid("unknown version %d (I understand 1 and 2)"
-                                % version)
         offset_table_size = 6 * fieldsize
         gotta_gotta_have_it.add(table_start, offset_table_size)
 
@@ -1259,6 +1259,7 @@ class Segmentation:
         # want to download file[offset:offset+size]
         self._offset = offset
         self._size = size
+        assert offset+size <= node._verifycap.size
         self._consumer = consumer
         self._lp = logparent
 
@@ -1360,13 +1361,17 @@ class Segmentation:
     def _retry_bad_segment(self, f, had_actual_segment_size):
         f.trap(BadSegmentNumberError) # guessed way wrong, off the end
         if had_actual_segment_size:
-            # but we should have known better, so this is a real error
+            # but we should have known better, so this is a real error. This
+            # indicates a code bug.
+            log.msg("Segmentation retried and failed with wrong segnum",
+                    level=log.WEIRD, parent=self._lp, umid="6Hd0ZA")
             return f
         # we didn't know better: try again with more information
+        assert self._node.segment_size is not None
         return self._maybe_fetch_next()
 
     def _error(self, f):
-        log.msg("Error in Segmentation",
+        log.msg("Error in Segmentation", failure=f,
                 level=log.WEIRD, parent=self._lp, umid="EYlXBg")
         self._alive = False
         self._hungry = False
@@ -1378,7 +1383,7 @@ class Segmentation:
         self._alive = False
         # cancel any outstanding segment request
         if self._cancel_segment_request:
-            self._cancel_segment_request()
+            self._cancel_segment_request.cancel()
             self._cancel_segment_request = None
     def pauseProducing(self):
         self._hungry = False
diff --git a/src/allmydata/immutable/layout.py b/src/allmydata/immutable/layout.py
index 6e07da7..27fb844 100644
--- a/src/allmydata/immutable/layout.py
+++ b/src/allmydata/immutable/layout.py
@@ -74,12 +74,16 @@ limitations described in #346.
 # they are still provided when writing so that older versions of Tahoe can
 # read them.
 
+FORCE_V2 = False # set briefly by unit tests to make small-sized V2 shares
+
 def make_write_bucket_proxy(rref, data_size, block_size, num_segments,
                             num_share_hashes, uri_extension_size_max, nodeid):
     # Use layout v1 for small files, so they'll be readable by older versions
     # (<tahoe-1.3.0). Use layout v2 for large files; they'll only be readable
     # by tahoe-1.3.0 or later.
     try:
+        if FORCE_V2:
+            raise FileTooLargeError
         wbp = WriteBucketProxy(rref, data_size, block_size, num_segments,
                                num_share_hashes, uri_extension_size_max, nodeid)
     except FileTooLargeError:
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index 664ab24..660fe82 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -10,11 +10,11 @@ from allmydata import uri
 from allmydata.storage.server import storage_index_to_dir
 from allmydata.util import base32, fileutil, spans, log
 from allmydata.util.consumer import download_to_data, MemoryConsumer
-from allmydata.immutable import upload
+from allmydata.immutable import upload, layout
 from allmydata.test.no_network import GridTestMixin
 from allmydata.test.common import ShouldFailMixin
 from allmydata.interfaces import NotEnoughSharesError, NoSharesError
-from allmydata.immutable.download2 import BadSegmentNumberError
+from allmydata.immutable.download2 import BadSegmentError, BadSegmentNumberError
 from foolscap.eventual import fireEventually, flushEventualQueue
 
 plaintext = "This is a moderate-sized file.\n" * 10
@@ -386,10 +386,10 @@ class DownloadTest(_Base, unittest.TestCase):
             s0 = shares[0]
             # make sure .cancel works too
             o0 = s0.get_block(0)
-            o0.cancel()
             o0.subscribe(lambda **kwargs: stay_empty.append(kwargs))
             o1 = s0.get_block(0)
             o2 = s0.get_block(0)
+            o0.cancel()
             o3 = s0.get_block(1) # state=BADSEGNUM
             d1 = defer.Deferred()
             d2 = defer.Deferred()
@@ -478,6 +478,100 @@ class DownloadTest(_Base, unittest.TestCase):
                             _try_download)
         return d
 
+    def test_download_bad_segment_retry(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        # damage the Node so that it always gets the wrong segment number
+        n = self.c0.create_node_from_uri(immutable_uri)
+        d = download_to_data(n)
+        def _got_data(data):
+            self.failUnlessEqual(data, plaintext)
+        d.addCallback(_got_data)
+        def _cause_damage(data):
+            # we leave n.num_segments alone, but break n_segment_size
+            log.msg("test_download_bad_segment_retry causing damage")
+            n._cnode._node.segment_size = 72 # 5 segs
+        d.addCallback(_cause_damage)
+        def _try_download():
+            con = MemoryConsumer()
+            d = n.read(con, 150, 20) # seg2
+            return d
+        d.addCallback(lambda ign:
+                      self.shouldFail(BadSegmentNumberError, "badseg",
+                                      "segnum=2, numsegs=1",
+                                      _try_download))
+        return d
+
+    def test_download_bad_segment_retry2(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # damage the Node so that it always gets a wrong (but still valid)
+        # segment number. We upload a file with 6 segments, then force a
+        # segsize which causes our attempt to get seg1 to actually fetch
+        # something else.
+
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 60 # 6 segs
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            n._cnode._node._build_guessed_tables(u.max_segment_size)
+            d = download_to_data(n)
+            def _got_data(data):
+                self.failUnlessEqual(data, plaintext)
+            d.addCallback(_got_data)
+            def _cause_damage1(data):
+                # we leave n.num_segments alone, but break n_segment_size
+                log.msg("test_download_bad_segment_retry2 causing damage")
+                n._cnode._node.segment_size = 90
+                # start the download, so Segmentation 1: gets the wrong size
+                # and 2: thinks it has the right size. But then fix the
+                # segsize before any blocks actually show up (since leaving
+                # segment_size broken would also break FEC decode).
+                con = MemoryConsumer()
+                # file[60:70] really lives in seg1[0:10]. But if we think the
+                # segsize is 90, we'll think it lives in seg0[60:70], so
+                # we'll ask for that, get seg0 (i.e. file[0:60]), notice the
+                # incomplete overlap, and hit the retry code. Because we
+                # tricked Segmentation into thinking that it wasn't guessing
+                # about the segsize, it will trigger the BadSegmentError
+                # failsafe on the first pass.
+                d = n.read(con, 60, 10)
+                n._cnode._node.segment_size = u.max_segment_size
+                def _continue():
+                    return d
+                return self.shouldFail(BadSegmentError, "badseg1",
+                                       "I cannot cope",
+                                       _continue)
+            d.addCallback(_cause_damage1)
+            def _cause_damage2(data):
+                # same thing, but set a segment_size which causes partial
+                # overlap instead of zero overlap
+                log.msg("test_download_bad_segment_retry2 causing damage2")
+                n._cnode._node.segment_size = 57
+                con = MemoryConsumer()
+                # file[57:67] really lives in seg0[57:60]+seg1[0:3]. But if
+                # we think the segsize is 57, we'll think it lives in
+                # seg1[0:10], so we'll ask for that, get seg1 (i.e.
+                # file[60:120]), and the overlap will start 3 bytes into the
+                # data we want. Since we don't yet have the first byte of the
+                # range, we'll hit the retry code.
+                d = n.read(con, 57, 10)
+                n._cnode._node.segment_size = u.max_segment_size
+                def _continue():
+                    return d
+                return self.shouldFail(BadSegmentError, "badseg2",
+                                       "I cannot cope",
+                                       _continue)
+            d.addCallback(_cause_damage2)
+            return d
+        d.addCallback(_uploaded)
+        return d
+
     def test_download_segment_terminate(self):
         self.basedir = self.mktemp()
         self.set_up_grid()
@@ -496,6 +590,17 @@ class DownloadTest(_Base, unittest.TestCase):
         d.addCallback(_check)
         return d
 
+    def test_stop_producing(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+
+        con = MemoryConsumer()
+        d = n.read(con)
+        con.producer.stopProducing()
+
     def OFFtest_download_segment_XXX(self):
         self.basedir = self.mktemp()
         self.set_up_grid()
@@ -523,6 +628,33 @@ class DownloadTest(_Base, unittest.TestCase):
         #d.addCallback(_done)
         return d
 
+    def test_duplicate_shares(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        self.load_shares()
+        # make sure everybody has a copy of sh0. The second server contacted
+        # will report two shares, and the ShareFinder will handle the
+        # duplicate by attaching both to the same CommonShare instance.
+        si = uri.from_string(immutable_uri).get_storage_index()
+        si_dir = storage_index_to_dir(si)
+        sh0_file = [sharefile
+                    for (shnum, serverid, sharefile)
+                    in self.find_shares(immutable_uri)
+                    if shnum == 0][0]
+        sh0_data = open(sh0_file, "rb").read()
+        for clientnum in immutable_shares:
+            if 0 in immutable_shares[clientnum]:
+                continue
+            cdir = self.get_serverdir(clientnum)
+            target = os.path.join(cdir, "shares", si_dir, "0")
+            outf = open(target, "wb")
+            outf.write(sh0_data)
+            outf.close()
+
+        d = self.download_immutable()
+        return d
 
 class Corruption(_Base, unittest.TestCase):
 
@@ -657,7 +789,7 @@ class Corruption(_Base, unittest.TestCase):
                 d.addCallback(_fix_sh0)
                 d.addCallback(fireEventually)
             corrupt_values = [(3, 2, "no-sh0"),
-                              (15, 2, "need-4th"),
+                              (15, 2, "need-4th"), # share looks v2
                               ]
             for i,newvalue,expected in corrupt_values:
                 d.addCallback(_corrupt_set, imm_uri, i, newvalue)
@@ -685,5 +817,82 @@ class Corruption(_Base, unittest.TestCase):
         return d
 
 
-# TODO: test corruption that takes a v1 share and changes the version number
-# to v2 (without changing anything else)
+class DownloadV2(_Base, unittest.TestCase):
+    # tests which exercise v2-share code. They first upload a file with
+    # FORCE_V2 set.
+
+    def setUp(self):
+        d = defer.maybeDeferred(_Base.setUp, self)
+        def _set_force_v2(ign):
+            self.old_force_v2 = layout.FORCE_V2
+            layout.FORCE_V2 = True
+        d.addCallback(_set_force_v2)
+        return d
+    def tearDown(self):
+        layout.FORCE_V2 = self.old_force_v2
+        return _Base.tearDown(self)
+
+    def test_download(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # upload a file
+        u = upload.Data(plaintext, None)
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            imm_uri = ur.uri
+            n = self.c0.create_node_from_uri(imm_uri)
+            return download_to_data(n)
+        d.addCallback(_uploaded)
+        return d
+
+    def test_download_no_overrun(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # tweak the client's copies of server-version data, so it believes
+        # that they're old and can't handle reads that overrun the length of
+        # the share. This exercises a different code path.
+        for (peerid, rref) in self.c0.storage_broker.get_all_servers():
+            v1 = rref.version["http://allmydata.org/tahoe/protocols/storage/v1"]
+            v1["tolerates-immutable-read-overrun"] = False
+
+        # upload a file
+        u = upload.Data(plaintext, None)
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            imm_uri = ur.uri
+            n = self.c0.create_node_from_uri(imm_uri)
+            return download_to_data(n)
+        d.addCallback(_uploaded)
+        return d
+
+    def OFF_test_no_overrun_corrupt_shver(self): # unnecessary
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        for (peerid, rref) in self.c0.storage_broker.get_all_servers():
+            v1 = rref.version["http://allmydata.org/tahoe/protocols/storage/v1"]
+            v1["tolerates-immutable-read-overrun"] = False
+
+        # upload a file
+        u = upload.Data(plaintext, None)
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            imm_uri = ur.uri
+            def _do_corrupt(which, newvalue):
+                def _corruptor(s, debug=False):
+                    return s[:which] + chr(newvalue) + s[which+1:]
+                self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
+            _do_corrupt(12+3, 0x00)
+            n = self.c0.create_node_from_uri(imm_uri)
+            d = download_to_data(n)
+            def _got_data(data):
+                self.failUnlessEqual(data, plaintext)
+            d.addCallback(_got_data)
+            return d
+        d.addCallback(_uploaded)
+        return d

commit b9558f87f9a0fa1fbd3f04dd0e11ac3a1803e090
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 13:36:20 2010 -0700

    test_download.py: big batch of download2-covering tests
---
 src/allmydata/test/test_download.py |  505 +++++++++++++++++++++++++++++++++--
 1 files changed, 485 insertions(+), 20 deletions(-)

diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index cffa132..664ab24 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -5,12 +5,17 @@
 
 import os
 from twisted.trial import unittest
+from twisted.internet import defer
 from allmydata import uri
 from allmydata.storage.server import storage_index_to_dir
-from allmydata.util import base32, fileutil
-from allmydata.util.consumer import download_to_data
+from allmydata.util import base32, fileutil, spans, log
+from allmydata.util.consumer import download_to_data, MemoryConsumer
 from allmydata.immutable import upload
 from allmydata.test.no_network import GridTestMixin
+from allmydata.test.common import ShouldFailMixin
+from allmydata.interfaces import NotEnoughSharesError, NoSharesError
+from allmydata.immutable.download2 import BadSegmentNumberError
+from foolscap.eventual import fireEventually, flushEventualQueue
 
 plaintext = "This is a moderate-sized file.\n" * 10
 mutable_plaintext = "This is a moderate-sized mutable file.\n" * 10
@@ -68,20 +73,7 @@ mutable_shares = {
 }
 #--------- END stored_shares.py ----------------
 
-class DownloadTest(GridTestMixin, unittest.TestCase):
-    timeout = 2400 # It takes longer than 240 seconds on Zandr's ARM box.
-    def test_download(self):
-        self.basedir = self.mktemp()
-        self.set_up_grid()
-        self.c0 = self.g.clients[0]
-
-        # do this to create the shares
-        #return self.create_shares()
-
-        self.load_shares()
-        d = self.download_immutable()
-        d.addCallback(self.download_mutable)
-        return d
+class _Base(GridTestMixin, ShouldFailMixin):
 
     def create_shares(self, ignored=None):
         u = upload.Data(plaintext, None)
@@ -191,12 +183,29 @@ class DownloadTest(GridTestMixin, unittest.TestCase):
         d.addCallback(_got_data)
         return d
 
+class DownloadTest(_Base, unittest.TestCase):
+    timeout = 2400 # It takes longer than 240 seconds on Zandr's ARM box.
+    def test_download(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # do this to create the shares
+        #return self.create_shares()
+
+        self.load_shares()
+        d = self.download_immutable()
+        d.addCallback(self.download_mutable)
+        return d
+
     def test_download_failover(self):
         self.basedir = self.mktemp()
         self.set_up_grid()
         self.c0 = self.g.clients[0]
 
         self.load_shares()
+        si = uri.from_string(immutable_uri).get_storage_index()
+        si_dir = storage_index_to_dir(si)
 
         n = self.c0.create_node_from_uri(immutable_uri)
         d = download_to_data(n)
@@ -204,12 +213,10 @@ class DownloadTest(GridTestMixin, unittest.TestCase):
             self.failUnlessEqual(data, plaintext)
         d.addCallback(_got_data)
 
-        def _clobber_shares(ign):
+        def _clobber_some_shares(ign):
             # find the three shares that were used, and delete them. Then
             # download again, forcing the downloader to fail over to other
             # shares
-            si = uri.from_string(immutable_uri).get_storage_index()
-            si_dir = storage_index_to_dir(si)
             for s in n._cnode._node._shares:
                 for clientnum in immutable_shares:
                     for shnum in immutable_shares[clientnum]:
@@ -217,8 +224,466 @@ class DownloadTest(GridTestMixin, unittest.TestCase):
                             fn = os.path.join(self.get_serverdir(clientnum),
                                               "shares", si_dir, str(shnum))
                             os.unlink(fn)
-        d.addCallback(_clobber_shares)
+        d.addCallback(_clobber_some_shares)
         d.addCallback(lambda ign: download_to_data(n))
         d.addCallback(_got_data)
+
+        def _clobber_most_shares(ign):
+            # delete all but one of the shares that are still alive
+            live_shares = [s for s in n._cnode._node._shares if s.is_alive()]
+            save_me = live_shares[0]._shnum
+            for clientnum in immutable_shares:
+                for shnum in immutable_shares[clientnum]:
+                    if shnum == save_me:
+                        continue
+                    fn = os.path.join(self.get_serverdir(clientnum),
+                                      "shares", si_dir, str(shnum))
+                    if os.path.exists(fn):
+                        os.unlink(fn)
+            # now the download should fail with NotEnoughSharesError
+            return self.shouldFail(NotEnoughSharesError, "1shares", None,
+                                   download_to_data, n)
+        d.addCallback(_clobber_most_shares)
+
+        def _clobber_all_shares(ign):
+            # delete the last remaining share
+            for clientnum in immutable_shares:
+                for shnum in immutable_shares[clientnum]:
+                    fn = os.path.join(self.get_serverdir(clientnum),
+                                      "shares", si_dir, str(shnum))
+                    if os.path.exists(fn):
+                        os.unlink(fn)
+            # now a new download should fail with NoSharesError. We want a
+            # new ImmutableFileNode so it will forget about the old shares.
+            # If we merely called create_node_from_uri() without first
+            # dereferencing the original node, the NodeMaker's _node_cache
+            # would give us back the old one.
+            n = None
+            n = self.c0.create_node_from_uri(immutable_uri)
+            return self.shouldFail(NoSharesError, "0shares", None,
+                                   download_to_data, n)
+        d.addCallback(_clobber_all_shares)
+        return d
+
+    def test_badguess(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+
+        # Cause the downloader to guess a segsize that's too low, so it will
+        # ask for a segment number that's too high (beyond the end of the
+        # real list, causing BadSegmentNumberError), to exercise
+        # Segmentation._retry_bad_segment
+
+        con1 = MemoryConsumer()
+        n._cnode._node._build_guessed_tables(90)
+        # plaintext size of 310 bytes, wrong-segsize of 90 bytes, will make
+        # us think that file[180:200] is in the third segment (segnum=2), but
+        # really there's only one segment
+        d = n.read(con1, 180, 20)
+        def _done(res):
+            self.failUnlessEqual("".join(con1.chunks), plaintext[180:200])
+        d.addCallback(_done)
+        return d
+
+    def test_simultaneous_badguess(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # upload a file with multiple segments, and a non-default segsize, to
+        # exercise the offset-guessing code. Because we don't tell the
+        # downloader about the unusual segsize, it will guess wrong, and have
+        # to do extra roundtrips to get the correct data.
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 70 # 5 segs, 8-wide hashtree
+        con1 = MemoryConsumer()
+        con2 = MemoryConsumer()
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            d1 = n.read(con1, 70, 20)
+            d2 = n.read(con2, 140, 20)
+            return defer.gatherResults([d1,d2])
+        d.addCallback(_uploaded)
+        def _done(res):
+            self.failUnlessEqual("".join(con1.chunks), plaintext[70:90])
+            self.failUnlessEqual("".join(con2.chunks), plaintext[140:160])
+        d.addCallback(_done)
+        return d
+
+    def test_simultaneous_goodguess(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # upload a file with multiple segments, and a non-default segsize, to
+        # exercise the offset-guessing code. This time we *do* tell the
+        # downloader about the unusual segsize, so it can guess right.
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 70 # 5 segs, 8-wide hashtree
+        con1 = MemoryConsumer()
+        con2 = MemoryConsumer()
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            n._cnode._node._build_guessed_tables(u.max_segment_size)
+            d1 = n.read(con1, 70, 20)
+            #d2 = n.read(con2, 140, 20) # XXX
+            d2 = defer.succeed(None)
+            return defer.gatherResults([d1,d2])
+        d.addCallback(_uploaded)
+        def _done(res):
+            self.failUnlessEqual("".join(con1.chunks), plaintext[70:90])
+            self.failUnlessEqual("".join(con2.chunks), plaintext[140:160])
+        #d.addCallback(_done)
+        return d
+
+    def test_sequential_goodguess(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        data = (plaintext*100)[:30000] # multiple of k
+
+        # upload a file with multiple segments, and a non-default segsize, to
+        # exercise the offset-guessing code. This time we *do* tell the
+        # downloader about the unusual segsize, so it can guess right.
+        u = upload.Data(data, None)
+        u.max_segment_size = 6000 # 5 segs, 8-wide hashtree
+        con1 = MemoryConsumer()
+        con2 = MemoryConsumer()
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            n._cnode._node._build_guessed_tables(u.max_segment_size)
+            d = n.read(con1, 12000, 20)
+            def _read1(ign):
+                self.failUnlessEqual("".join(con1.chunks), data[12000:12020])
+                return n.read(con2, 24000, 20)
+            d.addCallback(_read1)
+            def _read2(ign):
+                self.failUnlessEqual("".join(con2.chunks), data[24000:24020])
+            d.addCallback(_read2)
+            return d
+        d.addCallback(_uploaded)
+        return d
+
+
+    def test_simultaneous_get_blocks(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        self.load_shares()
+        stay_empty = []
+
+        n = self.c0.create_node_from_uri(immutable_uri)
+        d = download_to_data(n)
+        def _use_shares(ign):
+            shares = list(n._cnode._node._shares)
+            s0 = shares[0]
+            # make sure .cancel works too
+            o0 = s0.get_block(0)
+            o0.cancel()
+            o0.subscribe(lambda **kwargs: stay_empty.append(kwargs))
+            o1 = s0.get_block(0)
+            o2 = s0.get_block(0)
+            o3 = s0.get_block(1) # state=BADSEGNUM
+            d1 = defer.Deferred()
+            d2 = defer.Deferred()
+            d3 = defer.Deferred()
+            o1.subscribe(lambda **kwargs: d1.callback(kwargs))
+            o2.subscribe(lambda **kwargs: d2.callback(kwargs))
+            o3.subscribe(lambda **kwargs: d3.callback(kwargs))
+            return defer.gatherResults([d1,d2,d3])
+        d.addCallback(_use_shares)
+        def _done(res):
+            r1,r2,r3 = res
+            self.failUnlessEqual(r1["state"], "COMPLETE")
+            self.failUnlessEqual(r2["state"], "COMPLETE")
+            self.failUnlessEqual(r3["state"], "BADSEGNUM")
+            self.failUnless("block" in r1)
+            self.failUnless("block" in r2)
+            self.failIf(stay_empty)
+        d.addCallback(_done)
+        return d
+
+    def test_download_no_overrun(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        self.load_shares()
+
+        # tweak the client's copies of server-version data, so it believes
+        # that they're old and can't handle reads that overrun the length of
+        # the share. This exercises a different code path.
+        for (peerid, rref) in self.c0.storage_broker.get_all_servers():
+            v1 = rref.version["http://allmydata.org/tahoe/protocols/storage/v1"]
+            v1["tolerates-immutable-read-overrun"] = False
+
+        n = self.c0.create_node_from_uri(immutable_uri)
+        d = download_to_data(n)
+        def _got_data(data):
+            self.failUnlessEqual(data, plaintext)
+        d.addCallback(_got_data)
         return d
 
+    def test_download_segment(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+        cn = n._cnode
+        (d,c) = cn.get_segment(0)
+        def _got_segment((offset,data)):
+            self.failUnlessEqual(offset, 0)
+            self.failUnlessEqual(len(data), len(plaintext))
+        d.addCallback(_got_segment)
+        return d
+
+    def test_download_segment_cancel(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+        cn = n._cnode
+        (d,c) = cn.get_segment(0)
+        fired = []
+        d.addCallback(fired.append)
+        c.cancel()
+        d = fireEventually()
+        d.addCallback(flushEventualQueue)
+        def _check(ign):
+            self.failUnlessEqual(fired, [])
+        d.addCallback(_check)
+        return d
+
+    def test_download_bad_segment(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+        cn = n._cnode
+        def _try_download():
+            (d,c) = cn.get_segment(1)
+            return d
+        d = self.shouldFail(BadSegmentNumberError, "badseg",
+                            "segnum=1, numsegs=1",
+                            _try_download)
+        return d
+
+    def test_download_segment_terminate(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+        self.load_shares()
+        n = self.c0.create_node_from_uri(immutable_uri)
+        cn = n._cnode
+        (d,c) = cn.get_segment(0)
+        fired = []
+        d.addCallback(fired.append)
+        self.c0.terminator.disownServiceParent()
+        d = fireEventually()
+        d.addCallback(flushEventualQueue)
+        def _check(ign):
+            self.failUnlessEqual(fired, [])
+        d.addCallback(_check)
+        return d
+
+    def OFFtest_download_segment_XXX(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # upload a file with multiple segments, and a non-default segsize, to
+        # exercise the offset-guessing code. This time we *do* tell the
+        # downloader about the unusual segsize, so it can guess right.
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 70 # 5 segs, 8-wide hashtree
+        con1 = MemoryConsumer()
+        con2 = MemoryConsumer()
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            n = self.c0.create_node_from_uri(ur.uri)
+            n._cnode._node._build_guessed_tables(u.max_segment_size)
+            d1 = n.read(con1, 70, 20)
+            #d2 = n.read(con2, 140, 20)
+            d2 = defer.succeed(None)
+            return defer.gatherResults([d1,d2])
+        d.addCallback(_uploaded)
+        def _done(res):
+            self.failUnlessEqual("".join(con1.chunks), plaintext[70:90])
+            self.failUnlessEqual("".join(con2.chunks), plaintext[140:160])
+        #d.addCallback(_done)
+        return d
+
+
+class Corruption(_Base, unittest.TestCase):
+
+    def test_each_byte(self):
+        # Setting catalog_detection=True performs an exhaustive test of the
+        # Downloader's response to corruption in the lsb of each byte of the
+        # 2070-byte share, with two goals: make sure we tolerate all forms of
+        # corruption (i.e. don't hang or return bad data), and make a list of
+        # which bytes can be corrupted without influencing the download
+        # (since we don't need every byte of the share). That takes 50s to
+        # run on my laptop and doesn't have any actual asserts, so we don't
+        # normally do that.
+        self.catalog_detection = False
+
+        self.basedir = "download/Corruption/each_byte"
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        # to exercise the block-hash-tree code properly, we need to have
+        # multiple segments. We don't tell the downloader about the different
+        # segsize, so it guesses wrong and must do extra roundtrips.
+        u = upload.Data(plaintext, None)
+        u.max_segment_size = 120 # 3 segs, 4-wide hashtree
+
+        def _fix_sh0(res):
+            f = open(self.sh0_file, "wb")
+            f.write(self.sh0_orig)
+            f.close()
+        def _corrupt_flip(ign, imm_uri, which):
+            log.msg("corrupt %d" % which)
+            def _corruptor(s, debug=False):
+                return s[:which] + chr(ord(s[which])^0x01) + s[which+1:]
+            self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
+
+        def _corrupt_set(ign, imm_uri, which, newvalue):
+            log.msg("corrupt %d" % which)
+            def _corruptor(s, debug=False):
+                return s[:which] + chr(newvalue) + s[which+1:]
+            self.corrupt_shares_numbered(imm_uri, [0], _corruptor)
+
+        if self.catalog_detection:
+            undetected = spans.Spans()
+
+        def _download(ign, imm_uri, which, expected):
+            n = self.c0.create_node_from_uri(imm_uri)
+            # for this test to work, we need to have a new Node each time.
+            # Make sure the NodeMaker's weakcache hasn't interfered.
+            assert not n._cnode._node._shares
+            d = download_to_data(n)
+            def _got_data(data):
+                self.failUnlessEqual(data, plaintext)
+                shnums = sorted([s._shnum for s in n._cnode._node._shares])
+                no_sh0 = bool(0 not in shnums)
+                sh0 = [s for s in n._cnode._node._shares if s._shnum == 0]
+                sh0_had_corruption = False
+                if sh0 and sh0[0].had_corruption:
+                    sh0_had_corruption = True
+                num_needed = len(n._cnode._node._shares)
+                if self.catalog_detection:
+                    detected = no_sh0 or sh0_had_corruption or (num_needed!=3)
+                    if not detected:
+                        undetected.add(which, 1)
+                if expected == "no-sh0":
+                    self.failIfIn(0, shnums)
+                elif expected == "0bad-need-3":
+                    self.failIf(no_sh0)
+                    self.failUnless(sh0[0].had_corruption)
+                    self.failUnlessEqual(num_needed, 3)
+                elif expected == "need-4th":
+                    self.failIf(no_sh0)
+                    self.failUnless(sh0[0].had_corruption)
+                    self.failIfEqual(num_needed, 3)
+            d.addCallback(_got_data)
+            return d
+
+
+        d = self.c0.upload(u)
+        def _uploaded(ur):
+            imm_uri = ur.uri
+            self.sh0_file = [sharefile
+                             for (shnum, serverid, sharefile)
+                             in self.find_shares(imm_uri)
+                             if shnum == 0][0]
+            self.sh0_orig = open(self.sh0_file, "rb").read()
+            d = defer.succeed(None)
+            # 'victims' is a list of corruption tests to run. Each one flips
+            # the low-order bit of the specified offset in the share file (so
+            # offset=0 is the MSB of the container version, offset=15 is the
+            # LSB of the share version, offset=24 is the MSB of the
+            # data-block-offset, and offset=48 is the first byte of the first
+            # data-block). Each one also specifies what sort of corruption
+            # we're expecting to see.
+            no_sh0_victims = [0,1,2,3] # container version
+            need3_victims =  [ ] # none currently in this category
+            # when the offsets are corrupted, the Share will be unable to
+            # retrieve the data it wants (because it thinks that data lives
+            # off in the weeds somewhere), and Share treats DataUnavailable
+            # as abandon-this-share, so in general we'll be forced to look
+            # for a 4th share.
+            need_4th_victims = [12,13,14,15, # share version
+                                24,25,26,27, # offset[data]
+                                32,33,34,35, # offset[crypttext_hash_tree]
+                                36,37,38,39, # offset[block_hashes]
+                                44,45,46,47, # offset[UEB]
+                                ]
+            need_4th_victims.append(48) # block data
+            # when corrupting hash trees, we must corrupt a value that isn't
+            # directly set from somewhere else. Since we download data from
+            # seg0, corrupt something on its hash chain, like [2] (the
+            # right-hand child of the root)
+            need_4th_victims.append(600+2*32) # block_hashes[2]
+            # Share.loop is pretty conservative: it abandons the share at the
+            # first sign of corruption. It doesn't strictly need to be this
+            # way: if the UEB were corrupt, we could still get good block
+            # data from that share, as long as there was a good copy of the
+            # UEB elsewhere. If this behavior is relaxed, then corruption in
+            # the following fields (which are present in multiple shares)
+            # should fall into the "need3_victims" case instead of the
+            # "need_4th_victims" case.
+            need_4th_victims.append(376+2*32) # crypttext_hash_tree[2]
+            need_4th_victims.append(824) # share_hashes
+            need_4th_victims.append(994) # UEB length
+            need_4th_victims.append(998) # UEB
+            corrupt_me = ([(i,"no-sh0") for i in no_sh0_victims] +
+                          [(i, "0bad-need-3") for i in need3_victims] +
+                          [(i, "need-4th") for i in need_4th_victims])
+            if self.catalog_detection:
+                corrupt_me = [(i, "") for i in range(len(self.sh0_orig))]
+            for i,expected in corrupt_me:
+                d.addCallback(_corrupt_flip, imm_uri, i)
+                d.addCallback(_download, imm_uri, i, expected)
+                d.addCallback(_fix_sh0)
+                d.addCallback(fireEventually)
+            corrupt_values = [(3, 2, "no-sh0"),
+                              (15, 2, "need-4th"),
+                              ]
+            for i,newvalue,expected in corrupt_values:
+                d.addCallback(_corrupt_set, imm_uri, i, newvalue)
+                d.addCallback(_download, imm_uri, i, expected)
+                d.addCallback(_fix_sh0)
+                d.addCallback(fireEventually)
+            return d
+        d.addCallback(_uploaded)
+        def _show_results(ign):
+            print
+            print ("of [0:%d], corruption ignored in %s" %
+                   (len(self.sh0_orig), undetected.dump()))
+        if self.catalog_detection:
+            d.addCallback(_show_results)
+            # of [0:2070], corruption ignored in len=1133:
+            # [4-11],[16-23],[28-31],[152-439],[600-663],[1309-2069]
+            #  [4-11]: container sizes
+            #  [16-23]: share block/data sizes
+            #  [152-375]: plaintext hash tree
+            #  [376-408]: crypttext_hash_tree[0] (root)
+            #  [408-439]: crypttext_hash_tree[1] (computed)
+            #  [600-631]: block hash tree[0] (root)
+            #  [632-663]: block hash tree[1] (computed)
+            #  [1309-]: reserved+unused UEB space
+        return d
+
+
+# TODO: test corruption that takes a v1 share and changes the version number
+# to v2 (without changing anything else)

commit f95809cc42c452e5cf79462b2c9d9d016524cfe7
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 13:34:38 2010 -0700

    fix error log
---
 src/allmydata/immutable/download2.py |    4 ++--
 1 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index efab400..7252451 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -945,8 +945,8 @@ class SegmentFetcher:
             # oops, we were asking for a segment number beyond the end of the
             # file. This is an error.
             self.stop()
-            e = BadSegmentNumberError("%d > %d" % (self.segnum,
-                                                   self._node.num_segments))
+            e = BadSegmentNumberError("segnum=%d, numsegs=%d" %
+                                      (self.segnum, self._node.num_segments))
             f = Failure(e)
             self._node.fetch_failed(self, f)
             return

commit e6225230c3c0a2c3b24efd0a4cf94d52ff4c5f5f
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 13:34:26 2010 -0700

    set Share.actual_segment_size from the Node whenever possible: non-initial
    shares won't set the UEB and need a way to get the right value. Don't call
    _desire_block_hashes unless there's an active segment.
---
 src/allmydata/immutable/download2.py |   23 ++++++++++++++++-------
 1 files changed, 16 insertions(+), 7 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 5e9c19b..efab400 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -55,9 +55,12 @@ class Share:
         self._rref = rref
         self._server_version = server_version
         self._node = node # holds share_hash_tree and UEB
+        self.actual_segment_size = node.segment_size # might still be None
+        # XXX change node.guessed_segment_size to
+        # node.best_guess_segment_size(), which should give us the real ones
+        # if known, else its guess.
         self._guess_offsets(verifycap, node.guessed_segment_size)
         self.actual_offsets = None
-        self.actual_segment_size = None
         self._UEB_length = None
         self._commonshare = commonshare # holds block_hash_tree
         self._peerid = peerid
@@ -292,6 +295,8 @@ class Share:
             if not self._satisfy_UEB():
                 # can't check any hashes without the UEB
                 return False
+        self.actual_segment_size = self._node.segment_size # might be updated
+        assert self.actual_segment_size is not None
 
         # knowing the UEB means knowing num_segments. Despite the redundancy,
         # this is the best place to set this. CommonShare.set_numsegs will
@@ -408,8 +413,6 @@ class Share:
         self._received.remove(o["uri_extension"], fsize)
         try:
             self._node.validate_and_store_UEB(UEB_s)
-            self.actual_segment_size = self._node.segment_size
-            assert self.actual_segment_size is not None
             return True
         except (LayoutInvalid, BadHashError), e:
             # TODO: if this UEB was bad, we'll keep trying to validate it
@@ -575,6 +578,7 @@ class Share:
         desire = Spans(), Spans(), Spans()
         (want_it, need_it, gotta_gotta_have_it) = desire
 
+        self.actual_segment_size = self._node.segment_size # might be updated
         o = self.actual_offsets or self.guessed_offsets
         segsize = self.actual_segment_size or self.guessed_segment_size
         r = self._node._calculate_sizes(segsize)
@@ -597,8 +601,13 @@ class Share:
             # _desire_hashes or _desire_data unless the segnum looks
             # reasonable.
             if segnum < r["num_segments"]:
-                self._desire_hashes(desire, o, segnum)
+                # XXX somehow we're getting here for sh5. we don't yet know
+                # the actual_segment_size, we're still working off the guess.
+                # the ciphertext_hash_tree has been corrected, but the
+                # commonshare._block_hash_tree is still in the guessed state.
+                self._desire_share_hashes(desire, o)
                 if segnum is not None:
+                    self._desire_block_hashes(desire, o, segnum)
                     self._desire_data(desire, o, r, segnum, segsize)
             else:
                 log.msg("_desire: segnum(%d) looks wrong (numsegs=%d)"
@@ -665,15 +674,15 @@ class Share:
             # we know the length, so make sure we grab everything
             need_it.add(o["uri_extension"]+self._fieldsize, UEB_length)
 
-    def _desire_hashes(self, desire, o, segnum):
+    def _desire_share_hashes(self, desire, o):
         (want_it, need_it, gotta_gotta_have_it) = desire
 
         if self._node.share_hash_tree.needed_hashes(self._shnum):
             hashlen = o["uri_extension"] - o["share_hashes"]
             need_it.add(o["share_hashes"], hashlen)
 
-        if segnum is None:
-            return # I have achieved Zen: I desire nothing.
+    def _desire_block_hashes(self, desire, o, segnum):
+        (want_it, need_it, gotta_gotta_have_it) = desire
 
         # block hash chain
         for hashnum in self._commonshare.get_needed_block_hashes(segnum):

commit e31fa8ec0b01f37e49a134210b137bac70ffa43d
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 13:32:05 2010 -0700

    add had_corruption flag for benefit of unit tests
---
 src/allmydata/immutable/download2.py |   15 +++++++++++++--
 1 files changed, 13 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 4b78132..5e9c19b 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -104,6 +104,8 @@ class Share:
         # 2=offset table, 3=UEB_length and everything else (hashes, block),
         # 4=UEB.
 
+        self.had_corruption = False # for unit tests
+
     def __repr__(self):
         return "Share(sh%d-on-%s)" % (self._shnum, self._peerid_s)
 
@@ -272,6 +274,7 @@ class Share:
         # and sometimes you can't even get what you need
         disappointment = needed & self._unavailable
         if len(disappointment):
+            self.had_corruption = True
             raise DataUnavailable("need %s but will never get it" %
                                   disappointment.dump())
 
@@ -350,6 +353,7 @@ class Share:
             self._fieldsize = 0x8
             self._fieldstruct = "Q"
         else:
+            self.had_corruption = True
             raise LayoutInvalid("unknown version %d (I understand 1 and 2)"
                                 % version)
         offset_table_size = 6 * self._fieldsize
@@ -372,14 +376,16 @@ class Share:
 
         # validate the offsets a bit
         share_hashes_size = offsets["uri_extension"] - offsets["share_hashes"]
-        if share_hashes_size % (2+HASH_SIZE) != 0:
+        if share_hashes_size < 0 or share_hashes_size % (2+HASH_SIZE) != 0:
             # the share hash chain is stored as (hashnum,hash) pairs
+            self.had_corruption = True
             raise LayoutInvalid("share hashes malformed -- should be a"
                                 " multiple of %d bytes -- not %d" %
                                 (2+HASH_SIZE, share_hashes_size))
         block_hashes_size = offsets["share_hashes"] - offsets["block_hashes"]
-        if block_hashes_size % (HASH_SIZE) != 0:
+        if block_hashes_size < 0 or block_hashes_size % (HASH_SIZE) != 0:
             # the block hash tree is stored as a list of hashes
+            self.had_corruption = True
             raise LayoutInvalid("block hashes malformed -- should be a"
                                 " multiple of %d bytes -- not %d" %
                                 (HASH_SIZE, block_hashes_size))
@@ -411,6 +417,7 @@ class Share:
             # yet skip all but the first
             f = Failure(e)
             self._signal_corruption(f, o["uri_extension"], fsize+UEB_length)
+            self.had_corruption = True
             raise
 
     def _satisfy_share_hash_tree(self):
@@ -435,6 +442,7 @@ class Share:
         except (BadHashError, NotEnoughHashesError), e:
             f = Failure(e)
             self._signal_corruption(f, o["share_hashes"], hashlen)
+            self.had_corruption = True
             raise
         self._received.remove(o["share_hashes"], hashlen)
         return True
@@ -470,6 +478,7 @@ class Share:
                     failure=f, level=log.WEIRD, parent=self._lp, umid="yNyFdA")
             hsize = max(0, max(needed_hashes)) * HASH_SIZE
             self._signal_corruption(f, o_bh, hsize)
+            self.had_corruption = True
             raise
         for hashnum in needed_hashes:
             self._received.remove(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
@@ -497,6 +506,7 @@ class Share:
                     level=log.WEIRD, parent=self._lp, umid="iZI0TA")
             hsize = max(0, max(needed_hashes))*HASH_SIZE
             self._signal_corruption(f, start, hsize)
+            self.had_corruption = True
             raise
         for hashnum in needed_hashes:
             self._received.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
@@ -539,6 +549,7 @@ class Share:
             for o in observers:
                 o.notify(state=CORRUPT)
             self._signal_corruption(f, blockstart, blocklen)
+            self.had_corruption = True
         # in either case, we've retired this block
         self._requested_blocks.pop(0)
         # popping the request keeps us from turning around and wanting the

commit 9aa4d209e6d9df9e0e878011b8c85e3db19cc4be
Author: Brian Warner <warner@lothar.com>
Date:   Sun May 9 13:30:58 2010 -0700

    change data-tracking approach again: now pending/received/unavailable. Remove
    wanted/needed/requested. Build up wanted/needed/gottahaveit lists each loop,
    then discard them once requests are sent.
    
    This fixes response-to-corruption when the offset table is broken and causes
    sections to overlap. The first consumer (i.e. share hash tree) will eat the
    data, then the second consumer (i.e. block hash tree) will re-fetch that
    span. One of the two will spot the corrupt (i.e. wrong) data.
    
    Rearrange _desire code into more submethods.
    
    Don't call hashtree.needed_hashes() for obviously bad segnums.
    
    Don't treat shver>2 as the same as shver=2.
---
 src/allmydata/immutable/download2.py |  263 +++++++++++++++++-----------------
 1 files changed, 134 insertions(+), 129 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 6465e2c..4b78132 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -65,10 +65,15 @@ class Share:
         self._storage_index = verifycap.storage_index
         self._si_prefix = base32.b2a(verifycap.storage_index)[:8]
         self._shnum = shnum
-
+        # self._alive becomes False upon fatal corruption or server error
+        self._alive = True
         self._lp = log.msg(format="%(share)s created", share=repr(self),
                            level=log.NOISY, parent=logparent, umid="P7hv2w")
 
+        self._pending = Spans() # request sent but no response received yet
+        self._received = DataSpans() # ACK response received, with data
+        self._unavailable = Spans() # NAK response received, no data
+
         # any given byte of the share can be in one of four states:
         #  in: _wanted, _requested, _received
         #      FALSE    FALSE       FALSE : don't care about it at all
@@ -89,31 +94,6 @@ class Share:
         # consume a data block, we remove it from _requested, so a later
         # download can re-fetch it.
 
-        # self._needed contains data that we need, either metadata (like
-        # hashes) or block data. Once we've received the data, we remove it
-        # from self._needed . We only populate this with data that we're sure
-        # we need, computed after we've received the real offset table: no
-        # guesses!
-        self._needed = Spans()
-
-        # self._wanted contains data that we need, plus data we want (based
-        # upon our guessed offset table, or an initial 2KB fetch that
-        # probably gets all the important stuff). We may wind up not needing
-        # the data in self._wanted, once we learn the correct offset table.
-        self._wanted = Spans()
-
-        # self._requested contains ranges we've requested before: this data
-        # is either in-flight or answered-yes or answered-no.
-        self._requested = Spans() # we've sent a request for this
-        # self._received contains data that we haven't yet used
-        self._received = DataSpans() # we've received a response for this
-
-        # self._unavailable tracks data that we asked for but were not given.
-        # It is populated when we get a response that is shorter than its
-        # corresponding request. This occurs when we read past the end of the
-        # share.
-        self._unavailable = Spans()
-
         self._requested_blocks = [] # (segnum, set(observer2..))
         ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
         self._overrun_ok = ver["tolerates-immutable-read-overrun"]
@@ -124,9 +104,6 @@ class Share:
         # 2=offset table, 3=UEB_length and everything else (hashes, block),
         # 4=UEB.
 
-        # ._alive becomes False upon fatal corruption or server error
-        self._alive = True
-
     def __repr__(self):
         return "Share(sh%d-on-%s)" % (self._shnum, self._peerid_s)
 
@@ -222,12 +199,11 @@ class Share:
     def loop(self):
         try:
             # if any exceptions occur here, kill the download
-            log.msg("%s.loop, reqs=[%s], wanted=%s, needed=%s, requested=%s,"
-                    " received=%s, unavailable=%s" %
+            log.msg("%s.loop, reqs=[%s], pending=%s, received=%s,"
+                    " unavailable=%s" %
                     (repr(self),
                      ",".join([str(req[0]) for req in self._requested_blocks]),
-                     self._wanted.dump(), self._needed.dump(),
-                     self._requested.dump(), self._received.dump(),
+                     self._pending.dump(), self._received.dump(),
                      self._unavailable.dump() ),
                     level=log.NOISY, parent=self._lp, umid="BaL1zw")
             self._do_loop()
@@ -252,23 +228,20 @@ class Share:
         except DataUnavailable, e:
             # Abandon this share.
             log.msg(format="need data that will never be available"
-                    " from %s: wanted=%s, needed=%s, requested=%s,"
-                    " received=%s, unavailable=%s" %
+                    " from %s: pending=%s, received=%s, unavailable=%s" %
                     (repr(self),
-                     self._wanted.dump(), self._needed.dump(),
-                     self._requested.dump(), self._received.dump(),
+                     self._pending.dump(), self._received.dump(),
                      self._unavailable.dump() ),
                     level=log.UNUSUAL, parent=self._lp, umid="F7yJnQ")
             self._fail(Failure(e), log.UNUSUAL)
         except BaseException:
             self._fail(Failure())
             raise
-        log.msg("%s.loop done, reqs=[%s], wanted=%s, needed=%s, requested=%s,"
-                " received=%s, unavailable=%s" %
+        log.msg("%s.loop done, reqs=[%s], pending=%s, received=%s,"
+                " unavailable=%s" %
                 (repr(self),
                  ",".join([str(req[0]) for req in self._requested_blocks]),
-                 self._wanted.dump(), self._needed.dump(),
-                 self._requested.dump(), self._received.dump(),
+                 self._pending.dump(), self._received.dump(),
                  self._unavailable.dump() ),
                 level=log.NOISY, parent=self._lp, umid="9lRaRA")
 
@@ -289,18 +262,18 @@ class Share:
         # we determine what data we desire (to satisfy more requests). The
         # number of segments is finite, so I can't get no satisfaction
         # forever.
-        self._desire()
+        wanted, needed = self._desire()
 
         # Finally, send out requests for whatever we need (desire minus
         # have). You can't always get what you want, but if you try
-        # sometimes, you might find, you get what you need.
-        self._request_needed() # express desire
+        # sometimes, you just might find, you get what you need.
+        self._send_requests(wanted + needed)
 
-        # and sometimes you can't
-        injustice = self._needed & self._unavailable
-        if len(injustice):
+        # and sometimes you can't even get what you need
+        disappointment = needed & self._unavailable
+        if len(disappointment):
             raise DataUnavailable("need %s but will never get it" %
-                                  injustice.dump())
+                                  disappointment.dump())
 
     def _get_satisfaction(self):
         # return True if we retired a data block, and should therefore be
@@ -489,11 +462,11 @@ class Share:
         try:
             self._commonshare.process_block_hashes(block_hashes)
         except (BadHashError, NotEnoughHashesError), e:
-            f = Failure(e),
+            f = Failure(e)
             hashnums = ",".join([str(n) for n in sorted(block_hashes.keys())])
             log.msg(format="hash failure in block_hashes=(%(hashnums)s),"
                     " from %(share)s",
-                    hashnums=hashnums, shnum=self.shnum, share=repr(self),
+                    hashnums=hashnums, shnum=self._shnum, share=repr(self),
                     failure=f, level=log.WEIRD, parent=self._lp, umid="yNyFdA")
             hsize = max(0, max(needed_hashes)) * HASH_SIZE
             self._signal_corruption(f, o_bh, hsize)
@@ -539,15 +512,12 @@ class Share:
 
         block = self._received.pop(blockstart, blocklen)
         if not block:
+            log.msg("no data for block %s (want [%d:+%d])" % (repr(self),
+                                                              blockstart, blocklen))
             return False
         log.msg(format="%(share)s._satisfy_data_block [%(start)d:+%(length)d]",
                 share=repr(self), start=blockstart, length=blocklen,
                 level=log.NOISY, parent=self._lp, umid="uTDNZg")
-        # we removed the block from _received, but don't retain the data in
-        # our Node or CommonShare, so also remove it from _requested: this
-        # lets us ask for it again in a later download which uses this same
-        # Share object.
-        self._requested.remove(blockstart, blocklen)
         # this block is being retired, either as COMPLETE or CORRUPT, since
         # no further data reads will help
         assert self._requested_blocks[0][0] == segnum
@@ -563,9 +533,9 @@ class Share:
             # corruption in other parts of the share, this doesn't cause us
             # to abandon the whole share.
             f = Failure(e)
-            self.log(format="hash failure in block %(segnum)d, from %(share)s",
-                     share=repr(self), failure=f,
-                     level=log.WEIRD, parent=self._logparent, umid="mZjkqA")
+            log.msg(format="hash failure in block %(segnum)d, from %(share)s",
+                    segnum=segnum, share=repr(self), failure=f,
+                    level=log.WEIRD, parent=self._lp, umid="mZjkqA")
             for o in observers:
                 o.notify(state=CORRUPT)
             self._signal_corruption(f, blockstart, blocklen)
@@ -575,68 +545,74 @@ class Share:
         # block again right away
         return True # got satisfaction
 
-    def _want(self, start, length):
-        self._wanted.add(start, length)
-    def _need(self, start, length):
-        # well, you certainly *think* you need it
-        self._wanted.add(start, length)
-        # but you can't *actually* need it unless you know exactly what
-        # you're asking for
-        if self.actual_offsets:
-            self._needed.add(start, length)
-
     def _desire(self):
         segnum, observers = self._active_segnum_and_observers() # maybe None
 
-        if not self.actual_offsets:
-            self._desire_offsets()
+        # 'want_it' is for data we merely want: we know that we don't really
+        # need it. This includes speculative reads, like the first 1KB of the
+        # share (for the offset table) and the first 2KB of the UEB.
+        #
+        # 'need_it' is for data that, if we have the real offset table, we'll
+        # need. If we are only guessing at the offset table, it's merely
+        # wanted. (The share is abandoned if we can't get data that we really
+        # need).
+        #
+        # 'gotta_gotta_have_it' is for data that we absolutely need,
+        # independent of whether we're still guessing about the offset table:
+        # the version number and the offset table itself.
 
-        # we can use guessed offsets as long as this server tolerates overrun
-        if not self.actual_offsets and not self._overrun_ok:
-            return # must wait for the offsets to arrive
+        desire = Spans(), Spans(), Spans()
+        (want_it, need_it, gotta_gotta_have_it) = desire
 
         o = self.actual_offsets or self.guessed_offsets
         segsize = self.actual_segment_size or self.guessed_segment_size
-        if not self._node.have_UEB:
-            self._desire_UEB(o)
-
-        if self._node.share_hash_tree.needed_hashes(self._shnum):
-            hashlen = o["uri_extension"] - o["share_hashes"]
-            self._need(o["share_hashes"], hashlen)
-
-        if segnum is None:
-            return # I have achieved Zen: I desire nothing.
-
-        # block hash chain
-        for hashnum in self._commonshare.get_needed_block_hashes(segnum):
-            self._need(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+        r = self._node._calculate_sizes(segsize)
 
-        # ciphertext hash chain
-        for hashnum in self._node.get_needed_ciphertext_hashes(segnum):
-            self._need(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
+        if not self.actual_offsets:
+            # all _desire functions add bits to the three desire[] spans
+            self._desire_offsets(desire)
+
+        # we can use guessed offsets as long as this server tolerates
+        # overrun. Otherwise, we must wait for the offsets to arrive before
+        # we try to read anything else.
+        if self.actual_offsets or self._overrun_ok:
+            if not self._node.have_UEB:
+                self._desire_UEB(desire, o)
+            # They might ask for a segment that doesn't look right.
+            # _satisfy() will catch+reject bad segnums once we know the UEB
+            # (and therefore segsize and numsegs), so we'll only fail this
+            # test if we're still guessing. We want to avoid asking the
+            # hashtrees for needed_hashes() for bad segnums. So don't enter
+            # _desire_hashes or _desire_data unless the segnum looks
+            # reasonable.
+            if segnum < r["num_segments"]:
+                self._desire_hashes(desire, o, segnum)
+                if segnum is not None:
+                    self._desire_data(desire, o, r, segnum, segsize)
+            else:
+                log.msg("_desire: segnum(%d) looks wrong (numsegs=%d)"
+                        % (segnum, r["num_segments"]),
+                        level=log.UNUSUAL, parent=self._lp, umid="tuYRQQ")
 
-        # data
-        r = self._node._calculate_sizes(segsize)
-        tail = (segnum == r["num_segments"]-1)
-        datastart = o["data"]
-        blockstart = datastart + segnum * r["block_size"]
-        blocklen = r["block_size"]
-        if tail:
-            blocklen = r["tail_block_size"]
-        self._need(blockstart, blocklen)
-        #log.msg("end _desire: wanted=%s" % (self._wanted.dump(),))
+        log.msg("end _desire: want_it=%s need_it=%s gotta=%s"
+                % (want_it.dump(), need_it.dump(), gotta_gotta_have_it.dump()))
+        if self.actual_offsets:
+            return (want_it, need_it+gotta_gotta_have_it)
+        else:
+            return (want_it+need_it, gotta_gotta_have_it)
 
-    def _desire_offsets(self):
+    def _desire_offsets(self, desire):
+        (want_it, need_it, gotta_gotta_have_it) = desire
         if self._overrun_ok:
             # easy! this includes version number, sizes, and offsets
-            self._want(0, 1024)
+            want_it.add(0, 1024)
             return
 
         # v1 has an offset table that lives [0x0,0x24). v2 lives [0x0,0x44).
         # To be conservative, only request the data that we know lives there,
         # even if that means more roundtrips.
 
-        self._need(0, 4)  # version number, always safe
+        gotta_gotta_have_it.add(0, 4)  # version number, always safe
         version_s = self._received.get(0, 4)
         if not version_s:
             return
@@ -644,25 +620,30 @@ class Share:
         if version == 1:
             table_start = 0x0c
             fieldsize = 0x4
-        else:
+        elif version == 2:
             table_start = 0x14
             fieldsize = 0x8
+        else:
+            raise LayoutInvalid("unknown version %d (I understand 1 and 2)"
+                                % version)
         offset_table_size = 6 * fieldsize
-        self._need(table_start, offset_table_size)
+        gotta_gotta_have_it.add(table_start, offset_table_size)
+
+    def _desire_UEB(self, desire, o):
+        (want_it, need_it, gotta_gotta_have_it) = desire
 
-    def _desire_UEB(self, o):
         # UEB data is stored as (length,data).
         if self._overrun_ok:
             # We can pre-fetch 2kb, which should probably cover it. If it
             # turns out to be larger, we'll come back here later with a known
             # length and fetch the rest.
-            self._want(o["uri_extension"], 2048)
+            want_it.add(o["uri_extension"], 2048)
             # now, while that is probably enough to fetch the whole UEB, it
             # might not be, so we need to do the next few steps as well. In
             # most cases, the following steps will not actually add anything
-            # to self._wanted
+            # to need_it
 
-        self._need(o["uri_extension"], self._fieldsize)
+        need_it.add(o["uri_extension"], self._fieldsize)
         # only use a length if we're sure it's correct, otherwise we'll
         # probably fetch a huge number
         if not self.actual_offsets:
@@ -671,21 +652,51 @@ class Share:
         if UEB_length_s:
             (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
             # we know the length, so make sure we grab everything
-            self._need(o["uri_extension"]+self._fieldsize, UEB_length)
-
-    def _request_needed(self):
-        received = self._received.get_spans()
-        ask = self._wanted - self._requested - received
-        self._send_requests(ask) # this removes it from _wanted
-        # XXX then send requests for data blocks. All the hashes should
-        # arrive before the blocks, so the blocks can be consumed and
-        # released in a single turn. TODO: I removed this for simplicity.
+            need_it.add(o["uri_extension"]+self._fieldsize, UEB_length)
+
+    def _desire_hashes(self, desire, o, segnum):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+
+        if self._node.share_hash_tree.needed_hashes(self._shnum):
+            hashlen = o["uri_extension"] - o["share_hashes"]
+            need_it.add(o["share_hashes"], hashlen)
+
+        if segnum is None:
+            return # I have achieved Zen: I desire nothing.
+
+        # block hash chain
+        for hashnum in self._commonshare.get_needed_block_hashes(segnum):
+            need_it.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+
+        # ciphertext hash chain
+        for hashnum in self._node.get_needed_ciphertext_hashes(segnum):
+            need_it.add(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
+
+    def _desire_data(self, desire, o, r, segnum, segsize):
+        (want_it, need_it, gotta_gotta_have_it) = desire
+        tail = (segnum == r["num_segments"]-1)
+        datastart = o["data"]
+        blockstart = datastart + segnum * r["block_size"]
+        blocklen = r["block_size"]
+        if tail:
+            blocklen = r["tail_block_size"]
+        need_it.add(blockstart, blocklen)
+
+    def _send_requests(self, desired):
+        ask = desired - self._pending
+        log.msg("%s._send_requests, desired=%s, pending=%s, ask=%s" %
+                (repr(self), desired.dump(), self._pending.dump(), ask.dump()),
+                level=log.NOISY, parent=self._lp, umid="E94CVA")
+        # XXX At one time, this code distinguished between data blocks and
+        # hashes, and made sure to send (small) requests for hashes before
+        # sending (big) requests for blocks. The idea was to make sure that
+        # all hashes arrive before the blocks, so the blocks can be consumed
+        # and released in a single turn. I removed this for simplicity.
         # Reconsider the removal: maybe bring it back.
 
-    def _send_requests(self, needed):
-        for (start, length) in needed:
+        for (start, length) in ask:
             # TODO: quantize to reasonably-large blocks
-            self._requested.add(start, length)
+            self._pending.add(start, length)
             lp = log.msg(format="%(share)s._send_request"
                          " [%(start)d:+%(length)d]",
                          share=repr(self),
@@ -709,6 +720,8 @@ class Share:
         log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
                 share=repr(self), start=start, length=length, datalen=len(data),
                 level=log.NOISY, parent=lp, umid="5Qn6VQ")
+        self._pending.remove(start, length)
+        self._received.add(start, data)
 
         # if we ask for [a:c], and we get back [a:b] (b<c), that means we're
         # never going to get [b:c]. If we really need that data, this block
@@ -721,10 +734,6 @@ class Share:
         if len(data) < length:
             self._unavailable.add(start+len(data), length-len(data))
 
-        self._received.add(start, data)
-        self._wanted.remove(start, length)
-        self._needed.remove(start, length)
-
         # XXX if table corruption causes our sections to overlap, then one
         # consumer (i.e. block hash tree) will pop/remove the data that
         # another consumer (i.e. block data) mistakenly thinks it needs. It
@@ -801,10 +810,6 @@ class CommonShare:
         self._block_hash_tree.set_hashes({0: roothash})
 
     def get_needed_block_hashes(self, segnum):
-        needed = ",".join([str(n) for n in sorted(self._block_hash_tree.needed_hashes(segnum))])
-        log.msg("CommonShare.get_needed_block_hashes: segnum=%d needs %s" %
-                (segnum, needed),
-                level=log.NOISY, parent=self._logparent, umid="6qTMnw")
         # XXX: include_leaf=True needs thought: how did the old downloader do
         # it? I think it grabbed *all* block hashes and set them all at once.
         # Since we want to fetch less data, we either need to fetch the leaf
@@ -1551,7 +1556,7 @@ class _Node:
                 level=log.OPERATIONAL, parent=self._lp, umid="7sTrPw")
         h = hashutil.uri_extension_hash(UEB_s)
         if h != self._verifycap.uri_extension_hash:
-            raise hashutil.BadHashError
+            raise BadHashError
         UEB_dict = uri.unpack_extension(UEB_s)
         self._parse_and_store_UEB(UEB_dict) # sets self._stuff
         # TODO: a malformed (but authentic) UEB could throw an assertion in

commit 260d813acafb8293894a491f1e0bc466fa4a70a8
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 8 13:13:43 2010 -0700

    tweak logging a little bit
---
 src/allmydata/immutable/download2.py |    6 +++---
 1 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index c1170ac..6465e2c 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -755,17 +755,17 @@ class Share:
                 failure=f, parent=lp, level=log.UNUSUAL, umid="BZgAJw")
         # retire our observers, assuming we won't be able to make any
         # further progress
-        self._fail(f)
+        self._fail(f, log.UNUSUAL)
 
     def _trigger_loop(self, res):
         if self._alive:
             eventually(self.loop)
         return res
 
-    def _fail(self, f):
+    def _fail(self, f, level=log.WEIRD):
         log.msg(format="abandoning %(share)s",
                 share=repr(self), failure=f,
-                level=log.UNUSUAL, parent=self._lp, umid="JKM2Og")
+                level=level, parent=self._lp, umid="JKM2Og")
         self._alive = False
         for (segnum, observers) in self._requested_blocks:
             for o in observers:

commit e7735e384d17db687b3591d3aa081275a0d05d50
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 8 13:12:39 2010 -0700

    treat version >=3 shares as corrupt, not as v2 shares
---
 src/allmydata/immutable/download2.py |    5 ++++-
 1 files changed, 4 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 6571deb..c1170ac 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -372,10 +372,13 @@ class Share:
             table_start = 0x0c
             self._fieldsize = 0x4
             self._fieldstruct = "L"
-        else:
+        elif version == 2:
             table_start = 0x14
             self._fieldsize = 0x8
             self._fieldstruct = "Q"
+        else:
+            raise LayoutInvalid("unknown version %d (I understand 1 and 2)"
+                                % version)
         offset_table_size = 6 * self._fieldsize
         table_s = self._received.pop(table_start, offset_table_size)
         if table_s is None:

commit 70ac54b4cd9ce4f33e8f38fff47031abd0e1299c
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 8 13:12:02 2010 -0700

    track "wanted" and "needed" separately, and abandon the share when we can't
    get needed data
    
    This fixes a lost-progress hang when offset-table corruption makes us think
    that the data block lives beyond the real end of the share.
---
 src/allmydata/immutable/download2.py |  131 ++++++++++++++++++++++++++++------
 1 files changed, 109 insertions(+), 22 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index e7f833c..6571deb 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -36,6 +36,8 @@ class BadCiphertextHashError(Exception):
     pass
 class LayoutInvalid(Exception):
     pass
+class DataUnavailable(Exception):
+    pass
 
 class Share:
     """I represent a single instance of a single share (e.g. I reference the
@@ -87,9 +89,17 @@ class Share:
         # consume a data block, we remove it from _requested, so a later
         # download can re-fetch it.
 
-        # self._wanted contains data that we need, either metadata (like
+        # self._needed contains data that we need, either metadata (like
         # hashes) or block data. Once we've received the data, we remove it
-        # from self._wanted
+        # from self._needed . We only populate this with data that we're sure
+        # we need, computed after we've received the real offset table: no
+        # guesses!
+        self._needed = Spans()
+
+        # self._wanted contains data that we need, plus data we want (based
+        # upon our guessed offset table, or an initial 2KB fetch that
+        # probably gets all the important stuff). We may wind up not needing
+        # the data in self._wanted, once we learn the correct offset table.
         self._wanted = Spans()
 
         # self._requested contains ranges we've requested before: this data
@@ -98,6 +108,12 @@ class Share:
         # self._received contains data that we haven't yet used
         self._received = DataSpans() # we've received a response for this
 
+        # self._unavailable tracks data that we asked for but were not given.
+        # It is populated when we get a response that is shorter than its
+        # corresponding request. This occurs when we read past the end of the
+        # share.
+        self._unavailable = Spans()
+
         self._requested_blocks = [] # (segnum, set(observer2..))
         ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
         self._overrun_ok = ver["tolerates-immutable-read-overrun"]
@@ -206,13 +222,16 @@ class Share:
     def loop(self):
         try:
             # if any exceptions occur here, kill the download
-            log.msg("%s.loop, reqs=[%s], wanted=%s, requested=%s, received=%s" %
+            log.msg("%s.loop, reqs=[%s], wanted=%s, needed=%s, requested=%s,"
+                    " received=%s, unavailable=%s" %
                     (repr(self),
                      ",".join([str(req[0]) for req in self._requested_blocks]),
-                     self._wanted.dump(), self._requested.dump(),
-                     self._received.dump() ),
+                     self._wanted.dump(), self._needed.dump(),
+                     self._requested.dump(), self._received.dump(),
+                     self._unavailable.dump() ),
                     level=log.NOISY, parent=self._lp, umid="BaL1zw")
             self._do_loop()
+            # all exception cases call self._fail(), which clears self._alive
         except (BadHashError, NotEnoughHashesError, LayoutInvalid), e:
             # Abandon this share. We do this if we see corruption in the
             # offset table, the UEB, or a hash tree. We don't abandon the
@@ -226,13 +245,32 @@ class Share:
             #
             # _satisfy_*() code which detects corruption should first call
             # self._signal_corruption(), and then raise the exception.
-            log.msg(format="corruption detected in %(share)s, abandoning it",
+            log.msg(format="corruption detected in %(share)s",
                     share=repr(self),
                     level=log.UNUSUAL, parent=self._lp, umid="gWspVw")
-            self._fail(Failure(e)) # that clears self._alive
+            self._fail(Failure(e), log.UNUSUAL)
+        except DataUnavailable, e:
+            # Abandon this share.
+            log.msg(format="need data that will never be available"
+                    " from %s: wanted=%s, needed=%s, requested=%s,"
+                    " received=%s, unavailable=%s" %
+                    (repr(self),
+                     self._wanted.dump(), self._needed.dump(),
+                     self._requested.dump(), self._received.dump(),
+                     self._unavailable.dump() ),
+                    level=log.UNUSUAL, parent=self._lp, umid="F7yJnQ")
+            self._fail(Failure(e), log.UNUSUAL)
         except BaseException:
             self._fail(Failure())
             raise
+        log.msg("%s.loop done, reqs=[%s], wanted=%s, needed=%s, requested=%s,"
+                " received=%s, unavailable=%s" %
+                (repr(self),
+                 ",".join([str(req[0]) for req in self._requested_blocks]),
+                 self._wanted.dump(), self._needed.dump(),
+                 self._requested.dump(), self._received.dump(),
+                 self._unavailable.dump() ),
+                level=log.NOISY, parent=self._lp, umid="9lRaRA")
 
     def _do_loop(self):
         # we are (eventually) called after all state transitions:
@@ -253,11 +291,17 @@ class Share:
         # forever.
         self._desire()
 
-        # finally send out requests for whatever we need (desire minus have).
-        # You can't always get what you want, but, sometimes, you get what
-        # you need.
+        # Finally, send out requests for whatever we need (desire minus
+        # have). You can't always get what you want, but if you try
+        # sometimes, you might find, you get what you need.
         self._request_needed() # express desire
 
+        # and sometimes you can't
+        injustice = self._needed & self._unavailable
+        if len(injustice):
+            raise DataUnavailable("need %s but will never get it" %
+                                  injustice.dump())
+
     def _get_satisfaction(self):
         # return True if we retired a data block, and should therefore be
         # called again. Return False if we don't retire a data block (even if
@@ -528,6 +572,16 @@ class Share:
         # block again right away
         return True # got satisfaction
 
+    def _want(self, start, length):
+        self._wanted.add(start, length)
+    def _need(self, start, length):
+        # well, you certainly *think* you need it
+        self._wanted.add(start, length)
+        # but you can't *actually* need it unless you know exactly what
+        # you're asking for
+        if self.actual_offsets:
+            self._needed.add(start, length)
+
     def _desire(self):
         segnum, observers = self._active_segnum_and_observers() # maybe None
 
@@ -545,18 +599,18 @@ class Share:
 
         if self._node.share_hash_tree.needed_hashes(self._shnum):
             hashlen = o["uri_extension"] - o["share_hashes"]
-            self._wanted.add(o["share_hashes"], hashlen)
+            self._need(o["share_hashes"], hashlen)
 
         if segnum is None:
             return # I have achieved Zen: I desire nothing.
 
         # block hash chain
         for hashnum in self._commonshare.get_needed_block_hashes(segnum):
-            self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+            self._need(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # ciphertext hash chain
         for hashnum in self._node.get_needed_ciphertext_hashes(segnum):
-            self._wanted.add(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
+            self._need(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # data
         r = self._node._calculate_sizes(segsize)
@@ -566,20 +620,20 @@ class Share:
         blocklen = r["block_size"]
         if tail:
             blocklen = r["tail_block_size"]
-        self._wanted.add(blockstart, blocklen)
+        self._need(blockstart, blocklen)
         #log.msg("end _desire: wanted=%s" % (self._wanted.dump(),))
 
     def _desire_offsets(self):
         if self._overrun_ok:
             # easy! this includes version number, sizes, and offsets
-            self._wanted.add(0,1024)
+            self._want(0, 1024)
             return
 
         # v1 has an offset table that lives [0x0,0x24). v2 lives [0x0,0x44).
         # To be conservative, only request the data that we know lives there,
         # even if that means more roundtrips.
 
-        self._wanted.add(0,4)  # version number, always safe
+        self._need(0, 4)  # version number, always safe
         version_s = self._received.get(0, 4)
         if not version_s:
             return
@@ -591,7 +645,7 @@ class Share:
             table_start = 0x14
             fieldsize = 0x8
         offset_table_size = 6 * fieldsize
-        self._wanted.add(table_start, offset_table_size)
+        self._need(table_start, offset_table_size)
 
     def _desire_UEB(self, o):
         # UEB data is stored as (length,data).
@@ -599,13 +653,13 @@ class Share:
             # We can pre-fetch 2kb, which should probably cover it. If it
             # turns out to be larger, we'll come back here later with a known
             # length and fetch the rest.
-            self._wanted.add(o["uri_extension"], 2048)
+            self._want(o["uri_extension"], 2048)
             # now, while that is probably enough to fetch the whole UEB, it
             # might not be, so we need to do the next few steps as well. In
             # most cases, the following steps will not actually add anything
             # to self._wanted
 
-        self._wanted.add(o["uri_extension"], self._fieldsize)
+        self._need(o["uri_extension"], self._fieldsize)
         # only use a length if we're sure it's correct, otherwise we'll
         # probably fetch a huge number
         if not self.actual_offsets:
@@ -614,7 +668,7 @@ class Share:
         if UEB_length_s:
             (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
             # we know the length, so make sure we grab everything
-            self._wanted.add(o["uri_extension"]+self._fieldsize, UEB_length)
+            self._need(o["uri_extension"]+self._fieldsize, UEB_length)
 
     def _request_needed(self):
         received = self._received.get_spans()
@@ -652,10 +706,43 @@ class Share:
         log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
                 share=repr(self), start=start, length=length, datalen=len(data),
                 level=log.NOISY, parent=lp, umid="5Qn6VQ")
-        span = (start, length)
-        assert span in self._requested # XXX eh, not important
+
+        # if we ask for [a:c], and we get back [a:b] (b<c), that means we're
+        # never going to get [b:c]. If we really need that data, this block
+        # will never complete. The easiest way to get into this situation is
+        # to hit a share with a corrupted offset table, or one that's somehow
+        # been truncated. On the other hand, when overrun_ok is true, we ask
+        # for data beyond the end of the share all the time (it saves some
+        # RTT when we don't know the length of the share ahead of time). So
+        # not every asked-for-but-not-received byte is fatal.
+        if len(data) < length:
+            self._unavailable.add(start+len(data), length-len(data))
+
         self._received.add(start, data)
         self._wanted.remove(start, length)
+        self._needed.remove(start, length)
+
+        # XXX if table corruption causes our sections to overlap, then one
+        # consumer (i.e. block hash tree) will pop/remove the data that
+        # another consumer (i.e. block data) mistakenly thinks it needs. It
+        # won't ask for that data again, because the span is in
+        # self._requested. But that span won't be in self._unavailable
+        # because we got it back from the server. TODO: handle this properly
+        # (raise DataUnavailable). Then add sanity-checking
+        # no-overlaps-allowed tests to the offset-table unpacking code to
+        # catch this earlier. XXX
+
+        # accumulate a wanted/needed span (not as self._x, but passed into
+        # desire* functions). manage a pending/in-flight list. when the
+        # requests are sent out, empty/discard the wanted/needed span and
+        # populate/augment the pending list. when the responses come back,
+        # augment either received+data or unavailable.
+
+        # if a corrupt offset table results in double-usage, we'll send
+        # double requests.
+
+        # the wanted/needed span is only "wanted" for the first pass. Once
+        # the offset table arrives, it's all "needed".
 
     def _got_error(self, f, start, length, lp):
         log.msg(format="error requesting %(start)d+%(length)d"

commit da7a276fcbc7d2358365a3d7f878b5e99b268f81
Author: Brian Warner <warner@lothar.com>
Date:   Sat May 8 13:10:22 2010 -0700

    spans: add '&' operator for intersection of two span objects
---
 src/allmydata/test/test_util.py |   34 +++++++++++++++++++++++++++++++++-
 src/allmydata/util/spans.py     |    8 ++++++++
 2 files changed, 41 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/test/test_util.py b/src/allmydata/test/test_util.py
index de4a8ad..2fceee5 100644
--- a/src/allmydata/test/test_util.py
+++ b/src/allmydata/test/test_util.py
@@ -1609,6 +1609,13 @@ class SimpleSpans:
             self.remove(start, length)
         return self
 
+    def __and__(self, other):
+        s = self.__class__()
+        for i in other.each():
+            if i in self._have:
+                s.add(i, 1)
+        return s
+
     def __contains__(self, (start,length)):
         for i in range(start, start+length):
             if i not in self._have:
@@ -1644,6 +1651,11 @@ class ByteSpans(unittest.TestCase):
         self.failUnlessEqual(list(s2.each()), [3,10,11,16,20,21])
         self.failUnlessEqual(len(s2), 6)
 
+        s1 = SimpleSpans(3, 4) # 3 4 5 6
+        s2 = SimpleSpans(5, 4) # 5 6 7 8
+        i = s1 & s2
+        self.failUnlessEqual(list(i.each()), [5, 6])
+
     def _check1(self, s):
         self.failUnlessEqual(list(s), [(3,4)])
         self.failUnless(s)
@@ -1668,6 +1680,22 @@ class ByteSpans(unittest.TestCase):
         self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7])
         s = s2 - s3
         self.failUnlessEqual(list(s.each()), [5,6,7])
+        s = s1 & s2
+        self.failUnlessEqual(list(s.each()), [5,6,7])
+        s = s2 & s1
+        self.failUnlessEqual(list(s.each()), [5,6,7])
+        s = s1 & s3
+        self.failUnlessEqual(list(s.each()), [8,9])
+        s = s3 & s1
+        self.failUnlessEqual(list(s.each()), [8,9])
+        s = s2 & s3
+        self.failUnlessEqual(list(s.each()), [])
+        s = s3 & s2
+        self.failUnlessEqual(list(s.each()), [])
+        s = Spans() & s3
+        self.failUnlessEqual(list(s.each()), [])
+        s = s3 & Spans()
+        self.failUnlessEqual(list(s.each()), [])
 
         s = s1 + s2
         self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7,8,9])
@@ -1748,10 +1776,14 @@ class ByteSpans(unittest.TestCase):
                 ns1, ns2 = _create(what[7:11])
                 #print "s2 += %s" % ns2.dump()
                 s1 += ns1; s2 += ns2
-            else:
+            elif op in "de":
                 ns1, ns2 = _create(what[7:11])
                 #print "%s -= %s" % (s2.dump(), ns2.dump())
                 s1 -= ns1; s2 -= ns2
+            else:
+                ns1, ns2 = _create(what[7:11])
+                #print "%s &= %s" % (s2.dump(), ns2.dump())
+                s1 = s1 & ns1; s2 = s2 & ns2
             #print "s2 now %s" % s2.dump()
             self.failUnlessEqual(list(s1.each()), list(s2.each()))
             self.failUnlessEqual(len(s1), len(s2))
diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
index 853d207..2a199f0 100755
--- a/src/allmydata/util/spans.py
+++ b/src/allmydata/util/spans.py
@@ -179,6 +179,14 @@ class Spans:
             self.remove(start, length)
         return self
 
+    def __and__(self, other):
+        if not self._spans:
+            return self.__class__()
+        bounds = self.__class__(self._spans[0][0],
+                                self._spans[-1][0]+self._spans[-1][1])
+        not_other = bounds - other
+        return self - not_other
+
     def __contains__(self, (start,length)):
         for span_start,span_length in self._spans:
             o = overlap(start, length, span_start, span_length)

commit 539ff0c92fdb43453f0bffce921101b164c4dfc1
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 11:17:44 2010 -0700

    fix _desire_data for tail segments, make umids unique
---
 src/allmydata/immutable/download2.py |   10 ++--------
 1 files changed, 2 insertions(+), 8 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 7131b21..e7f833c 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -560,7 +560,7 @@ class Share:
 
         # data
         r = self._node._calculate_sizes(segsize)
-        tail = (segnum == r["num_segments"])
+        tail = (segnum == r["num_segments"]-1)
         datastart = o["data"]
         blockstart = datastart + segnum * r["block_size"]
         blocklen = r["block_size"]
@@ -662,7 +662,7 @@ class Share:
                 " from %(server)s for si %(si)s",
                 start=start, length=length,
                 server=self._peerid_s, si=self._si_prefix,
-                failure=f, parent=lp, level=log.UNUSUAL, umid="qZu0wg")
+                failure=f, parent=lp, level=log.UNUSUAL, umid="BZgAJw")
         # retire our observers, assuming we won't be able to make any
         # further progress
         self._fail(f)
@@ -1978,13 +1978,7 @@ class ImmutableFileNode:
 # variables in the CopiedFailure
 
 # tests to write:
-# * two-segment file, tweak default-segsize so we guess right, start
-#   simultaneous reads of both segments. goal is to cover Share.get_block
-#   in the for(self._requested_blocks) loop.
-# * server with _overrun_ok=False, to exercise the non-trivial [:1024] path
-#   inside Share._desire_offsets
 # * truncated share, so _satisfy_* doesn't get all it wants
 # * v2 share, exercise large-offset-table code
 # * slow server
 # * hash failures of all sorts
-# * NoSharesError

commit 78a567c2d27d13ba501c6f82c9a452191a784442
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 11:17:18 2010 -0700

    move download2_off slightly more out of the way
---
 src/allmydata/immutable/download2_off.py    |  634 ---------------------------
 src/allmydata/immutable/download2_off.pyOFF |  634 +++++++++++++++++++++++++++
 2 files changed, 634 insertions(+), 634 deletions(-)

diff --git a/src/allmydata/immutable/download2_off.py b/src/allmydata/immutable/download2_off.py
deleted file mode 100755
index d2b8b99..0000000
--- a/src/allmydata/immutable/download2_off.py
+++ /dev/null
@@ -1,634 +0,0 @@
-#! /usr/bin/python
-
-# known (shnum,Server) pairs are sorted into a list according to
-# desireability. This sort is picking a winding path through a matrix of
-# [shnum][server]. The goal is to get diversity of both shnum and server.
-
-# The initial order is:
-#  find the lowest shnum on the first server, add it
-#  look at the next server, find the lowest shnum that we don't already have
-#   if any
-#  next server, etc, until all known servers are checked
-#  now look at servers that we skipped (because ...
-
-# Keep track of which block requests are outstanding by (shnum,Server). Don't
-# bother prioritizing "validated" shares: the overhead to pull the share hash
-# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
-# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
-# 832 bytes). Each time a block request is sent, also request any necessary
-# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
-# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
-
-# For each outstanding segread, walk the list and send requests (skipping
-# outstanding shnums) until requests for k distinct shnums are in flight. If
-# we can't do that, ask for more. If we get impatient on a request, find the
-# first non-outstanding
-
-# start with the first Share in the list, and send a request. Then look at
-# the next one. If we already have a pending request for the same shnum or
-# server, push that Share down onto the fallback list and try the next one,
-# etc. If we run out of non-fallback shares, use the fallback ones,
-# preferring shnums that we don't have outstanding requests for (i.e. assume
-# that all requests will complete). Do this by having a second fallback list.
-
-# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
-# per file, not thousands.
-
-# actually, don't bother sorting the initial list. Append Shares as the
-# responses come back, that will put the fastest servers at the front of the
-# list, and give a tiny preference to servers that are earlier in the
-# permuted order.
-
-# more ideas:
-#  sort shares by:
-#   1: number of roundtrips needed to get some data
-#   2: share number
-#   3: ms of RTT delay
-# maybe measure average time-to-completion of requests, compare completion
-# time against that, much larger indicates congestion on the server side
-# or the server's upstream speed is less than our downstream. Minimum
-# time-to-completion indicates min(our-downstream,their-upstream). Could
-# fetch shares one-at-a-time to measure that better.
-
-# when should we risk duplicate work and send a new request?
-
-def walk(self):
-    shares = sorted(list)
-    oldshares = copy(shares)
-    outstanding = list()
-    fallbacks = list()
-    second_fallbacks = list()
-    while len(outstanding.nonlate.shnums) < k: # need more requests
-        while oldshares:
-            s = shares.pop(0)
-            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
-                fallbacks.append(s)
-                continue
-            outstanding.append(s)
-            send_request(s)
-            break #'while need_more_requests'
-        # must use fallback list. Ask for more servers while we're at it.
-        ask_for_more_servers()
-        while fallbacks:
-            s = fallbacks.pop(0)
-            if s.shnum in outstanding.shnums:
-                # assume that the outstanding requests will complete, but
-                # send new requests for other shnums to existing servers
-                second_fallbacks.append(s)
-                continue
-            outstanding.append(s)
-            send_request(s)
-            break #'while need_more_requests'
-        # if we get here, we're being forced to send out multiple queries per
-        # share. We've already asked for more servers, which might help. If
-        # there are no late outstanding queries, then duplicate shares won't
-        # help. Don't send queries for duplicate shares until some of the
-        # queries are late.
-        if outstanding.late:
-            # we're allowed to try any non-outstanding share
-            while second_fallbacks:
-                pass
-    newshares = outstanding + fallbacks + second_fallbacks + oldshares
-        
-
-class Server:
-    """I represent an abstract Storage Server. One day, the StorageBroker
-    will return instances of me. For now, the StorageBroker returns (peerid,
-    RemoteReference) tuples, and this code wraps a Server instance around
-    them.
-    """
-    def __init__(self, peerid, ss):
-        self.peerid = peerid
-        self.remote = ss
-        self._remote_buckets = {} # maps shnum to RIBucketReader
-        # TODO: release the bucket references on shares that we no longer
-        # want. OTOH, why would we not want them? Corruption?
-
-    def send_query(self, storage_index):
-        """I return a Deferred that fires with a set of shnums. If the server
-        had shares available, I will retain the RemoteReferences to its
-        buckets, so that get_data(shnum, range) can be called later."""
-        d = self.remote.callRemote("get_buckets", self.storage_index)
-        d.addCallback(self._got_response)
-        return d
-
-    def _got_response(self, r):
-        self._remote_buckets = r
-        return set(r.keys())
-
-class ShareOnAServer:
-    """I represent one instance of a share, known to live on a specific
-    server. I am created every time a server responds affirmatively to a
-    do-you-have-block query."""
-
-    def __init__(self, shnum, server):
-        self._shnum = shnum
-        self._server = server
-        self._block_hash_tree = None
-
-    def cost(self, segnum):
-        """I return a tuple of (roundtrips, bytes, rtt), indicating how
-        expensive I think it would be to fetch the given segment. Roundtrips
-        indicates how many roundtrips it is likely to take (one to get the
-        data and hashes, plus one to get the offset table and UEB if this is
-        the first segment we've ever fetched). 'bytes' is how many bytes we
-        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
-        seconds for a trivial request. The downloading algorithm will compare
-        costs to decide which shares should be used."""
-        # the most significant factor here is roundtrips: a Share for which
-        # we already have the offset table is better to than a brand new one
-
-    def max_bandwidth(self):
-        """Return a float, indicating the highest plausible bytes-per-second
-        that I've observed coming from this share. This will be based upon
-        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
-        can we used to estimate the server's upstream bandwidth. Clearly this
-        is only accurate if a share is retrieved with no contention for
-        either the upstream, downstream, or middle of the connection, but it
-        may still serve as a useful metric for deciding which servers to pull
-        from."""
-
-    def get_segment(self, segnum):
-        """I return a Deferred that will fire with the segment data, or
-        errback."""
-
-class NativeShareOnAServer(ShareOnAServer):
-    """For tahoe native (foolscap) servers, I contain a RemoteReference to
-    the RIBucketReader instance."""
-    def __init__(self, shnum, server, rref):
-        ShareOnAServer.__init__(self, shnum, server)
-        self._rref = rref # RIBucketReader
-
-class Share:
-    def __init__(self, shnum):
-        self._shnum = shnum
-        # _servers are the Server instances which appear to hold a copy of
-        # this share. It is populated when the ValidShare is first created,
-        # or when we receive a get_buckets() response for a shnum that
-        # already has a ValidShare instance. When we lose the connection to a
-        # server, we remove it.
-        self._servers = set()
-        # offsets, UEB, and share_hash_tree all live in the parent.
-        # block_hash_tree lives here.
-        self._block_hash_tree = None
-
-        self._want
-
-    def get_servers(self):
-        return self._servers
-
-
-    def get_block(self, segnum):
-        # read enough data to obtain a single validated block
-        if not self.have_offsets:
-            # we get the offsets in their own read, since they tell us where
-            # everything else lives. We must fetch offsets for each share
-            # separately, since they aren't directly covered by the UEB.
-            pass
-        if not self.parent.have_ueb:
-            # use _guessed_segsize to make a guess about the layout, so we
-            # can fetch both the offset table and the UEB in the same read.
-            # This also requires making a guess about the presence or absence
-            # of the plaintext_hash_tree. Oh, and also the version number. Oh
-            # well.
-            pass
-
-class CiphertextDownloader:
-    """I manage all downloads for a single file. I operate a state machine
-    with input events that are local read() requests, responses to my remote
-    'get_bucket' and 'read_bucket' messages, and connection establishment and
-    loss. My outbound events are connection establishment requests and bucket
-    read requests messages.
-    """
-    # eventually this will merge into the FileNode
-    ServerClass = Server # for tests to override
-
-    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
-                 shutdowner):
-        # values we get from the filecap
-        self._storage_index = si = storage_index
-        self._ueb_hash = ueb_hash
-        self._size = size
-        self._needed_shares = k
-        self._total_shares = N
-        self._share_hash_tree = IncompleteHashTree(self._total_shares)
-        # values we discover when we first fetch the UEB
-        self._ueb = None # is dict after UEB fetch+validate
-        self._segsize = None
-        self._numsegs = None
-        self._blocksize = None
-        self._tail_segsize = None
-        self._ciphertext_hash = None # optional
-        # structures we create when we fetch the UEB, then continue to fill
-        # as we download the file
-        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
-        self._ciphertext_hash_tree = None
-
-        # values we learn as we download the file
-        self._offsets = {} # (shnum,Server) to offset table (dict)
-        self._block_hash_tree = {} # shnum to IncompleteHashTree
-        # other things which help us
-        self._guessed_segsize = min(128*1024, size)
-        self._active_share_readers = {} # maps shnum to Reader instance
-        self._share_readers = [] # sorted by preference, best first
-        self._readers = set() # set of Reader instances
-        self._recent_horizon = 10 # seconds
-
-        # 'shutdowner' is a MultiService parent used to cancel all downloads
-        # when the node is shutting down, to let tests have a clean reactor.
-
-        self._init_available_servers()
-        self._init_find_enough_shares()
-
-    # _available_servers is an iterator that provides us with Server
-    # instances. Each time we pull out a Server, we immediately send it a
-    # query, so we don't need to keep track of who we've sent queries to.
-
-    def _init_available_servers(self):
-        self._available_servers = self._get_available_servers()
-        self._no_more_available_servers = False
-
-    def _get_available_servers(self):
-        """I am a generator of servers to use, sorted by the order in which
-        we should query them. I make sure there are no duplicates in this
-        list."""
-        # TODO: make StorageBroker responsible for this non-duplication, and
-        # replace this method with a simple iter(get_servers_for_index()),
-        # plus a self._no_more_available_servers=True
-        seen = set()
-        sb = self._storage_broker
-        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
-            if peerid not in seen:
-                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
-                seen.add(peerid)
-        self._no_more_available_servers = True
-
-    # this block of code is responsible for having enough non-problematic
-    # distinct shares/servers available and ready for download, and for
-    # limiting the number of queries that are outstanding. The idea is that
-    # we'll use the k fastest/best shares, and have the other ones in reserve
-    # in case those servers stop responding or respond too slowly. We keep
-    # track of all known shares, but we also keep track of problematic shares
-    # (ones with hash failures or lost connections), so we can put them at
-    # the bottom of the list.
-
-    def _init_find_enough_shares(self):
-        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
-        # where viable (but not yet validated) shares are located. Each
-        # get_bucket() response adds to this map, each act of validation
-        # removes from it.
-        self._sharemap = DictOfSets()
-
-        # _sharemap maps shnum to set of Servers, and remembers where viable
-        # shares are located. Each get_bucket() response adds to this map,
-        # each hash failure or disconnect removes from it. (TODO: if we
-        # disconnect but reconnect later, we should be allowed to re-query).
-        self._sharemap = DictOfSets()
-
-        # _problem_shares is a set of (shnum, Server) tuples, and
-
-        # _queries_in_flight maps a Server to a timestamp, which remembers
-        # which servers we've sent queries to (and when) but have not yet
-        # heard a response. This lets us put a limit on the number of
-        # outstanding queries, to limit the size of the work window (how much
-        # extra work we ask servers to do in the hopes of keeping our own
-        # pipeline filled). We remove a Server from _queries_in_flight when
-        # we get an answer/error or we finally give up. If we ever switch to
-        # a non-connection-oriented protocol (like UDP, or forwarded Chord
-        # queries), we can use this information to retransmit any query that
-        # has gone unanswered for too long.
-        self._queries_in_flight = dict()
-
-    def _count_recent_queries_in_flight(self):
-        now = time.time()
-        recent = now - self._recent_horizon
-        return len([s for (s,when) in self._queries_in_flight.items()
-                    if when > recent])
-
-    def _find_enough_shares(self):
-        # goal: have 2*k distinct not-invalid shares available for reading,
-        # from 2*k distinct servers. Do not have more than 4*k "recent"
-        # queries in flight at a time.
-        if (len(self._sharemap) >= 2*self._needed_shares
-            and len(self._sharemap.values) >= 2*self._needed_shares):
-            return
-        num = self._count_recent_queries_in_flight()
-        while num < 4*self._needed_shares:
-            try:
-                s = self._available_servers.next()
-            except StopIteration:
-                return # no more progress can be made
-            self._queries_in_flight[s] = time.time()
-            d = s.send_query(self._storage_index)
-            d.addBoth(incidentally, self._queries_in_flight.discard, s)
-            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
-                                           for shnum in shnums],
-                           lambda f: self._query_error(f, s))
-            d.addErrback(self._error)
-            d.addCallback(self._reschedule)
-            num += 1
-
-    def _query_error(self, f, s):
-        # a server returned an error, log it gently and ignore
-        level = log.WEIRD
-        if f.check(DeadReferenceError):
-            level = log.UNUSUAL
-        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
-                failure=f, level=level, umid="3uuBUQ")
-
-    # this block is responsible for turning known shares into usable shares,
-    # by fetching enough data to validate their contents.
-
-    # UEB (from any share)
-    # share hash chain, validated (from any share, for given shnum)
-    # block hash (any share, given shnum)
-
-    def _got_ueb(self, ueb_data, share):
-        if self._ueb is not None:
-            return
-        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
-            share.error("UEB hash does not match")
-            return
-        d = uri.unpack_extension(ueb_data)
-        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
-
-
-        # There are several kinds of things that can be found in a UEB.
-        # First, things that we really need to learn from the UEB in order to
-        # do this download. Next: things which are optional but not redundant
-        # -- if they are present in the UEB they will get used. Next, things
-        # that are optional and redundant. These things are required to be
-        # consistent: they don't have to be in the UEB, but if they are in
-        # the UEB then they will be checked for consistency with the
-        # already-known facts, and if they are inconsistent then an exception
-        # will be raised. These things aren't actually used -- they are just
-        # tested for consistency and ignored. Finally: things which are
-        # deprecated -- they ought not be in the UEB at all, and if they are
-        # present then a warning will be logged but they are otherwise
-        # ignored.
-
-        # First, things that we really need to learn from the UEB:
-        # segment_size, crypttext_root_hash, and share_root_hash.
-        self._segsize = d['segment_size']
-
-        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
-        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
-
-        self._tail_segsize = self._size % self._segsize
-        if self._tail_segsize == 0:
-            self._tail_segsize = self._segsize
-        # padding for erasure code
-        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
-                                                    self._needed_shares)
-
-        # Ciphertext hash tree root is mandatory, so that there is at most
-        # one ciphertext that matches this read-cap or verify-cap. The
-        # integrity check on the shares is not sufficient to prevent the
-        # original encoder from creating some shares of file A and other
-        # shares of file B.
-        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
-        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
-
-        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
-
-
-        # Next: things that are optional and not redundant: crypttext_hash
-        if 'crypttext_hash' in d:
-            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
-                self._ciphertext_hash = d['crypttext_hash']
-            else:
-                log.msg("ignoring bad-length UEB[crypttext_hash], "
-                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
-                                                   hashutil.CRYPTO_VAL_SIZE),
-                        umid="oZkGLA", level=log.WEIRD)
-
-        # we ignore all of the redundant fields when downloading. The
-        # Verifier uses a different code path which does not ignore them.
-
-        # finally, set self._ueb as a marker that we don't need to request it
-        # anymore
-        self._ueb = d
-
-    def _got_share_hashes(self, hashes, share):
-        assert isinstance(hashes, dict)
-        try:
-            self._share_hash_tree.set_hashes(hashes)
-        except (IndexError, BadHashError, NotEnoughHashesError), le:
-            share.error("Bad or missing hashes")
-            return
-
-    #def _got_block_hashes(
-
-    def _init_validate_enough_shares(self):
-        # _valid_shares maps shnum to ValidatedShare instances, and is
-        # populated once the block hash root has been fetched and validated
-        # (which requires any valid copy of the UEB, and a valid copy of the
-        # share hash chain for each shnum)
-        self._valid_shares = {}
-
-        # _target_shares is an ordered list of ReadyShare instances, each of
-        # which is a (shnum, server) tuple. It is sorted in order of
-        # preference: we expect to get the fastest response from the
-        # ReadyShares at the front of the list. It is also sorted to
-        # distribute the shnums, so that fetching shares from
-        # _target_shares[:k] is likely (but not guaranteed) to give us k
-        # distinct shares. The rule is that we skip over entries for blocks
-        # that we've already received, limit the number of recent queries for
-        # the same block, 
-        self._target_shares = []
-
-    def _validate_enough_shares(self):
-        # my goal is to have at least 2*k distinct validated shares from at
-        # least 2*k distinct servers
-        valid_share_servers = set()
-        for vs in self._valid_shares.values():
-            valid_share_servers.update(vs.get_servers())
-        if (len(self._valid_shares) >= 2*self._needed_shares
-            and len(self._valid_share_servers) >= 2*self._needed_shares):
-            return
-        #for 
-
-    def _reschedule(self, _ign):
-        # fire the loop again
-        if not self._scheduled:
-            self._scheduled = True
-            eventually(self._loop)
-
-    def _loop(self):
-        self._scheduled = False
-        # what do we need?
-
-        self._find_enough_shares()
-        self._validate_enough_shares()
-
-        if not self._ueb:
-            # we always need a copy of the UEB
-            pass
-
-    def _error(self, f):
-        # this is an unexpected error: a coding bug
-        log.err(f, level=log.UNUSUAL)
-            
-
-
-# using a single packed string (and an offset table) may be an artifact of
-# our native storage server: other backends might allow cheap multi-part
-# files (think S3, several buckets per share, one for each section).
-
-# find new names for:
-#  data_holder
-#  Share / Share2  (ShareInstance / Share? but the first is more useful)
-
-class IShare(Interface):
-    """I represent a single instance of a single share (e.g. I reference the
-    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
-    This interface is used by SegmentFetcher to retrieve validated blocks.
-    """
-    def get_block(segnum):
-        """Return an Observer2, which will be notified with the following
-        events:
-         state=COMPLETE, block=data (terminal): validated block data
-         state=OVERDUE (non-terminal): we have reason to believe that the
-                                       request might have stalled, or we
-                                       might just be impatient
-         state=CORRUPT (terminal): the data we received was corrupt
-         state=DEAD (terminal): the connection has failed
-        """
-
-
-# it'd be nice if we receive the hashes before the block, or just
-# afterwards, so we aren't stuck holding on to unvalidated blocks
-# that we can't process. If we guess the offsets right, we can
-# accomplish this by sending the block request after the metadata
-# requests (by keeping two separate requestlists), and have a one RTT
-# pipeline like:
-#  1a=metadata, 1b=block
-#  1b->process+deliver : one RTT
-
-# But if we guess wrong, and fetch the wrong part of the block, we'll
-# have a pipeline that looks like:
-#  1a=wrong metadata, 1b=wrong block
-#  1a->2a=right metadata,2b=right block
-#  2b->process+deliver
-# which means two RTT and buffering one block (which, since we'll
-# guess the segsize wrong for everything, means buffering one
-# segment)
-
-# if we start asking for multiple segments, we could get something
-# worse:
-#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
-#  1a->2a=right metadata,2b=right block0,2c=right block1, .
-#  2b->process+deliver
-
-# which means two RTT but fetching and buffering the whole file
-# before delivering anything. However, since we don't know when the
-# other shares are going to arrive, we need to avoid having more than
-# one block in the pipeline anyways. So we shouldn't be able to get
-# into this state.
-
-# it also means that, instead of handling all of
-# self._requested_blocks at once, we should only be handling one
-# block at a time: one of the requested block should be special
-# (probably FIFO). But retire all we can.
-
-    # this might be better with a Deferred, using COMPLETE as the success
-    # case and CORRUPT/DEAD in an errback, because that would let us hold the
-    # 'share' and 'shnum' arguments locally (instead of roundtripping them
-    # through Share.send_request). But that OVERDUE is not terminal. So I
-    # want a new sort of callback mechanism, with the extra-argument-passing
-    # aspects of Deferred, but without being so one-shot. Is this a job for
-    # Observer? No, it doesn't take extra arguments. So this uses Observer2.
-
-
-class Reader:
-    """I am responsible for a single offset+size read of the file. I handle
-    segmentation: I figure out which segments are necessary, request them
-    (from my CiphertextDownloader) in order, and trim the segments down to
-    match the offset+size span. I use the Producer/Consumer interface to only
-    request one segment at a time.
-    """
-    implements(IPushProducer)
-    def __init__(self, consumer, offset, size):
-        self._needed = []
-        self._consumer = consumer
-        self._hungry = False
-        self._offset = offset
-        self._size = size
-        self._segsize = None
-    def start(self):
-        self._alive = True
-        self._deferred = defer.Deferred()
-        # the process doesn't actually start until set_segment_size()
-        return self._deferred
-
-    def set_segment_size(self, segsize):
-        if self._segsize is not None:
-            return
-        self._segsize = segsize
-        self._compute_segnums()
-
-    def _compute_segnums(self, segsize):
-        # now that we know the file's segsize, what segments (and which
-        # ranges of each) will we need?
-        size = self._size
-        offset = self._offset
-        while size:
-            assert size >= 0
-            this_seg_num = int(offset / self._segsize)
-            this_seg_offset = offset - (seg_num*self._segsize)
-            this_seg_size = min(size, self._segsize-seg_offset)
-            size -= this_seg_size
-            if size:
-                offset += this_seg_size
-            yield (this_seg_num, this_seg_offset, this_seg_size)
-
-    def get_needed_segments(self):
-        return set([segnum for (segnum, off, size) in self._needed])
-
-
-    def stopProducing(self):
-        self._hungry = False
-        self._alive = False
-        # TODO: cancel the segment requests
-    def pauseProducing(self):
-        self._hungry = False
-    def resumeProducing(self):
-        self._hungry = True
-    def add_segment(self, segnum, offset, size):
-        self._needed.append( (segnum, offset, size) )
-    def got_segment(self, segnum, segdata):
-        """Return True if this schedule has more to go, or False if it is
-        done."""
-        assert self._needed[0][segnum] == segnum
-        (_ign, offset, size) = self._needed.pop(0)
-        data = segdata[offset:offset+size]
-        self._consumer.write(data)
-        if not self._needed:
-            # we're done
-            self._alive = False
-            self._hungry = False
-            self._consumer.unregisterProducer()
-            self._deferred.callback(self._consumer)
-    def error(self, f):
-        self._alive = False
-        self._hungry = False
-        self._consumer.unregisterProducer()
-        self._deferred.errback(f)
-
-
-
-class x:
-    def OFFread(self, consumer, offset=0, size=None):
-        """I am the main entry point, from which FileNode.read() can get
-        data."""
-        # tolerate concurrent operations: each gets its own Reader
-        if size is None:
-            size = self._size - offset
-        r = Reader(consumer, offset, size)
-        self._readers.add(r)
-        d = r.start()
-        if self.segment_size is not None:
-            r.set_segment_size(self.segment_size)
-            # TODO: if we can't find any segments, and thus never get a
-            # segsize, tell the Readers to give up
-        return d
diff --git a/src/allmydata/immutable/download2_off.pyOFF b/src/allmydata/immutable/download2_off.pyOFF
new file mode 100755
index 0000000..d2b8b99
--- /dev/null
+++ b/src/allmydata/immutable/download2_off.pyOFF
@@ -0,0 +1,634 @@
+#! /usr/bin/python
+
+# known (shnum,Server) pairs are sorted into a list according to
+# desireability. This sort is picking a winding path through a matrix of
+# [shnum][server]. The goal is to get diversity of both shnum and server.
+
+# The initial order is:
+#  find the lowest shnum on the first server, add it
+#  look at the next server, find the lowest shnum that we don't already have
+#   if any
+#  next server, etc, until all known servers are checked
+#  now look at servers that we skipped (because ...
+
+# Keep track of which block requests are outstanding by (shnum,Server). Don't
+# bother prioritizing "validated" shares: the overhead to pull the share hash
+# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
+# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
+# 832 bytes). Each time a block request is sent, also request any necessary
+# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
+# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
+
+# For each outstanding segread, walk the list and send requests (skipping
+# outstanding shnums) until requests for k distinct shnums are in flight. If
+# we can't do that, ask for more. If we get impatient on a request, find the
+# first non-outstanding
+
+# start with the first Share in the list, and send a request. Then look at
+# the next one. If we already have a pending request for the same shnum or
+# server, push that Share down onto the fallback list and try the next one,
+# etc. If we run out of non-fallback shares, use the fallback ones,
+# preferring shnums that we don't have outstanding requests for (i.e. assume
+# that all requests will complete). Do this by having a second fallback list.
+
+# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
+# per file, not thousands.
+
+# actually, don't bother sorting the initial list. Append Shares as the
+# responses come back, that will put the fastest servers at the front of the
+# list, and give a tiny preference to servers that are earlier in the
+# permuted order.
+
+# more ideas:
+#  sort shares by:
+#   1: number of roundtrips needed to get some data
+#   2: share number
+#   3: ms of RTT delay
+# maybe measure average time-to-completion of requests, compare completion
+# time against that, much larger indicates congestion on the server side
+# or the server's upstream speed is less than our downstream. Minimum
+# time-to-completion indicates min(our-downstream,their-upstream). Could
+# fetch shares one-at-a-time to measure that better.
+
+# when should we risk duplicate work and send a new request?
+
+def walk(self):
+    shares = sorted(list)
+    oldshares = copy(shares)
+    outstanding = list()
+    fallbacks = list()
+    second_fallbacks = list()
+    while len(outstanding.nonlate.shnums) < k: # need more requests
+        while oldshares:
+            s = shares.pop(0)
+            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
+                fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # must use fallback list. Ask for more servers while we're at it.
+        ask_for_more_servers()
+        while fallbacks:
+            s = fallbacks.pop(0)
+            if s.shnum in outstanding.shnums:
+                # assume that the outstanding requests will complete, but
+                # send new requests for other shnums to existing servers
+                second_fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # if we get here, we're being forced to send out multiple queries per
+        # share. We've already asked for more servers, which might help. If
+        # there are no late outstanding queries, then duplicate shares won't
+        # help. Don't send queries for duplicate shares until some of the
+        # queries are late.
+        if outstanding.late:
+            # we're allowed to try any non-outstanding share
+            while second_fallbacks:
+                pass
+    newshares = outstanding + fallbacks + second_fallbacks + oldshares
+        
+
+class Server:
+    """I represent an abstract Storage Server. One day, the StorageBroker
+    will return instances of me. For now, the StorageBroker returns (peerid,
+    RemoteReference) tuples, and this code wraps a Server instance around
+    them.
+    """
+    def __init__(self, peerid, ss):
+        self.peerid = peerid
+        self.remote = ss
+        self._remote_buckets = {} # maps shnum to RIBucketReader
+        # TODO: release the bucket references on shares that we no longer
+        # want. OTOH, why would we not want them? Corruption?
+
+    def send_query(self, storage_index):
+        """I return a Deferred that fires with a set of shnums. If the server
+        had shares available, I will retain the RemoteReferences to its
+        buckets, so that get_data(shnum, range) can be called later."""
+        d = self.remote.callRemote("get_buckets", self.storage_index)
+        d.addCallback(self._got_response)
+        return d
+
+    def _got_response(self, r):
+        self._remote_buckets = r
+        return set(r.keys())
+
+class ShareOnAServer:
+    """I represent one instance of a share, known to live on a specific
+    server. I am created every time a server responds affirmatively to a
+    do-you-have-block query."""
+
+    def __init__(self, shnum, server):
+        self._shnum = shnum
+        self._server = server
+        self._block_hash_tree = None
+
+    def cost(self, segnum):
+        """I return a tuple of (roundtrips, bytes, rtt), indicating how
+        expensive I think it would be to fetch the given segment. Roundtrips
+        indicates how many roundtrips it is likely to take (one to get the
+        data and hashes, plus one to get the offset table and UEB if this is
+        the first segment we've ever fetched). 'bytes' is how many bytes we
+        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
+        seconds for a trivial request. The downloading algorithm will compare
+        costs to decide which shares should be used."""
+        # the most significant factor here is roundtrips: a Share for which
+        # we already have the offset table is better to than a brand new one
+
+    def max_bandwidth(self):
+        """Return a float, indicating the highest plausible bytes-per-second
+        that I've observed coming from this share. This will be based upon
+        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
+        can we used to estimate the server's upstream bandwidth. Clearly this
+        is only accurate if a share is retrieved with no contention for
+        either the upstream, downstream, or middle of the connection, but it
+        may still serve as a useful metric for deciding which servers to pull
+        from."""
+
+    def get_segment(self, segnum):
+        """I return a Deferred that will fire with the segment data, or
+        errback."""
+
+class NativeShareOnAServer(ShareOnAServer):
+    """For tahoe native (foolscap) servers, I contain a RemoteReference to
+    the RIBucketReader instance."""
+    def __init__(self, shnum, server, rref):
+        ShareOnAServer.__init__(self, shnum, server)
+        self._rref = rref # RIBucketReader
+
+class Share:
+    def __init__(self, shnum):
+        self._shnum = shnum
+        # _servers are the Server instances which appear to hold a copy of
+        # this share. It is populated when the ValidShare is first created,
+        # or when we receive a get_buckets() response for a shnum that
+        # already has a ValidShare instance. When we lose the connection to a
+        # server, we remove it.
+        self._servers = set()
+        # offsets, UEB, and share_hash_tree all live in the parent.
+        # block_hash_tree lives here.
+        self._block_hash_tree = None
+
+        self._want
+
+    def get_servers(self):
+        return self._servers
+
+
+    def get_block(self, segnum):
+        # read enough data to obtain a single validated block
+        if not self.have_offsets:
+            # we get the offsets in their own read, since they tell us where
+            # everything else lives. We must fetch offsets for each share
+            # separately, since they aren't directly covered by the UEB.
+            pass
+        if not self.parent.have_ueb:
+            # use _guessed_segsize to make a guess about the layout, so we
+            # can fetch both the offset table and the UEB in the same read.
+            # This also requires making a guess about the presence or absence
+            # of the plaintext_hash_tree. Oh, and also the version number. Oh
+            # well.
+            pass
+
+class CiphertextDownloader:
+    """I manage all downloads for a single file. I operate a state machine
+    with input events that are local read() requests, responses to my remote
+    'get_bucket' and 'read_bucket' messages, and connection establishment and
+    loss. My outbound events are connection establishment requests and bucket
+    read requests messages.
+    """
+    # eventually this will merge into the FileNode
+    ServerClass = Server # for tests to override
+
+    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
+                 shutdowner):
+        # values we get from the filecap
+        self._storage_index = si = storage_index
+        self._ueb_hash = ueb_hash
+        self._size = size
+        self._needed_shares = k
+        self._total_shares = N
+        self._share_hash_tree = IncompleteHashTree(self._total_shares)
+        # values we discover when we first fetch the UEB
+        self._ueb = None # is dict after UEB fetch+validate
+        self._segsize = None
+        self._numsegs = None
+        self._blocksize = None
+        self._tail_segsize = None
+        self._ciphertext_hash = None # optional
+        # structures we create when we fetch the UEB, then continue to fill
+        # as we download the file
+        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
+        self._ciphertext_hash_tree = None
+
+        # values we learn as we download the file
+        self._offsets = {} # (shnum,Server) to offset table (dict)
+        self._block_hash_tree = {} # shnum to IncompleteHashTree
+        # other things which help us
+        self._guessed_segsize = min(128*1024, size)
+        self._active_share_readers = {} # maps shnum to Reader instance
+        self._share_readers = [] # sorted by preference, best first
+        self._readers = set() # set of Reader instances
+        self._recent_horizon = 10 # seconds
+
+        # 'shutdowner' is a MultiService parent used to cancel all downloads
+        # when the node is shutting down, to let tests have a clean reactor.
+
+        self._init_available_servers()
+        self._init_find_enough_shares()
+
+    # _available_servers is an iterator that provides us with Server
+    # instances. Each time we pull out a Server, we immediately send it a
+    # query, so we don't need to keep track of who we've sent queries to.
+
+    def _init_available_servers(self):
+        self._available_servers = self._get_available_servers()
+        self._no_more_available_servers = False
+
+    def _get_available_servers(self):
+        """I am a generator of servers to use, sorted by the order in which
+        we should query them. I make sure there are no duplicates in this
+        list."""
+        # TODO: make StorageBroker responsible for this non-duplication, and
+        # replace this method with a simple iter(get_servers_for_index()),
+        # plus a self._no_more_available_servers=True
+        seen = set()
+        sb = self._storage_broker
+        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
+            if peerid not in seen:
+                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
+                seen.add(peerid)
+        self._no_more_available_servers = True
+
+    # this block of code is responsible for having enough non-problematic
+    # distinct shares/servers available and ready for download, and for
+    # limiting the number of queries that are outstanding. The idea is that
+    # we'll use the k fastest/best shares, and have the other ones in reserve
+    # in case those servers stop responding or respond too slowly. We keep
+    # track of all known shares, but we also keep track of problematic shares
+    # (ones with hash failures or lost connections), so we can put them at
+    # the bottom of the list.
+
+    def _init_find_enough_shares(self):
+        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
+        # where viable (but not yet validated) shares are located. Each
+        # get_bucket() response adds to this map, each act of validation
+        # removes from it.
+        self._sharemap = DictOfSets()
+
+        # _sharemap maps shnum to set of Servers, and remembers where viable
+        # shares are located. Each get_bucket() response adds to this map,
+        # each hash failure or disconnect removes from it. (TODO: if we
+        # disconnect but reconnect later, we should be allowed to re-query).
+        self._sharemap = DictOfSets()
+
+        # _problem_shares is a set of (shnum, Server) tuples, and
+
+        # _queries_in_flight maps a Server to a timestamp, which remembers
+        # which servers we've sent queries to (and when) but have not yet
+        # heard a response. This lets us put a limit on the number of
+        # outstanding queries, to limit the size of the work window (how much
+        # extra work we ask servers to do in the hopes of keeping our own
+        # pipeline filled). We remove a Server from _queries_in_flight when
+        # we get an answer/error or we finally give up. If we ever switch to
+        # a non-connection-oriented protocol (like UDP, or forwarded Chord
+        # queries), we can use this information to retransmit any query that
+        # has gone unanswered for too long.
+        self._queries_in_flight = dict()
+
+    def _count_recent_queries_in_flight(self):
+        now = time.time()
+        recent = now - self._recent_horizon
+        return len([s for (s,when) in self._queries_in_flight.items()
+                    if when > recent])
+
+    def _find_enough_shares(self):
+        # goal: have 2*k distinct not-invalid shares available for reading,
+        # from 2*k distinct servers. Do not have more than 4*k "recent"
+        # queries in flight at a time.
+        if (len(self._sharemap) >= 2*self._needed_shares
+            and len(self._sharemap.values) >= 2*self._needed_shares):
+            return
+        num = self._count_recent_queries_in_flight()
+        while num < 4*self._needed_shares:
+            try:
+                s = self._available_servers.next()
+            except StopIteration:
+                return # no more progress can be made
+            self._queries_in_flight[s] = time.time()
+            d = s.send_query(self._storage_index)
+            d.addBoth(incidentally, self._queries_in_flight.discard, s)
+            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
+                                           for shnum in shnums],
+                           lambda f: self._query_error(f, s))
+            d.addErrback(self._error)
+            d.addCallback(self._reschedule)
+            num += 1
+
+    def _query_error(self, f, s):
+        # a server returned an error, log it gently and ignore
+        level = log.WEIRD
+        if f.check(DeadReferenceError):
+            level = log.UNUSUAL
+        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
+                failure=f, level=level, umid="3uuBUQ")
+
+    # this block is responsible for turning known shares into usable shares,
+    # by fetching enough data to validate their contents.
+
+    # UEB (from any share)
+    # share hash chain, validated (from any share, for given shnum)
+    # block hash (any share, given shnum)
+
+    def _got_ueb(self, ueb_data, share):
+        if self._ueb is not None:
+            return
+        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
+            share.error("UEB hash does not match")
+            return
+        d = uri.unpack_extension(ueb_data)
+        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
+
+
+        # There are several kinds of things that can be found in a UEB.
+        # First, things that we really need to learn from the UEB in order to
+        # do this download. Next: things which are optional but not redundant
+        # -- if they are present in the UEB they will get used. Next, things
+        # that are optional and redundant. These things are required to be
+        # consistent: they don't have to be in the UEB, but if they are in
+        # the UEB then they will be checked for consistency with the
+        # already-known facts, and if they are inconsistent then an exception
+        # will be raised. These things aren't actually used -- they are just
+        # tested for consistency and ignored. Finally: things which are
+        # deprecated -- they ought not be in the UEB at all, and if they are
+        # present then a warning will be logged but they are otherwise
+        # ignored.
+
+        # First, things that we really need to learn from the UEB:
+        # segment_size, crypttext_root_hash, and share_root_hash.
+        self._segsize = d['segment_size']
+
+        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
+        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
+
+        self._tail_segsize = self._size % self._segsize
+        if self._tail_segsize == 0:
+            self._tail_segsize = self._segsize
+        # padding for erasure code
+        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
+                                                    self._needed_shares)
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B.
+        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
+        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
+
+        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
+
+
+        # Next: things that are optional and not redundant: crypttext_hash
+        if 'crypttext_hash' in d:
+            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
+                self._ciphertext_hash = d['crypttext_hash']
+            else:
+                log.msg("ignoring bad-length UEB[crypttext_hash], "
+                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
+                                                   hashutil.CRYPTO_VAL_SIZE),
+                        umid="oZkGLA", level=log.WEIRD)
+
+        # we ignore all of the redundant fields when downloading. The
+        # Verifier uses a different code path which does not ignore them.
+
+        # finally, set self._ueb as a marker that we don't need to request it
+        # anymore
+        self._ueb = d
+
+    def _got_share_hashes(self, hashes, share):
+        assert isinstance(hashes, dict)
+        try:
+            self._share_hash_tree.set_hashes(hashes)
+        except (IndexError, BadHashError, NotEnoughHashesError), le:
+            share.error("Bad or missing hashes")
+            return
+
+    #def _got_block_hashes(
+
+    def _init_validate_enough_shares(self):
+        # _valid_shares maps shnum to ValidatedShare instances, and is
+        # populated once the block hash root has been fetched and validated
+        # (which requires any valid copy of the UEB, and a valid copy of the
+        # share hash chain for each shnum)
+        self._valid_shares = {}
+
+        # _target_shares is an ordered list of ReadyShare instances, each of
+        # which is a (shnum, server) tuple. It is sorted in order of
+        # preference: we expect to get the fastest response from the
+        # ReadyShares at the front of the list. It is also sorted to
+        # distribute the shnums, so that fetching shares from
+        # _target_shares[:k] is likely (but not guaranteed) to give us k
+        # distinct shares. The rule is that we skip over entries for blocks
+        # that we've already received, limit the number of recent queries for
+        # the same block, 
+        self._target_shares = []
+
+    def _validate_enough_shares(self):
+        # my goal is to have at least 2*k distinct validated shares from at
+        # least 2*k distinct servers
+        valid_share_servers = set()
+        for vs in self._valid_shares.values():
+            valid_share_servers.update(vs.get_servers())
+        if (len(self._valid_shares) >= 2*self._needed_shares
+            and len(self._valid_share_servers) >= 2*self._needed_shares):
+            return
+        #for 
+
+    def _reschedule(self, _ign):
+        # fire the loop again
+        if not self._scheduled:
+            self._scheduled = True
+            eventually(self._loop)
+
+    def _loop(self):
+        self._scheduled = False
+        # what do we need?
+
+        self._find_enough_shares()
+        self._validate_enough_shares()
+
+        if not self._ueb:
+            # we always need a copy of the UEB
+            pass
+
+    def _error(self, f):
+        # this is an unexpected error: a coding bug
+        log.err(f, level=log.UNUSUAL)
+            
+
+
+# using a single packed string (and an offset table) may be an artifact of
+# our native storage server: other backends might allow cheap multi-part
+# files (think S3, several buckets per share, one for each section).
+
+# find new names for:
+#  data_holder
+#  Share / Share2  (ShareInstance / Share? but the first is more useful)
+
+class IShare(Interface):
+    """I represent a single instance of a single share (e.g. I reference the
+    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
+    This interface is used by SegmentFetcher to retrieve validated blocks.
+    """
+    def get_block(segnum):
+        """Return an Observer2, which will be notified with the following
+        events:
+         state=COMPLETE, block=data (terminal): validated block data
+         state=OVERDUE (non-terminal): we have reason to believe that the
+                                       request might have stalled, or we
+                                       might just be impatient
+         state=CORRUPT (terminal): the data we received was corrupt
+         state=DEAD (terminal): the connection has failed
+        """
+
+
+# it'd be nice if we receive the hashes before the block, or just
+# afterwards, so we aren't stuck holding on to unvalidated blocks
+# that we can't process. If we guess the offsets right, we can
+# accomplish this by sending the block request after the metadata
+# requests (by keeping two separate requestlists), and have a one RTT
+# pipeline like:
+#  1a=metadata, 1b=block
+#  1b->process+deliver : one RTT
+
+# But if we guess wrong, and fetch the wrong part of the block, we'll
+# have a pipeline that looks like:
+#  1a=wrong metadata, 1b=wrong block
+#  1a->2a=right metadata,2b=right block
+#  2b->process+deliver
+# which means two RTT and buffering one block (which, since we'll
+# guess the segsize wrong for everything, means buffering one
+# segment)
+
+# if we start asking for multiple segments, we could get something
+# worse:
+#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
+#  1a->2a=right metadata,2b=right block0,2c=right block1, .
+#  2b->process+deliver
+
+# which means two RTT but fetching and buffering the whole file
+# before delivering anything. However, since we don't know when the
+# other shares are going to arrive, we need to avoid having more than
+# one block in the pipeline anyways. So we shouldn't be able to get
+# into this state.
+
+# it also means that, instead of handling all of
+# self._requested_blocks at once, we should only be handling one
+# block at a time: one of the requested block should be special
+# (probably FIFO). But retire all we can.
+
+    # this might be better with a Deferred, using COMPLETE as the success
+    # case and CORRUPT/DEAD in an errback, because that would let us hold the
+    # 'share' and 'shnum' arguments locally (instead of roundtripping them
+    # through Share.send_request). But that OVERDUE is not terminal. So I
+    # want a new sort of callback mechanism, with the extra-argument-passing
+    # aspects of Deferred, but without being so one-shot. Is this a job for
+    # Observer? No, it doesn't take extra arguments. So this uses Observer2.
+
+
+class Reader:
+    """I am responsible for a single offset+size read of the file. I handle
+    segmentation: I figure out which segments are necessary, request them
+    (from my CiphertextDownloader) in order, and trim the segments down to
+    match the offset+size span. I use the Producer/Consumer interface to only
+    request one segment at a time.
+    """
+    implements(IPushProducer)
+    def __init__(self, consumer, offset, size):
+        self._needed = []
+        self._consumer = consumer
+        self._hungry = False
+        self._offset = offset
+        self._size = size
+        self._segsize = None
+    def start(self):
+        self._alive = True
+        self._deferred = defer.Deferred()
+        # the process doesn't actually start until set_segment_size()
+        return self._deferred
+
+    def set_segment_size(self, segsize):
+        if self._segsize is not None:
+            return
+        self._segsize = segsize
+        self._compute_segnums()
+
+    def _compute_segnums(self, segsize):
+        # now that we know the file's segsize, what segments (and which
+        # ranges of each) will we need?
+        size = self._size
+        offset = self._offset
+        while size:
+            assert size >= 0
+            this_seg_num = int(offset / self._segsize)
+            this_seg_offset = offset - (seg_num*self._segsize)
+            this_seg_size = min(size, self._segsize-seg_offset)
+            size -= this_seg_size
+            if size:
+                offset += this_seg_size
+            yield (this_seg_num, this_seg_offset, this_seg_size)
+
+    def get_needed_segments(self):
+        return set([segnum for (segnum, off, size) in self._needed])
+
+
+    def stopProducing(self):
+        self._hungry = False
+        self._alive = False
+        # TODO: cancel the segment requests
+    def pauseProducing(self):
+        self._hungry = False
+    def resumeProducing(self):
+        self._hungry = True
+    def add_segment(self, segnum, offset, size):
+        self._needed.append( (segnum, offset, size) )
+    def got_segment(self, segnum, segdata):
+        """Return True if this schedule has more to go, or False if it is
+        done."""
+        assert self._needed[0][segnum] == segnum
+        (_ign, offset, size) = self._needed.pop(0)
+        data = segdata[offset:offset+size]
+        self._consumer.write(data)
+        if not self._needed:
+            # we're done
+            self._alive = False
+            self._hungry = False
+            self._consumer.unregisterProducer()
+            self._deferred.callback(self._consumer)
+    def error(self, f):
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
+
+
+
+class x:
+    def OFFread(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data."""
+        # tolerate concurrent operations: each gets its own Reader
+        if size is None:
+            size = self._size - offset
+        r = Reader(consumer, offset, size)
+        self._readers.add(r)
+        d = r.start()
+        if self.segment_size is not None:
+            r.set_segment_size(self.segment_size)
+            # TODO: if we can't find any segments, and thus never get a
+            # segsize, tell the Readers to give up
+        return d

commit eaf3f28dd07e7799c88e1fd06cf9761fb118efde
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 11:17:03 2010 -0700

    new tool to check uniqueness of umids
---
 Makefile            |    2 ++
 misc/check-umids.py |   30 ++++++++++++++++++++++++++++++
 2 files changed, 32 insertions(+), 0 deletions(-)

diff --git a/Makefile b/Makefile
index 441cd32..efb91df 100644
--- a/Makefile
+++ b/Makefile
@@ -178,6 +178,8 @@ endif
 
 pyflakes:
 	$(PYTHON) -OOu `which pyflakes` src/allmydata |sort |uniq
+check-umids:
+	$(PYTHON) misc/check-umids.py `find src/allmydata -name '*.py'`
 
 count-lines:
 	@echo -n "files: "
diff --git a/misc/check-umids.py b/misc/check-umids.py
new file mode 100755
index 0000000..05e8825
--- /dev/null
+++ b/misc/check-umids.py
@@ -0,0 +1,30 @@
+#! /usr/bin/python
+
+# ./rumid.py foo.py
+
+import sys, re, os
+
+ok = True
+umids = {}
+
+for fn in sys.argv[1:]:
+    fn = os.path.abspath(fn)
+    for lineno,line in enumerate(open(fn, "r").readlines()):
+        lineno = lineno+1
+        if "umid" not in line:
+            continue
+        mo = re.search("umid=[\"\']([^\"\']+)[\"\']", line)
+        if mo:
+            umid = mo.group(1)
+            if umid in umids:
+                oldfn, oldlineno = umids[umid]
+                print "%s:%d: duplicate umid '%s'" % (fn, lineno, umid)
+                print "%s:%d: first used here" % (oldfn, oldlineno)
+                ok = False
+            umids[umid] = (fn,lineno)
+
+if ok:
+    print "all umids are unique"
+else:
+    print "some umids were duplicates"
+    sys.exit(1)

commit 9cddde4f40375fcfd08b0c22c23fa40784ee1a5e
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 11:16:44 2010 -0700

    checker: give it a unique umid
---
 src/allmydata/immutable/checker.py |    4 +++-
 1 files changed, 3 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/checker.py b/src/allmydata/immutable/checker.py
index 2f2d8f1..31c70e3 100644
--- a/src/allmydata/immutable/checker.py
+++ b/src/allmydata/immutable/checker.py
@@ -85,7 +85,9 @@ class Checker(log.PrefixingLogMixin):
             level = log.WEIRD
             if f.check(DeadReferenceError):
                 level = log.UNUSUAL
-            self.log("failure from server on 'get_buckets' the REMOTE failure was:", facility="tahoe.immutable.checker", failure=f, level=level, umid="3uuBUQ")
+            self.log("failure from server on 'get_buckets' the REMOTE failure was:",
+                     facility="tahoe.immutable.checker",
+                     failure=f, level=level, umid="AX7wZQ")
             return ({}, serverid, False)
 
         d.addCallbacks(_wrap_results, _trap_errs)

commit 9b633d6b7720ff4cfa2c7e926024ca523b0693b5
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 10:05:42 2010 -0700

    notes on more tests to write
---
 src/allmydata/immutable/download2.py |   17 ++++++++++-------
 1 files changed, 10 insertions(+), 7 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 012f884..7131b21 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1978,10 +1978,13 @@ class ImmutableFileNode:
 # variables in the CopiedFailure
 
 # tests to write:
-#  two-segment file, tweak default-segsize so we guess right, start
-#  simultaneous reads of both segments. goal is to cover Share.get_block
-#  in the for(self._requested_blocks) loop.
-#
-#  to cover deeper in Share.get_block for loop, we need to bypass Node.read
-#  and call Share.get_block directly (Nodes and SegmentFetchers will avoid
-#  doing multiple get_blocks)
+# * two-segment file, tweak default-segsize so we guess right, start
+#   simultaneous reads of both segments. goal is to cover Share.get_block
+#   in the for(self._requested_blocks) loop.
+# * server with _overrun_ok=False, to exercise the non-trivial [:1024] path
+#   inside Share._desire_offsets
+# * truncated share, so _satisfy_* doesn't get all it wants
+# * v2 share, exercise large-offset-table code
+# * slow server
+# * hash failures of all sorts
+# * NoSharesError

commit fa84ed02293eccac1fadb15415740a1bae00da57
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 10:05:37 2010 -0700

    handle wrong-guess correctly, instead of bailing
---
 src/allmydata/immutable/download2.py |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 45917e3..012f884 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1215,7 +1215,7 @@ class Segmentation:
         # the overlap is file[o[0]:o[0]+o[1]]
         if not o or o[0] != self._offset:
             # we didn't get the first byte, so we can't use this segment
-            if self._node.segment_size is not None:
+            if had_actual_segment_size:
                 # and we should have gotten it right. This is big problem.
                 log.msg("Segmentation handed wrong data (but we knew better):"
                         " want [%d-%d), given [%d-%d), for segnum=%d,"

commit e5062159ea61f409573b2c6e4bd573b9d1df65ee
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 10:05:14 2010 -0700

    make it possible to override maxsegsize to influence the guess, for unit tests.
---
 src/allmydata/immutable/download2.py |   27 ++++++++++++++++-----------
 src/allmydata/immutable/upload.py    |    6 ++++--
 src/allmydata/interfaces.py          |    3 +++
 3 files changed, 23 insertions(+), 13 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 031c4ab..45917e3 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -9,7 +9,7 @@ from twisted.internet.interfaces import IPushProducer, IConsumer
 
 from foolscap.api import eventually
 from allmydata.interfaces import IImmutableFileNode, IUploadResults, \
-     NotEnoughSharesError, NoSharesError, HASH_SIZE
+     NotEnoughSharesError, NoSharesError, HASH_SIZE, DEFAULT_MAX_SEGMENT_SIZE
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 from allmydata.util import base32, log, hashutil, mathutil, idlib
@@ -1307,16 +1307,11 @@ class _Node:
         self.share_hash_tree = IncompleteHashTree(N)
 
         # we guess the segment size, so Segmentation can pull non-initial
-        # segments in a single roundtrip
-        max_segment_size = 128*KiB # TODO: pull from elsewhere, maybe the
-                                   # same place as upload.BaseUploadable
-        s = mathutil.next_multiple(min(verifycap.size, max_segment_size), k)
-        self.guessed_segment_size = s
-        r = self._calculate_sizes(self.guessed_segment_size)
-        self.guessed_num_segments = r["num_segments"]
-        # as with CommonShare, our ciphertext_hash_tree is a stub until we
-        # get the real num_segments
-        self.ciphertext_hash_tree = IncompleteHashTree(self.guessed_num_segments)
+        # segments in a single roundtrip. This populates
+        # .guessed_segment_size, .guessed_num_segments, and
+        # .ciphertext_hash_tree (with a dummy, to let us guess which hashes
+        # we'll need)
+        self._build_guessed_tables(DEFAULT_MAX_SEGMENT_SIZE)
 
         # filled in when we parse a valid UEB
         self.have_UEB = False
@@ -1351,6 +1346,16 @@ class _Node:
         self._sharefinder = ShareFinder(storage_broker, verifycap, self, lp)
         self._shares = set()
 
+    def _build_guessed_tables(self, max_segment_size):
+        size = min(self._verifycap.size, max_segment_size)
+        s = mathutil.next_multiple(size, self._verifycap.needed_shares)
+        self.guessed_segment_size = s
+        r = self._calculate_sizes(self.guessed_segment_size)
+        self.guessed_num_segments = r["num_segments"]
+        # as with CommonShare, our ciphertext_hash_tree is a stub until we
+        # get the real num_segments
+        self.ciphertext_hash_tree = IncompleteHashTree(self.guessed_num_segments)
+
     def __repr__(self):
         return "Imm_Node(%s)" % (self._si_prefix,)
 
diff --git a/src/allmydata/immutable/upload.py b/src/allmydata/immutable/upload.py
index ca7d56b..7ac86c2 100644
--- a/src/allmydata/immutable/upload.py
+++ b/src/allmydata/immutable/upload.py
@@ -20,7 +20,8 @@ from allmydata.util.assertutil import precondition
 from allmydata.util.rrefutil import add_version_to_remote_reference
 from allmydata.interfaces import IUploadable, IUploader, IUploadResults, \
      IEncryptedUploadable, RIEncryptedUploadable, IUploadStatus, \
-     NoServersError, InsufficientVersionError, UploadUnhappinessError
+     NoServersError, InsufficientVersionError, UploadUnhappinessError, \
+     DEFAULT_MAX_SEGMENT_SIZE
 from allmydata.immutable import layout
 from pycryptopp.cipher.aes import AES
 
@@ -1170,7 +1171,8 @@ class AssistedUploader:
         return self._upload_status
 
 class BaseUploadable:
-    default_max_segment_size = 128*KiB # overridden by max_segment_size
+    # this is overridden by max_segment_size
+    default_max_segment_size = DEFAULT_MAX_SEGMENT_SIZE
     default_encoding_param_k = 3 # overridden by encoding_parameters
     default_encoding_param_happy = 7
     default_encoding_param_n = 10
diff --git a/src/allmydata/interfaces.py b/src/allmydata/interfaces.py
index 4cfe9c9..3a7fa7f 100644
--- a/src/allmydata/interfaces.py
+++ b/src/allmydata/interfaces.py
@@ -24,6 +24,9 @@ WriteEnablerSecret = Hash # used to protect mutable bucket modifications
 LeaseRenewSecret = Hash # used to protect bucket lease renewal requests
 LeaseCancelSecret = Hash # used to protect bucket lease cancellation requests
 
+KiB = 1024
+DEFAULT_MAX_SEGMENT_SIZE = 128*KiB
+
 class RIStubClient(RemoteInterface):
     """Each client publishes a service announcement for a dummy object called
     the StubClient. This object doesn't actually offer any services, but the

commit 45f4898e4ddece43f089ab4407ab5603a324ad4d
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 10:02:08 2010 -0700

    improve log messages
---
 src/allmydata/immutable/download2.py |   14 ++++++++------
 1 files changed, 8 insertions(+), 6 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index ea99c43..031c4ab 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -493,9 +493,8 @@ class Share:
         block = self._received.pop(blockstart, blocklen)
         if not block:
             return False
-        log.msg(format="%(share)s._satisfy_data_block, len(block)=%(blocklen)d",
-                share=repr(self),
-                blocklen=len(block),
+        log.msg(format="%(share)s._satisfy_data_block [%(start)d:+%(length)d]",
+                share=repr(self), start=blockstart, length=blocklen,
                 level=log.NOISY, parent=self._lp, umid="uTDNZg")
         # we removed the block from _received, but don't retain the data in
         # our Node or CommonShare, so also remove it from _requested: this
@@ -652,7 +651,7 @@ class Share:
             return
         log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
                 share=repr(self), start=start, length=length, datalen=len(data),
-                level=log.NOISY, parent=lp, umid="sgVAyA")
+                level=log.NOISY, parent=lp, umid="5Qn6VQ")
         span = (start, length)
         assert span in self._requested # XXX eh, not important
         self._received.add(start, data)
@@ -1172,6 +1171,9 @@ class Segmentation:
             return
         n = self._node
         have_actual_segment_size = n.segment_size is not None
+        guess_s = ""
+        if not have_actual_segment_size:
+            guess_s = "probably "
         segment_size = n.segment_size or n.guessed_segment_size
         if self._offset == 0:
             # great! we want segment0 for sure
@@ -1179,8 +1181,8 @@ class Segmentation:
         else:
             # this might be a guess
             wanted_segnum = self._offset // segment_size
-        log.msg(format="_fetch_next(offset=%(offset)d) wants segnum=%(segnum)d",
-                offset=self._offset, segnum=wanted_segnum,
+        log.msg(format="_fetch_next(offset=%(offset)d) %(guess)swants segnum=%(segnum)d",
+                offset=self._offset, guess=guess_s, segnum=wanted_segnum,
                 level=log.NOISY, parent=self._lp, umid="5WfN0w")
         self._active_segnum = wanted_segnum
         d,c = n.get_segment(wanted_segnum, self._lp)

commit a870b048c4ca24d9cc8afc6b7ce75696ad1e00ca
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 10:01:37 2010 -0700

    remove unused _active_segnum method
---
 src/allmydata/immutable/download2.py |    5 -----
 1 files changed, 0 insertions(+), 5 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 3578aa0..ea99c43 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -196,11 +196,6 @@ class Share:
         self._requested_blocks = new_requests
 
     # internal methods
-    def _active_segnum(self):
-        if self._requested_blocks:
-            return self._requested_blocks[0]
-        return None
-
     def _active_segnum_and_observers(self):
         if self._requested_blocks:
             # we only retrieve information for one segment at a time, to

commit 2ffde79b165c8becaa268c8743d33afa7fae51a3
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 07:49:09 2010 -0700

    fix Observer2.cancel weakref handling, it was broken and never got called
---
 src/allmydata/immutable/download2.py      |    2 +-
 src/allmydata/immutable/download2_util.py |   20 ++++++++++++++------
 2 files changed, 15 insertions(+), 7 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index f8737f3..3578aa0 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -176,7 +176,7 @@ class Share:
                 level=log.NOISY, parent=self._lp, umid="RTo9MQ")
         assert segnum >= 0
         o = Observer2()
-        o.set_canceler(self._cancel_block_request)
+        o.set_canceler(self, "_cancel_block_request")
         for i,(segnum0,observers) in enumerate(self._requested_blocks):
             if segnum0 == segnum:
                 observers.add(o)
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
index 9e20ff4..d45f5cc 100755
--- a/src/allmydata/immutable/download2_util.py
+++ b/src/allmydata/immutable/download2_util.py
@@ -11,11 +11,18 @@ class Observer2:
         self._undelivered_results = []
         self._canceler = None
 
-    def set_canceler(self, f):
+    def set_canceler(self, c, methname):
+        """I will call c.METHNAME(self) when somebody cancels me."""
         # we use a weakref to avoid creating a cycle between us and the thing
         # we're observing: they'll be holding a reference to us to compare
-        # against the value we pass to their canceler function.
-        self._canceler = weakref.ref(f)
+        # against the value we pass to their canceler function. However,
+        # since bound methods are first-class objects (and not kept alive by
+        # the object they're bound to), we can't just stash a weakref to the
+        # bound cancel method. Instead, we must hold a weakref to the actual
+        # object, and obtain its cancel method later.
+        # http://code.activestate.com/recipes/81253-weakmethod/ has an
+        # alternative.
+        self._canceler = (weakref.ref(c), methname)
 
     def subscribe(self, observer, **watcher_kwargs):
         self._watcher = (observer, watcher_kwargs)
@@ -35,9 +42,10 @@ class Observer2:
         eventually(o, **kwargs)
 
     def cancel(self):
-        f = self._canceler()
-        if f:
-            f(self)
+        wr,methname = self._canceler
+        o = wr()
+        if o:
+            getattr(o,methname)(self)
 
 
 def incidentally(res, f, *args, **kwargs):

commit 19cbafaa632b93ce396a9e6781d967c6579ce016
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 07:48:06 2010 -0700

    clean up corruption-detecting handling and logging
    
    rename Share._not_dead to Share._alive, ignore responses when dead
---
 src/allmydata/immutable/download2.py |  218 +++++++++++++++++++++++-----------
 1 files changed, 148 insertions(+), 70 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index e88ff1a..f8737f3 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -34,6 +34,8 @@ class BadSegmentError(Exception):
     pass
 class BadCiphertextHashError(Exception):
     pass
+class LayoutInvalid(Exception):
+    pass
 
 class Share:
     """I represent a single instance of a single share (e.g. I reference the
@@ -106,18 +108,22 @@ class Share:
         # 2=offset table, 3=UEB_length and everything else (hashes, block),
         # 4=UEB.
 
-        self._dead = False
+        # ._alive becomes False upon fatal corruption or server error
+        self._alive = True
 
     def __repr__(self):
         return "Share(sh%d-on-%s)" % (self._shnum, self._peerid_s)
 
-    def not_dead(self):
+    def is_alive(self):
         # XXX: reconsider. If the share sees a single error, should it remain
-        # dead for all time? Or should the next segment try again? Also,
-        # 'not_dead' is a dorky method name. This DEAD state is stored
-        # elsewhere too (SegmentFetcher per-share states?) and needs to be
-        # consistent.
-        return not self._dead
+        # dead for all time? Or should the next segment try again? This DEAD
+        # state is stored elsewhere too (SegmentFetcher per-share states?)
+        # and needs to be consistent. We clear _alive in self._fail(), which
+        # is called upon a network error, or layout failure, or hash failure
+        # in the UEB or a hash tree. We do not _fail() for a hash failure in
+        # a block, but of course we still tell our callers about
+        # state=CORRUPT so they'll find a different share.
+        return self._alive
 
     def _guess_offsets(self, verifycap, guessed_segment_size):
         self.guessed_segment_size = guessed_segment_size
@@ -212,6 +218,23 @@ class Share:
                      self._received.dump() ),
                     level=log.NOISY, parent=self._lp, umid="BaL1zw")
             self._do_loop()
+        except (BadHashError, NotEnoughHashesError, LayoutInvalid), e:
+            # Abandon this share. We do this if we see corruption in the
+            # offset table, the UEB, or a hash tree. We don't abandon the
+            # whole share if we see corruption in a data block (we abandon
+            # just the one block, and still try to get data from other blocks
+            # on the same server). In theory, we could get good data from a
+            # share with a corrupt UEB (by first getting the UEB from some
+            # other share), or corrupt hash trees, but the logic to decide
+            # when this is safe is non-trivial. So for now, give up at the
+            # first sign of corruption.
+            #
+            # _satisfy_*() code which detects corruption should first call
+            # self._signal_corruption(), and then raise the exception.
+            log.msg(format="corruption detected in %(share)s, abandoning it",
+                    share=repr(self),
+                    level=log.UNUSUAL, parent=self._lp, umid="gWspVw")
+            self._fail(Failure(e)) # that clears self._alive
         except BaseException:
             self._fail(Failure())
             raise
@@ -221,6 +244,8 @@ class Share:
         #  new segments added to self._requested_blocks
         #  new data received from servers (responses to our read() calls)
         #  impatience timer fires (server appears slow)
+        if not self._alive:
+            return
 
         # First, consume all of the information that we currently have, for
         # all the segments people currently want.
@@ -329,6 +354,24 @@ class Share:
         self.actual_offsets = offsets
         log.msg("actual offsets: data=%d, plaintext_hash_tree=%d, crypttext_hash_tree=%d, block_hashes=%d, share_hashes=%d, uri_extension=%d" % tuple(fields))
         self._received.remove(0, 4) # don't need this anymore
+
+        # validate the offsets a bit
+        share_hashes_size = offsets["uri_extension"] - offsets["share_hashes"]
+        if share_hashes_size % (2+HASH_SIZE) != 0:
+            # the share hash chain is stored as (hashnum,hash) pairs
+            raise LayoutInvalid("share hashes malformed -- should be a"
+                                " multiple of %d bytes -- not %d" %
+                                (2+HASH_SIZE, share_hashes_size))
+        block_hashes_size = offsets["share_hashes"] - offsets["block_hashes"]
+        if block_hashes_size % (HASH_SIZE) != 0:
+            # the block hash tree is stored as a list of hashes
+            raise LayoutInvalid("block hashes malformed -- should be a"
+                                " multiple of %d bytes -- not %d" %
+                                (HASH_SIZE, block_hashes_size))
+        # we only look at 'crypttext_hash_tree' if the UEB says we're
+        # actually using it. Same with 'plaintext_hash_tree'. This gives us
+        # some wiggle room: a place to stash data for later extensions.
+
         return True
 
     def _satisfy_UEB(self):
@@ -347,13 +390,13 @@ class Share:
             self.actual_segment_size = self._node.segment_size
             assert self.actual_segment_size is not None
             return True
-        except BadHashError:
+        except (LayoutInvalid, BadHashError), e:
             # TODO: if this UEB was bad, we'll keep trying to validate it
             # over and over again. Only log.err on the first one, or better
             # yet skip all but the first
-            f = Failure()
+            f = Failure(e)
             self._signal_corruption(f, o["uri_extension"], fsize+UEB_length)
-            return False
+            raise
 
     def _satisfy_share_hash_tree(self):
         # the share hash chain is stored as (hashnum,hash) tuples, so you
@@ -374,12 +417,12 @@ class Share:
         try:
             self._node.process_share_hashes(share_hashes)
             # adds to self._node.share_hash_tree
-            self._received.remove(o["share_hashes"], hashlen)
-            return True
-        except (BadHashError, NotEnoughHashesError, IndexError):
-            f = Failure()
+        except (BadHashError, NotEnoughHashesError), e:
+            f = Failure(e)
             self._signal_corruption(f, o["share_hashes"], hashlen)
-            return False
+            raise
+        self._received.remove(o["share_hashes"], hashlen)
+        return True
 
     def _signal_corruption(self, f, start, offset):
         # there was corruption somewhere in the given range
@@ -401,29 +444,45 @@ class Share:
         # we've gotten them all, because the hash tree will throw an
         # exception if we only give it a partial set (which it therefore
         # cannot validate)
-        commonshare = self._commonshare
-        ok = commonshare.process_block_hashes(block_hashes, self._peerid_s)
-        if not ok:
-            return False
+        try:
+            self._commonshare.process_block_hashes(block_hashes)
+        except (BadHashError, NotEnoughHashesError), e:
+            f = Failure(e),
+            hashnums = ",".join([str(n) for n in sorted(block_hashes.keys())])
+            log.msg(format="hash failure in block_hashes=(%(hashnums)s),"
+                    " from %(share)s",
+                    hashnums=hashnums, shnum=self.shnum, share=repr(self),
+                    failure=f, level=log.WEIRD, parent=self._lp, umid="yNyFdA")
+            hsize = max(0, max(needed_hashes)) * HASH_SIZE
+            self._signal_corruption(f, o_bh, hsize)
+            raise
         for hashnum in needed_hashes:
             self._received.remove(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
         return True
 
     def _satisfy_ciphertext_hash_tree(self, needed_hashes):
         start = self.actual_offsets["crypttext_hash_tree"]
-        ciphertext_hashes = {}
+        hashes = {}
         for hashnum in needed_hashes:
             hashdata = self._received.get(start+hashnum*HASH_SIZE, HASH_SIZE)
             if hashdata:
-                ciphertext_hashes[hashnum] = hashdata
+                hashes[hashnum] = hashdata
             else:
                 return False # missing some hashes
         # we don't submit any hashes to the ciphertext_hash_tree until we've
         # gotten them all
-        ok = self._node.process_ciphertext_hashes(ciphertext_hashes,
-                                                  self._shnum, self._peerid_s)
-        if not ok:
-            return False
+        try:
+            self._node.process_ciphertext_hashes(hashes)
+        except (BadHashError, NotEnoughHashesError), e:
+            f = Failure(e)
+            hashnums = ",".join([str(n) for n in sorted(hashes.keys())])
+            log.msg(format="hash failure in ciphertext_hashes=(%(hashnums)s),"
+                    " from %(share)s",
+                    hashnums=hashnums, share=repr(self), failure=f,
+                    level=log.WEIRD, parent=self._lp, umid="iZI0TA")
+            hsize = max(0, max(needed_hashes))*HASH_SIZE
+            self._signal_corruption(f, start, hsize)
+            raise
         for hashnum in needed_hashes:
             self._received.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
         return True
@@ -451,23 +510,32 @@ class Share:
         # this block is being retired, either as COMPLETE or CORRUPT, since
         # no further data reads will help
         assert self._requested_blocks[0][0] == segnum
-        commonshare = self._commonshare
-        ok = commonshare.check_block(segnum, block, self._peerid_s)
-        if ok:
+        try:
+            self._commonshare.check_block(segnum, block)
+            # hurrah, we have a valid block. Deliver it.
             for o in observers:
                 # goes to SegmentFetcher._block_request_activity
                 o.notify(state=COMPLETE, block=block)
-        else:
+        except (BadHashError, NotEnoughHashesError), e:
+            # rats, we have a corrupt block. Notify our clients that they
+            # need to look elsewhere, and advise the server. Unlike
+            # corruption in other parts of the share, this doesn't cause us
+            # to abandon the whole share.
+            f = Failure(e)
+            self.log(format="hash failure in block %(segnum)d, from %(share)s",
+                     share=repr(self), failure=f,
+                     level=log.WEIRD, parent=self._logparent, umid="mZjkqA")
             for o in observers:
                 o.notify(state=CORRUPT)
-        self._requested_blocks.pop(0) # retired
+            self._signal_corruption(f, blockstart, blocklen)
+        # in either case, we've retired this block
+        self._requested_blocks.pop(0)
         # popping the request keeps us from turning around and wanting the
         # block again right away
         return True # got satisfaction
 
     def _desire(self):
         segnum, observers = self._active_segnum_and_observers() # maybe None
-        commonshare = self._commonshare
 
         if not self.actual_offsets:
             self._desire_offsets()
@@ -489,7 +557,7 @@ class Share:
             return # I have achieved Zen: I desire nothing.
 
         # block hash chain
-        for hashnum in commonshare.get_needed_block_hashes(segnum):
+        for hashnum in self._commonshare.get_needed_block_hashes(segnum):
             self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # ciphertext hash chain
@@ -575,7 +643,7 @@ class Share:
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length, lp)
             d.addErrback(self._got_error, start, length, lp)
-            d.addCallback(incidentally, eventually, self.loop)
+            d.addCallback(self._trigger_loop)
             d.addErrback(lambda f:
                          log.err(format="unhandled error during send_request",
                                  failure=f, parent=self._lp,
@@ -585,6 +653,8 @@ class Share:
         return self._rref.callRemote("read", start, length)
 
     def _got_data(self, data, start, length, lp):
+        if not self._alive:
+            return
         log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
                 share=repr(self), start=start, length=length, datalen=len(data),
                 level=log.NOISY, parent=lp, umid="sgVAyA")
@@ -603,11 +673,16 @@ class Share:
         # further progress
         self._fail(f)
 
+    def _trigger_loop(self, res):
+        if self._alive:
+            eventually(self.loop)
+        return res
+
     def _fail(self, f):
         log.msg(format="abandoning %(share)s",
                 share=repr(self), failure=f,
                 level=log.UNUSUAL, parent=self._lp, umid="JKM2Og")
-        self._dead = True
+        self._alive = False
         for (segnum, observers) in self._requested_blocks:
             for o in observers:
                 o.notify(state=DEAD, f=f)
@@ -654,33 +729,16 @@ class CommonShare:
         # the same time.
         return self._block_hash_tree.needed_hashes(segnum, include_leaf=True)
 
-    def process_block_hashes(self, block_hashes, serverid_s):
+    def process_block_hashes(self, block_hashes):
         assert self._know_numsegs
-        try:
-            self._block_hash_tree.set_hashes(block_hashes)
-            return True
-        except (BadHashError, NotEnoughHashesError):
-            hashnums = ",".join([str(n) for n in sorted(block_hashes.keys())])
-            log.msg(format="hash failure in block_hashes=(%(hashnums)s),"
-                    " shnum=%(shnum)d SI=%(si)s server=%(server)s",
-                    hashnums=hashnums, shnum=self.shnum,
-                    si=self.si_prefix, server=serverid_s, failure=Failure(),
-                    level=log.WEIRD, parent=self._logparent, umid="yNyFdA")
-        return False
+        # this may raise BadHashError or NotEnoughHashesError
+        self._block_hash_tree.set_hashes(block_hashes)
 
-    def check_block(self, segnum, block, serverid_s):
+    def check_block(self, segnum, block):
         assert self._know_numsegs
         h = hashutil.block_hash(block)
-        try:
-            self._block_hash_tree.set_hashes(leaves={segnum: h})
-            return True
-        except (BadHashError, NotEnoughHashesError):
-            self.log(format="hash failure in block %(segnum)d,"
-                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
-                     segnum=segnum, shnum=self.shnum, si=self.si_prefix,
-                     server=serverid_s, failure=Failure(),
-                     level=log.WEIRD, parent=self._logparent, umid="mZjkqA")
-        return False
+        # this may raise BadHashError or NotEnoughHashesError
+        self._block_hash_tree.set_hashes(leaves={segnum: h})
 
 # all classes are also Services, and the rule is that you don't initiate more
 # work unless self.running
@@ -879,6 +937,8 @@ class SegmentFetcher:
 
     def _block_request_activity(self, share, shnum, state, block=None, f=None):
         # called by Shares, in response to our s.send_request() calls.
+        if not self._running:
+            return
         log.msg("SegmentFetcher(%s)._block_request_activity:"
                 " Share(sh%d-on-%s) -> %s" %
                 (self._node._si_prefix, shnum, share._peerid_s, state),
@@ -1294,6 +1354,9 @@ class _Node:
         self._sharefinder = ShareFinder(storage_broker, verifycap, self, lp)
         self._shares = set()
 
+    def __repr__(self):
+        return "Imm_Node(%s)" % (self._si_prefix,)
+
     def stop(self):
         # called by the Terminator at shutdown, mostly for tests
         if self._active_segment:
@@ -1371,8 +1434,11 @@ class _Node:
         if self._active_segment is None and self._segment_requests:
             segnum = self._segment_requests[0][0]
             k = self._verifycap.needed_shares
+            log.msg(format="%(node)s._start_new_segment: segnum=%(segnum)d",
+                    node=repr(self), segnum=segnum,
+                    level=log.NOISY, umid="wAlnHQ")
             self._active_segment = fetcher = SegmentFetcher(self, segnum, k)
-            active_shares = [s for s in self._shares if s.not_dead()]
+            active_shares = [s for s in self._shares if s.is_alive()]
             fetcher.add_shares(active_shares) # this triggers the loop
 
 
@@ -1503,24 +1569,24 @@ class _Node:
 
 
     def process_share_hashes(self, share_hashes):
+        for hashnum in share_hashes:
+            if hashnum >= len(self.share_hash_tree):
+                # "BadHashError" is normally for e.g. a corrupt block. We
+                # sort of abuse it here to mean a badly numbered hash (which
+                # indicates corruption in the number bytes, rather than in
+                # the data bytes).
+                raise BadHashError("hashnum %d doesn't fit in hashtree(%d)"
+                                   % (hashnum, len(self.share_hash_tree)))
         self.share_hash_tree.set_hashes(share_hashes)
 
     def get_needed_ciphertext_hashes(self, segnum):
         cht = self.ciphertext_hash_tree
         return cht.needed_hashes(segnum, include_leaf=True)
-    def process_ciphertext_hashes(self, hashes, shnum, serverid_s):
+    def process_ciphertext_hashes(self, hashes):
         assert self.num_segments is not None
-        try:
-            self.ciphertext_hash_tree.set_hashes(hashes)
-            return True
-        except (BadHashError, NotEnoughHashesError):
-            hashnums = ",".join([str(n) for n in sorted(hashes.keys())])
-            log.msg(format="hash failure in ciphertext_hashes=(%(hashnums)s),"
-                    " shnum=%(shnum)d SI=%(si)s server=%(server)s",
-                    hashnums=hashnums, shnum=shnum,
-                    si=self._si_prefix, server=serverid_s, failure=Failure(),
-                    level=log.WEIRD, parent=self._lp, umid="iZI0TA")
-        return False
+        # this may raise BadHashError or NotEnoughHashesError
+        self.ciphertext_hash_tree.set_hashes(hashes)
+
 
     # called by our child SegmentFetcher
 
@@ -1905,3 +1971,15 @@ class ImmutableFileNode:
 #
 # fix was to not call self.stop when ShareFinder runs out of shares. stop()
 # is now only called by the Terminator.
+
+# TODO: make sure that _signal_corruption(f) isn't sending private local
+# variables in the CopiedFailure
+
+# tests to write:
+#  two-segment file, tweak default-segsize so we guess right, start
+#  simultaneous reads of both segments. goal is to cover Share.get_block
+#  in the for(self._requested_blocks) loop.
+#
+#  to cover deeper in Share.get_block for loop, we need to bypass Node.read
+#  and call Share.get_block directly (Nodes and SegmentFetchers will avoid
+#  doing multiple get_blocks)

commit 608872de0a57b29edd8e93bdbf4f53bf96d53af8
Author: Brian Warner <warner@lothar.com>
Date:   Wed May 5 07:45:01 2010 -0700

    coverage tools: ignore errors, display lines-uncovered in elisp mode
---
 Makefile                         |    2 +-
 misc/coding_tools/coverage.el    |    3 ++-
 misc/coding_tools/coverage2el.py |    7 +++++--
 3 files changed, 8 insertions(+), 4 deletions(-)

diff --git a/Makefile b/Makefile
index 3e4be60..441cd32 100644
--- a/Makefile
+++ b/Makefile
@@ -134,7 +134,7 @@ quicktest-coverage:
 
 coverage-output:
 	rm -rf coverage-html
-	coverage html -d coverage-html
+	coverage html -i -d coverage-html $(COVERAGE_OMIT)
 	cp .coverage coverage-html/coverage.data
 	@echo "now point your browser at coverage-html/index.html"
 
diff --git a/misc/coding_tools/coverage.el b/misc/coding_tools/coverage.el
index bad490f..8d69d5d 100644
--- a/misc/coding_tools/coverage.el
+++ b/misc/coding_tools/coverage.el
@@ -84,7 +84,8 @@
                            'face '(:box "red")
                            )
               )
-            (message "Added annotations")
+            (message (format "Added annotations: %d uncovered lines"
+                             (safe-length uncovered-code-lines)))
             )
           )
       (message "unable to find coverage for this file"))
diff --git a/misc/coding_tools/coverage2el.py b/misc/coding_tools/coverage2el.py
index ed94bd0..7d03a27 100644
--- a/misc/coding_tools/coverage2el.py
+++ b/misc/coding_tools/coverage2el.py
@@ -1,5 +1,5 @@
 
-from coverage import coverage, summary
+from coverage import coverage, summary, misc
 
 class ElispReporter(summary.SummaryReporter):
     def report(self):
@@ -21,7 +21,10 @@ class ElispReporter(summary.SummaryReporter):
         out.write("(let ((results (make-hash-table :test 'equal)))\n")
         for cu in self.code_units:
             f = cu.filename
-            (fn, executable, missing, mf) = self.coverage.analysis(cu)
+            try:
+                (fn, executable, missing, mf) = self.coverage.analysis(cu)
+            except misc.NoSource:
+                continue
             code_linenumbers = executable
             uncovered_code = missing
             covered_linenumbers = sorted(set(executable) - set(missing))

commit e1fe342932677ff26c30a5da26e262a3d559e957
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 19:25:14 2010 -0700

    update NodeMaker() calls to new signature
---
 src/allmydata/test/test_dirnode.py |   10 ++++------
 src/allmydata/test/test_mutable.py |    2 +-
 src/allmydata/test/test_web.py     |    2 +-
 3 files changed, 6 insertions(+), 8 deletions(-)

diff --git a/src/allmydata/test/test_dirnode.py b/src/allmydata/test/test_dirnode.py
index 14a613a..ac4838a 100644
--- a/src/allmydata/test/test_dirnode.py
+++ b/src/allmydata/test/test_dirnode.py
@@ -1178,7 +1178,7 @@ class Packing(unittest.TestCase):
     def test_unpack_and_pack_behavior(self):
         known_tree = b32decode(self.known_tree)
         nodemaker = NodeMaker(None, None, None,
-                              None, None, None,
+                              None, None,
                               {"k": 3, "n": 10}, None)
         write_uri = "URI:SSK-RO:e3mdrzfwhoq42hy5ubcz6rp3o4:ybyibhnp3vvwuq2vaw2ckjmesgkklfs6ghxleztqidihjyofgw7q"
         filenode = nodemaker.create_from_cap(write_uri)
@@ -1240,8 +1240,7 @@ class Packing(unittest.TestCase):
         return kids
 
     def test_deep_immutable(self):
-        nm = NodeMaker(None, None, None, None, None, None, {"k": 3, "n": 10},
-                       None)
+        nm = NodeMaker(None, None, None, None, None, {"k": 3, "n": 10}, None)
         fn = MinimalFakeMutableFile()
 
         kids = self._make_kids(nm, ["imm", "lit", "write", "read",
@@ -1335,7 +1334,7 @@ class FakeNodeMaker(NodeMaker):
 class FakeClient2(Client):
     def __init__(self):
         self.nodemaker = FakeNodeMaker(None, None, None,
-                                       None, None, None,
+                                       None, None,
                                        {"k":3,"n":10}, None)
     def create_node_from_uri(self, rwcap, rocap):
         return self.nodemaker.create_from_cap(rwcap, rocap)
@@ -1619,8 +1618,7 @@ class Deleter(GridTestMixin, unittest.TestCase):
         def _do_delete(ignored):
             nm = UCWEingNodeMaker(c0.storage_broker, c0._secret_holder,
                                   c0.get_history(), c0.getServiceNamed("uploader"),
-                                  c0.downloader,
-                                  c0.download_cache_dirman,
+                                  c0.terminator,
                                   c0.get_encoding_parameters(),
                                   c0._key_generator)
             n = nm.create_from_cap(self.root_uri)
diff --git a/src/allmydata/test/test_mutable.py b/src/allmydata/test/test_mutable.py
index 30d1083..021e196 100644
--- a/src/allmydata/test/test_mutable.py
+++ b/src/allmydata/test/test_mutable.py
@@ -197,7 +197,7 @@ def make_nodemaker(s=None, num_peers=10):
     keygen = client.KeyGenerator()
     keygen.set_default_keysize(522)
     nodemaker = NodeMaker(storage_broker, sh, None,
-                          None, None, None,
+                          None, None,
                           {"k": 3, "n": 10}, keygen)
     return nodemaker
 
diff --git a/src/allmydata/test/test_web.py b/src/allmydata/test/test_web.py
index c67e81c..28b2b1f 100644
--- a/src/allmydata/test/test_web.py
+++ b/src/allmydata/test/test_web.py
@@ -109,7 +109,7 @@ class FakeClient(Client):
         self.uploader = FakeUploader()
         self.uploader.setServiceParent(self)
         self.nodemaker = FakeNodeMaker(None, self._secret_holder, None,
-                                       self.uploader, None, None,
+                                       self.uploader, None,
                                        None, None)
 
     def startService(self):

commit 3814edd20c154a7df9feee171fce4fe07c42a844
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 19:24:38 2010 -0700

    Don't find servers until someone calls read().
    
    This helps unit tests which create FileNodes (with fake resources) but don't
    ever try to download them. Also tolerate terminator=None.
---
 src/allmydata/immutable/download2.py |   18 +++++++++++++++---
 1 files changed, 15 insertions(+), 3 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 2f80768..e88ff1a 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -915,8 +915,8 @@ class ShareFinder:
                  max_outstanding_requests=10):
         self.running = True # stopped by Share.stop, from Terminator
         self.verifycap = verifycap
-        s = storage_broker.get_servers_for_index(verifycap.storage_index)
-        self._servers = iter(s)
+        self._started = False
+        self._storage_broker = storage_broker
         self.share_consumer = self.node = node
         self.max_outstanding_requests = max_outstanding_requests
 
@@ -933,6 +933,16 @@ class ShareFinder:
                            si=self._si_prefix,
                            level=log.NOISY, parent=logparent, umid="2xjj2A")
 
+    def start_finding_servers(self):
+        # don't get servers until somebody uses us: creating the
+        # ImmutableFileNode should not cause work to happen yet. Test case is
+        # test_dirnode, which creates us with storage_broker=None
+        if not self._started:
+            si = self.verifycap.storage_index
+            s = self._storage_broker.get_servers_for_index(si)
+            self._servers = iter(s)
+            self._started = True
+
     def log(self, *args, **kwargs):
         if "parent" not in kwargs:
             kwargs["parent"] = self._lp
@@ -945,6 +955,7 @@ class ShareFinder:
     def hungry(self):
         self.log(format="ShareFinder[si=%(si)s] hungry",
                  si=self._si_prefix, level=log.NOISY, umid="NywYaQ")
+        self.start_finding_servers()
         self._hungry = True
         eventually(self.loop)
 
@@ -1224,7 +1235,8 @@ class _Node:
         self._storage_broker = storage_broker
         self._si_prefix = base32.b2a_l(verifycap.storage_index[:8], 60)
         self.running = True
-        terminator.register(self) # calls self.stop() at stopService()
+        if terminator:
+            terminator.register(self) # calls self.stop() at stopService()
         # the rules are:
         # 1: Only send network requests if you're active (self.running is True)
         # 2: Use TimerService, not reactor.callLater

commit bc2fba7957c92861bf8aa69ad5b1e8a7e765eee9
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 02:18:43 2010 -0700

    enhance tests, tolerate new error messages
---
 src/allmydata/test/test_cli.py      |   12 ++++++++++--
 src/allmydata/test/test_download.py |   34 ++++++++++++++++++++++++++++++++++
 2 files changed, 44 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/test/test_cli.py b/src/allmydata/test/test_cli.py
index 7f84dd6..9b4b032 100644
--- a/src/allmydata/test/test_cli.py
+++ b/src/allmydata/test/test_cli.py
@@ -2308,12 +2308,19 @@ class Errors(GridTestMixin, CLITestMixin, unittest.TestCase):
             self.delete_shares_numbered(ur.uri, range(1,10))
         d.addCallback(_stash_bad)
 
+        # the download is abandoned as soon as it's clear that we won't get
+        # enough shares. The one remaining share might be in either the
+        # COMPLETE or the PENDING state.
+        in_complete_msg = "ran out of shares: 1 complete, 0 pending, 0 overdue, 0 unused, need 3"
+        in_pending_msg = "ran out of shares: 0 complete, 1 pending, 0 overdue, 0 unused, need 3"
+
         d.addCallback(lambda ign: self.do_cli("get", self.uri_1share))
         def _check1((rc, out, err)):
             self.failIfEqual(rc, 0)
             self.failUnless("410 Gone" in err, err)
             self.failUnlessIn("NotEnoughSharesError: ", err)
-            self.failUnlessIn("ran out of shares: 1 complete, 0 pending, 0 overdue, 0 unused, need 3", err)
+            self.failUnless(in_complete_msg in err or in_pending_msg in err,
+                            err)
         d.addCallback(_check1)
 
         targetf = os.path.join(self.basedir, "output")
@@ -2322,7 +2329,8 @@ class Errors(GridTestMixin, CLITestMixin, unittest.TestCase):
             self.failIfEqual(rc, 0)
             self.failUnless("410 Gone" in err, err)
             self.failUnlessIn("NotEnoughSharesError: ", err)
-            self.failUnlessIn("ran out of shares: 1 complete, 0 pending, 0 overdue, 0 unused, need 3", err)
+            self.failUnless(in_complete_msg in err or in_pending_msg in err,
+                            err)
             self.failIf(os.path.exists(targetf))
         d.addCallback(_check2)
 
diff --git a/src/allmydata/test/test_download.py b/src/allmydata/test/test_download.py
index b54bf01..cffa132 100644
--- a/src/allmydata/test/test_download.py
+++ b/src/allmydata/test/test_download.py
@@ -178,6 +178,9 @@ class DownloadTest(GridTestMixin, unittest.TestCase):
         def _got_data(data):
             self.failUnlessEqual(data, plaintext)
         d.addCallback(_got_data)
+        # make sure we can use the same node twice
+        d.addCallback(lambda ign: download_to_data(n))
+        d.addCallback(_got_data)
         return d
 
     def download_mutable(self, ignored=None):
@@ -188,3 +191,34 @@ class DownloadTest(GridTestMixin, unittest.TestCase):
         d.addCallback(_got_data)
         return d
 
+    def test_download_failover(self):
+        self.basedir = self.mktemp()
+        self.set_up_grid()
+        self.c0 = self.g.clients[0]
+
+        self.load_shares()
+
+        n = self.c0.create_node_from_uri(immutable_uri)
+        d = download_to_data(n)
+        def _got_data(data):
+            self.failUnlessEqual(data, plaintext)
+        d.addCallback(_got_data)
+
+        def _clobber_shares(ign):
+            # find the three shares that were used, and delete them. Then
+            # download again, forcing the downloader to fail over to other
+            # shares
+            si = uri.from_string(immutable_uri).get_storage_index()
+            si_dir = storage_index_to_dir(si)
+            for s in n._cnode._node._shares:
+                for clientnum in immutable_shares:
+                    for shnum in immutable_shares[clientnum]:
+                        if s._shnum == shnum:
+                            fn = os.path.join(self.get_serverdir(clientnum),
+                                              "shares", si_dir, str(shnum))
+                            os.unlink(fn)
+        d.addCallback(_clobber_shares)
+        d.addCallback(lambda ign: download_to_data(n))
+        d.addCallback(_got_data)
+        return d
+

commit 597c70f1d219213d8b4f619d0144adc03e31d526
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 02:17:52 2010 -0700

    _share_observers might be empty, tolerate it
---
 src/allmydata/immutable/download2.py |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 642fe78..2f80768 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -885,7 +885,7 @@ class SegmentFetcher:
                 level=log.NOISY, umid="vilNWA")
         # COMPLETE, CORRUPT, DEAD, BADSEGNUM are terminal.
         if state in (COMPLETE, CORRUPT, DEAD, BADSEGNUM):
-            del self._share_observers[share]
+            self._share_observers.pop(share, None)
         if state is COMPLETE:
             # 'block' is fully validated
             self._shares[share] = COMPLETE

commit e7b48bbe03c7c2901c9afd5c87f0a2108f64291a
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 02:17:40 2010 -0700

    comment on bug that got fixed
---
 src/allmydata/immutable/download2.py |   17 +++++++++++++++--
 1 files changed, 15 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 3d2cf74..642fe78 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -913,7 +913,7 @@ class RequestToken:
 class ShareFinder:
     def __init__(self, storage_broker, verifycap, node, logparent=None,
                  max_outstanding_requests=10):
-        self.running = True
+        self.running = True # stopped by Share.stop, from Terminator
         self.verifycap = verifycap
         s = storage_broker.get_servers_for_index(verifycap.storage_index)
         self._servers = iter(s)
@@ -1000,7 +1000,6 @@ class ShareFinder:
         # we have nothing in flight. No further progress can be made. They
         # are destined to remain hungry.
         self.share_consumer.no_more_shares()
-        self.stop()
 
     def send_request(self, server):
         peerid, rref = server
@@ -1880,3 +1879,17 @@ class ImmutableFileNode:
 # log budget: when downloading at 1MBps (i.e. 8 segments-per-second), 10
 # log.OPERATIONAL per second, 100 log.NOISY per second. With k=3, that's 3
 # log.NOISY per block fetch.
+
+
+# test_cli.Error failed for a while: ShareFinder created, used up
+# (NotEnoughSharesError), started again. The self.running=False is the
+# problem.
+#
+# The second download is hungry, but because ShareFinder.running is false, it
+# never notifies the SegmentFetcher that there are no more shares coming, so
+# the download never completes. To trigger this in tests, we need the first
+# download to want more shares (so it must fail with NotEnoughSharesError, or
+# we must lose a share/server between downloads).
+#
+# fix was to not call self.stop when ShareFinder runs out of shares. stop()
+# is now only called by the Terminator.

commit ff9edaea0120ec3ebc0a2094b6b3215d6af8d473
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 02:03:43 2010 -0700

    fix minor typo in log message
---
 src/allmydata/immutable/download2.py |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 35f5918..3d2cf74 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1306,7 +1306,7 @@ class _Node:
             size = self._verifycap.size
         # clip size so offset+size does not go past EOF
         size = min(size, self._verifycap.size-offset)
-        lp = log.msg(format="imm Node(%(si)s.read(%(offset)d, %(size)d)",
+        lp = log.msg(format="imm Node(%(si)s).read(%(offset)d, %(size)d)",
                      si=base32.b2a(self._verifycap.storage_index)[:8],
                      offset=offset, size=size,
                      level=log.OPERATIONAL, parent=self._lp, umid="l3j3Ww")

commit 8c215c912a2f7e565507d2b4c4f6d10edd71da1d
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 02:03:32 2010 -0700

    clarify what satisfy/desire means when there is no active segnum
---
 src/allmydata/immutable/download2.py |   10 +++++++---
 1 files changed, 7 insertions(+), 3 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 5ba76d0..35f5918 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -260,7 +260,11 @@ class Share:
         cs.set_numsegs(self._node.num_segments)
 
         segnum, observers = self._active_segnum_and_observers()
-        if segnum >= self._node.num_segments:
+        # if segnum is None, we don't really need to do anything (we have no
+        # outstanding readers right now), but we'll fill in the bits that
+        # aren't tied to any particular segment.
+
+        if segnum is not None and segnum >= self._node.num_segments:
             for o in observers:
                 o.notify(state=BADSEGNUM)
             self._requested_blocks.pop(0)
@@ -462,7 +466,7 @@ class Share:
         return True # got satisfaction
 
     def _desire(self):
-        segnum, observers = self._active_segnum_and_observers()
+        segnum, observers = self._active_segnum_and_observers() # maybe None
         commonshare = self._commonshare
 
         if not self.actual_offsets:
@@ -482,7 +486,7 @@ class Share:
             self._wanted.add(o["share_hashes"], hashlen)
 
         if segnum is None:
-            return # only need block hashes or blocks for active segments
+            return # I have achieved Zen: I desire nothing.
 
         # block hash chain
         for hashnum in commonshare.get_needed_block_hashes(segnum):

commit fe8a52fd4eb61ac0a9556521e6e88a68e0b49527
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 30 02:01:38 2010 -0700

    Fix fetch-data-block-twice problem by cleaning up _wanted/_requested/_received
    
    * merge Share._wanted_blocks into Share._wanted: don't differentiate between
      data and metadata when ordering fetches. This might be a mistake. I'm not
      currently convinced that we can save RAM-footprint * time by fetching
      hashes first, but I must think about it more.
    * add Share.__repr__ and use it in logging
    * clean up _wanted/_requested/_received handling: remove received data from
      _wanted. This fixed a fetch-data-block-twice problem.
---
 src/allmydata/immutable/download2.py |  101 ++++++++++++++++++++++------------
 1 files changed, 66 insertions(+), 35 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index a18b834..5ba76d0 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -62,21 +62,40 @@ class Share:
         self._si_prefix = base32.b2a(verifycap.storage_index)[:8]
         self._shnum = shnum
 
-        self._lp = log.msg(format="Share(%(si)s) on server=%(server)s starting",
-                           si=self._si_prefix, server=self._peerid_s,
+        self._lp = log.msg(format="%(share)s created", share=repr(self),
                            level=log.NOISY, parent=logparent, umid="P7hv2w")
 
-        self._wanted = Spans() # desired metadata
-        self._wanted_blocks = Spans() # desired block data
-        # self._requested contains ranges we've requested before (either
-        # in-flight or answered-yes or answered-no). If they answered-yes,
-        # the corresponding data will be in self._received. If we remove the
-        # data from self._received (i.e. for large data blocks), we'll also
-        # remove the range from self._requested, so that we can ask for it
-        # again later.
+        # any given byte of the share can be in one of four states:
+        #  in: _wanted, _requested, _received
+        #      FALSE    FALSE       FALSE : don't care about it at all
+        #      TRUE     FALSE       FALSE : want it, haven't yet asked for it
+        #      TRUE     TRUE        FALSE : request is in-flight
+        #                                   or didn't get it
+        #      FALSE    TRUE        TRUE  : got it, haven't used it yet
+        #      FALSE    TRUE        FALSE : got it and used it
+        #      FALSE    FALSE       FALSE : block consumed, ready to ask again
+        #
+        # when we request data and get a NAK, we leave it in _requested
+        # to remind ourself to not ask for it again. We don't explicitly
+        # remove it from anything (maybe this should change).
+        #
+        # We retain the hashtrees in the Node, so we leave those spans in
+        # _requested (and never ask for them again, as long as the Node is
+        # alive). But we don't retain data blocks (too big), so when we
+        # consume a data block, we remove it from _requested, so a later
+        # download can re-fetch it.
+
+        # self._wanted contains data that we need, either metadata (like
+        # hashes) or block data. Once we've received the data, we remove it
+        # from self._wanted
+        self._wanted = Spans()
+
+        # self._requested contains ranges we've requested before: this data
+        # is either in-flight or answered-yes or answered-no.
         self._requested = Spans() # we've sent a request for this
         # self._received contains data that we haven't yet used
         self._received = DataSpans() # we've received a response for this
+
         self._requested_blocks = [] # (segnum, set(observer2..))
         ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
         self._overrun_ok = ver["tolerates-immutable-read-overrun"]
@@ -89,6 +108,9 @@ class Share:
 
         self._dead = False
 
+    def __repr__(self):
+        return "Share(sh%d-on-%s)" % (self._shnum, self._peerid_s)
+
     def not_dead(self):
         # XXX: reconsider. If the share sees a single error, should it remain
         # dead for all time? Or should the next segment try again? Also,
@@ -144,8 +166,7 @@ class Share:
          - state=DEAD, f=Failure: the server reported an error, this share
                                   is unusable
         """
-        log.msg("Share(sh%d-on-%s).get_block(%d)" %
-                (self._shnum, self._peerid_s, segnum),
+        log.msg("%s.get_block(%d)" % (repr(self), segnum),
                 level=log.NOISY, parent=self._lp, umid="RTo9MQ")
         assert segnum >= 0
         o = Observer2()
@@ -184,11 +205,11 @@ class Share:
     def loop(self):
         try:
             # if any exceptions occur here, kill the download
-            log.msg("Share(sh%d on %s).loop, reqs=[%s], wanted=%s, requested=%s, received=%s, wanted_blocks=%s" %
-                    (self._shnum, self._peerid_s,
+            log.msg("%s.loop, reqs=[%s], wanted=%s, requested=%s, received=%s" %
+                    (repr(self),
                      ",".join([str(req[0]) for req in self._requested_blocks]),
                      self._wanted.dump(), self._requested.dump(),
-                     self._received.dump(), self._wanted_blocks.dump() ),
+                     self._received.dump() ),
                     level=log.NOISY, parent=self._lp, umid="BaL1zw")
             self._do_loop()
         except BaseException:
@@ -414,6 +435,14 @@ class Share:
         block = self._received.pop(blockstart, blocklen)
         if not block:
             return False
+        log.msg(format="%(share)s._satisfy_data_block, len(block)=%(blocklen)d",
+                share=repr(self),
+                blocklen=len(block),
+                level=log.NOISY, parent=self._lp, umid="uTDNZg")
+        # we removed the block from _received, but don't retain the data in
+        # our Node or CommonShare, so also remove it from _requested: this
+        # lets us ask for it again in a later download which uses this same
+        # Share object.
         self._requested.remove(blockstart, blocklen)
         # this block is being retired, either as COMPLETE or CORRUPT, since
         # no further data reads will help
@@ -428,6 +457,8 @@ class Share:
             for o in observers:
                 o.notify(state=CORRUPT)
         self._requested_blocks.pop(0) # retired
+        # popping the request keeps us from turning around and wanting the
+        # block again right away
         return True # got satisfaction
 
     def _desire(self):
@@ -469,8 +500,8 @@ class Share:
         blocklen = r["block_size"]
         if tail:
             blocklen = r["tail_block_size"]
-        self._wanted_blocks.add(blockstart, blocklen)
-
+        self._wanted.add(blockstart, blocklen)
+        #log.msg("end _desire: wanted=%s" % (self._wanted.dump(),))
 
     def _desire_offsets(self):
         if self._overrun_ok:
@@ -520,22 +551,21 @@ class Share:
             self._wanted.add(o["uri_extension"]+self._fieldsize, UEB_length)
 
     def _request_needed(self):
-        # send requests for metadata first, to avoid hanging on to large data
-        # blocks any longer than necessary.
         received = self._received.get_spans()
-        self._send_requests(self._wanted - received - self._requested)
-        # then send requests for data blocks. All the hashes should arrive
-        # before the blocks, so the blocks can be consumed and released in a
-        # single turn.
-        self._send_requests(self._wanted_blocks - received - self._requested)
+        ask = self._wanted - self._requested - received
+        self._send_requests(ask) # this removes it from _wanted
+        # XXX then send requests for data blocks. All the hashes should
+        # arrive before the blocks, so the blocks can be consumed and
+        # released in a single turn. TODO: I removed this for simplicity.
+        # Reconsider the removal: maybe bring it back.
 
     def _send_requests(self, needed):
         for (start, length) in needed:
             # TODO: quantize to reasonably-large blocks
             self._requested.add(start, length)
-            lp = log.msg(format="_send_request(sh%(shnum)d-on-%(peerid)s)"
+            lp = log.msg(format="%(share)s._send_request"
                          " [%(start)d:+%(length)d]",
-                         shnum=self._shnum, peerid=self._peerid_s,
+                         share=repr(self),
                          start=start, length=length,
                          level=log.NOISY, parent=self._lp, umid="sgVAyA")
             d = self._send_request(start, length)
@@ -551,12 +581,13 @@ class Share:
         return self._rref.callRemote("read", start, length)
 
     def _got_data(self, data, start, length, lp):
-        log.msg(format="_got_data [%(start)d:+%(length)d] -> %(datalen)d",
-                start=start, length=length, datalen=len(data),
+        log.msg(format="%(share)s._got_data [%(start)d:+%(length)d] -> %(datalen)d",
+                share=repr(self), start=start, length=length, datalen=len(data),
                 level=log.NOISY, parent=lp, umid="sgVAyA")
         span = (start, length)
-        assert span in self._requested
+        assert span in self._requested # XXX eh, not important
         self._received.add(start, data)
+        self._wanted.remove(start, length)
 
     def _got_error(self, f, start, length, lp):
         log.msg(format="error requesting %(start)d+%(length)d"
@@ -569,8 +600,8 @@ class Share:
         self._fail(f)
 
     def _fail(self, f):
-        log.msg(format="abandoning Share(sh%(shnum)d-on-%(peerid)s",
-                failure=f, shnum=self._shnum, peerid=self._peerid_s,
+        log.msg(format="abandoning %(share)s",
+                share=repr(self), failure=f,
                 level=log.UNUSUAL, parent=self._lp, umid="JKM2Og")
         self._dead = True
         for (segnum, observers) in self._requested_blocks:
@@ -844,6 +875,10 @@ class SegmentFetcher:
 
     def _block_request_activity(self, share, shnum, state, block=None, f=None):
         # called by Shares, in response to our s.send_request() calls.
+        log.msg("SegmentFetcher(%s)._block_request_activity:"
+                " Share(sh%d-on-%s) -> %s" %
+                (self._node._si_prefix, shnum, share._peerid_s, state),
+                level=log.NOISY, umid="vilNWA")
         # COMPLETE, CORRUPT, DEAD, BADSEGNUM are terminal.
         if state in (COMPLETE, CORRUPT, DEAD, BADSEGNUM):
             del self._share_observers[share]
@@ -864,10 +899,6 @@ class SegmentFetcher:
         elif state is BADSEGNUM:
             self._shares[share] = BADSEGNUM # ???
             self._bad_segnum = True
-        log.msg("SegmentFetcher(%s)._block_request_activity:"
-                " Share(sh%d-on-%s) -> %s" %
-                (self._node._si_prefix, shnum, share._peerid_s, state),
-                level=log.NOISY, umid="vilNWA")
         eventually(self.loop)
 
 

commit 6e4c029f22bd6f2edbea14d9d749fd238cefb582
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 21:46:28 2010 -0700

    update test_cli.Error to match new NotEnoughSharesError message
---
 src/allmydata/test/test_cli.py |    4 ++--
 1 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/allmydata/test/test_cli.py b/src/allmydata/test/test_cli.py
index ef6090a..7f84dd6 100644
--- a/src/allmydata/test/test_cli.py
+++ b/src/allmydata/test/test_cli.py
@@ -2313,7 +2313,7 @@ class Errors(GridTestMixin, CLITestMixin, unittest.TestCase):
             self.failIfEqual(rc, 0)
             self.failUnless("410 Gone" in err, err)
             self.failUnlessIn("NotEnoughSharesError: ", err)
-            self.failUnlessIn("Failed to get enough shareholders: have 1, need 3", err)
+            self.failUnlessIn("ran out of shares: 1 complete, 0 pending, 0 overdue, 0 unused, need 3", err)
         d.addCallback(_check1)
 
         targetf = os.path.join(self.basedir, "output")
@@ -2322,7 +2322,7 @@ class Errors(GridTestMixin, CLITestMixin, unittest.TestCase):
             self.failIfEqual(rc, 0)
             self.failUnless("410 Gone" in err, err)
             self.failUnlessIn("NotEnoughSharesError: ", err)
-            self.failUnlessIn("Failed to get enough shareholders: have 1, need 3", err)
+            self.failUnlessIn("ran out of shares: 1 complete, 0 pending, 0 overdue, 0 unused, need 3", err)
             self.failIf(os.path.exists(targetf))
         d.addCallback(_check2)
 

commit a2accbebf8656e1d4b82f49c962acbef04c33af9
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 21:46:04 2010 -0700

    SegmentFetcher: handle stop() better, fix 1<=numsh<k error message, add logging
---
 src/allmydata/immutable/download2.py |   26 +++++++++++++++++++-------
 1 files changed, 19 insertions(+), 7 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 22cb4e8..a18b834 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -603,11 +603,8 @@ class CommonShare:
         return bool(not self._block_hash_tree[0])
 
     def set_block_hash_root(self, roothash):
-        lp = log.msg("CommonShare.set_block_hash_root: %s" % repr(roothash),
-                     level=log.NOISY, parent=self._logparent, umid="wwG5Gw")
+        assert self._know_numsegs
         self._block_hash_tree.set_hashes({0: roothash})
-        log.msg("done with set_block_hash_root",
-                level=log.NOISY, parent=lp, umid="xjWKcw")
 
     def get_needed_block_hashes(self, segnum):
         needed = ",".join([str(n) for n in sorted(self._block_hash_tree.needed_hashes(segnum))])
@@ -696,9 +693,11 @@ class SegmentFetcher:
         self._running = True
 
     def stop(self):
+        log.msg("SegmentFetcher(%s).stop" % self._node._si_prefix,
+                level=log.NOISY, umid="LWyqpg")
         self._cancel_all_requests()
         self._running = False
-        del self._shares # let GC work # ??? XXX
+        self._shares.clear() # let GC work # ??? XXX
 
 
     # called by our parent _Node
@@ -723,7 +722,7 @@ class SegmentFetcher:
         """shnums for which at least one state is in the following list"""
         shnums = []
         for shnum,shares in self._shnums.iteritems():
-            matches = [s for s in shares if self._shares[s] in states]
+            matches = [s for s in shares if self._shares.get(s) in states]
             if matches:
                 shnums.append(shnum)
         return len(shnums)
@@ -762,8 +761,9 @@ class SegmentFetcher:
             self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) < k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
-            self.stop()
 
+            log.msg("share states: %r" % (self._shares,),
+                    level=log.NOISY, umid="0ThykQ")
             if self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) == 0:
                 format = ("no shares (need %(k)d)."
                           " Last failure: %(last_failure)s")
@@ -787,6 +787,7 @@ class SegmentFetcher:
             log.msg(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
             e = error(format % args)
             f = Failure(e)
+            self.stop()
             self._node.fetch_failed(self, f)
             return
 
@@ -863,6 +864,10 @@ class SegmentFetcher:
         elif state is BADSEGNUM:
             self._shares[share] = BADSEGNUM # ???
             self._bad_segnum = True
+        log.msg("SegmentFetcher(%s)._block_request_activity:"
+                " Share(sh%d-on-%s) -> %s" %
+                (self._node._si_prefix, shnum, share._peerid_s, state),
+                level=log.NOISY, umid="vilNWA")
         eventually(self.loop)
 
 
@@ -1377,6 +1382,13 @@ class _Node:
         self.tail_block_size = r["tail_block_size"]
         log.msg("actual sizes: %s" % (r,),
                 level=log.NOISY, parent=self._lp, umid="PY6P5Q")
+        if (self.segment_size == self.guessed_segment_size
+            and self.num_segments == self.guessed_num_segments):
+            log.msg("my guess was right!",
+                    level=log.NOISY, parent=self._lp, umid="x340Ow")
+        else:
+            log.msg("my guess was wrong! Extra round trips for me.",
+                    level=log.NOISY, parent=self._lp, umid="tb7RJw")
 
         # zfec.Decode() instantiation is fast, but still, let's use the same
         # codec instance for all but the last segment. 3-of-10 takes 15us on

commit beea1b81abdaaedad6f92ab5d893acb05bc7ee79
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 21:40:52 2010 -0700

    cosmetic: stop using 'rdata=self._received' abbreviation
---
 src/allmydata/immutable/download2.py |   27 +++++++++++----------------
 1 files changed, 11 insertions(+), 16 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index e22f19d..22cb4e8 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -309,15 +309,14 @@ class Share:
     def _satisfy_UEB(self):
         o = self.actual_offsets
         fsize = self._fieldsize
-        rdata = self._received
-        UEB_length_s = rdata.get(o["uri_extension"], fsize)
+        UEB_length_s = self._received.get(o["uri_extension"], fsize)
         if not UEB_length_s:
             return False
         (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
-        UEB_s = rdata.pop(o["uri_extension"]+fsize, UEB_length)
+        UEB_s = self._received.pop(o["uri_extension"]+fsize, UEB_length)
         if not UEB_s:
             return False
-        rdata.remove(o["uri_extension"], fsize)
+        self._received.remove(o["uri_extension"], fsize)
         try:
             self._node.validate_and_store_UEB(UEB_s)
             self.actual_segment_size = self._node.segment_size
@@ -337,10 +336,9 @@ class Share:
         # exactly where they are. So fetch everything, and parse the results
         # later.
         o = self.actual_offsets
-        rdata = self._received
         hashlen = o["uri_extension"] - o["share_hashes"]
         assert hashlen % (2+HASH_SIZE) == 0
-        hashdata = rdata.get(o["share_hashes"], hashlen)
+        hashdata = self._received.get(o["share_hashes"], hashlen)
         if not hashdata:
             return False
         share_hashes = {}
@@ -351,7 +349,7 @@ class Share:
         try:
             self._node.process_share_hashes(share_hashes)
             # adds to self._node.share_hash_tree
-            rdata.remove(o["share_hashes"], hashlen)
+            self._received.remove(o["share_hashes"], hashlen)
             return True
         except (BadHashError, NotEnoughHashesError, IndexError):
             f = Failure()
@@ -366,11 +364,10 @@ class Share:
                                   self._storage_index, self._shnum, reason)
 
     def _satisfy_block_hash_tree(self, needed_hashes):
-        o = self.actual_offsets
-        rdata = self._received
+        o_bh = self.actual_offsets["block_hashes"]
         block_hashes = {}
         for hashnum in needed_hashes:
-            hashdata = rdata.get(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+            hashdata = self._received.get(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
             if hashdata:
                 block_hashes[hashnum] = hashdata
             else:
@@ -384,15 +381,14 @@ class Share:
         if not ok:
             return False
         for hashnum in needed_hashes:
-            rdata.remove(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+            self._received.remove(o_bh+hashnum*HASH_SIZE, HASH_SIZE)
         return True
 
     def _satisfy_ciphertext_hash_tree(self, needed_hashes):
         start = self.actual_offsets["crypttext_hash_tree"]
-        rdata = self._received
         ciphertext_hashes = {}
         for hashnum in needed_hashes:
-            hashdata = rdata.get(start+hashnum*HASH_SIZE, HASH_SIZE)
+            hashdata = self._received.get(start+hashnum*HASH_SIZE, HASH_SIZE)
             if hashdata:
                 ciphertext_hashes[hashnum] = hashdata
             else:
@@ -404,7 +400,7 @@ class Share:
         if not ok:
             return False
         for hashnum in needed_hashes:
-            rdata.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
+            self._received.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
         return True
 
     def _satisfy_data_block(self, segnum, observers):
@@ -502,7 +498,6 @@ class Share:
 
     def _desire_UEB(self, o):
         # UEB data is stored as (length,data).
-        rdata = self._received
         if self._overrun_ok:
             # We can pre-fetch 2kb, which should probably cover it. If it
             # turns out to be larger, we'll come back here later with a known
@@ -518,7 +513,7 @@ class Share:
         # probably fetch a huge number
         if not self.actual_offsets:
             return
-        UEB_length_s = rdata.get(o["uri_extension"], self._fieldsize)
+        UEB_length_s = self._received.get(o["uri_extension"], self._fieldsize)
         if UEB_length_s:
             (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
             # we know the length, so make sure we grab everything

commit 9c2e558908ef7bf33ec0db89b94b4990b7168352
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 21:40:08 2010 -0700

    when removing block data from self._received, also remove range from
    self._requested
    
    This allows a second download to re-fetch block data, otherwise we ask for
    zero bytes forever.
---
 src/allmydata/immutable/download2.py |   11 ++++++++---
 1 files changed, 8 insertions(+), 3 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 47ee04a..e22f19d 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -68,6 +68,12 @@ class Share:
 
         self._wanted = Spans() # desired metadata
         self._wanted_blocks = Spans() # desired block data
+        # self._requested contains ranges we've requested before (either
+        # in-flight or answered-yes or answered-no). If they answered-yes,
+        # the corresponding data will be in self._received. If we remove the
+        # data from self._received (i.e. for large data blocks), we'll also
+        # remove the range from self._requested, so that we can ask for it
+        # again later.
         self._requested = Spans() # we've sent a request for this
         # self._received contains data that we haven't yet used
         self._received = DataSpans() # we've received a response for this
@@ -409,10 +415,10 @@ class Share:
         if tail:
             blocklen = self._node.tail_block_size
 
-        rdata = self._received
-        block = rdata.pop(blockstart, blocklen)
+        block = self._received.pop(blockstart, blocklen)
         if not block:
             return False
+        self._requested.remove(blockstart, blocklen)
         # this block is being retired, either as COMPLETE or CORRUPT, since
         # no further data reads will help
         assert self._requested_blocks[0][0] == segnum
@@ -555,7 +561,6 @@ class Share:
                 level=log.NOISY, parent=lp, umid="sgVAyA")
         span = (start, length)
         assert span in self._requested
-        self._requested.remove(start, length)
         self._received.add(start, data)
 
     def _got_error(self, f, start, length, lp):

commit 8fb5b859ebb741f6fa9dd6c5782662d158035380
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 11:47:22 2010 -0700

    client.py: remove now-unused cachedir
---
 src/allmydata/client.py |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/client.py b/src/allmydata/client.py
index a1ed272..b01fbe8 100644
--- a/src/allmydata/client.py
+++ b/src/allmydata/client.py
@@ -16,7 +16,7 @@ from allmydata.immutable.download2_util import Terminator
 from allmydata.immutable.offloaded import Helper
 from allmydata.control import ControlServer
 from allmydata.introducer.client import IntroducerClient
-from allmydata.util import hashutil, base32, pollmixin, cachedir, log
+from allmydata.util import hashutil, base32, pollmixin, log
 from allmydata.util.abbreviate import parse_abbreviated_size
 from allmydata.util.time_format import parse_duration, parse_date
 from allmydata.stats import StatsProvider

commit f90912c12a24fdae4c6658abd072d5fb9faaad1c
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 11:46:38 2010 -0700

    make test_system.SystemTest pass:
    
     * don't track self._received and self._received_data separately. This broke
       the second read (with a shared Node) because we removed the data block
       from one but not the other, preventing us from asking for it again.
     * report NoSharesError separately from NotEnoughSharesError
     * tolerate read() with size= too large (by clipping it)
---
 src/allmydata/immutable/download2.py |   93 ++++++++++++++++++++--------------
 1 files changed, 55 insertions(+), 38 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 3c82a5a..47ee04a 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -8,8 +8,8 @@ from twisted.internet import defer
 from twisted.internet.interfaces import IPushProducer, IConsumer
 
 from foolscap.api import eventually
-from allmydata.interfaces import HASH_SIZE, NotEnoughSharesError, \
-     IImmutableFileNode, IUploadResults
+from allmydata.interfaces import IImmutableFileNode, IUploadResults, \
+     NotEnoughSharesError, NoSharesError, HASH_SIZE
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 from allmydata.util import base32, log, hashutil, mathutil, idlib
@@ -69,8 +69,8 @@ class Share:
         self._wanted = Spans() # desired metadata
         self._wanted_blocks = Spans() # desired block data
         self._requested = Spans() # we've sent a request for this
-        self._received = Spans() # we've received a response for this
-        self._received_data = DataSpans() # the response contents, still unused
+        # self._received contains data that we haven't yet used
+        self._received = DataSpans() # we've received a response for this
         self._requested_blocks = [] # (segnum, set(observer2..))
         ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
         self._overrun_ok = ver["tolerates-immutable-read-overrun"]
@@ -138,7 +138,8 @@ class Share:
          - state=DEAD, f=Failure: the server reported an error, this share
                                   is unusable
         """
-        log.msg("Share.get_block(segnum=%d)" % segnum,
+        log.msg("Share(sh%d-on-%s).get_block(%d)" %
+                (self._shnum, self._peerid_s, segnum),
                 level=log.NOISY, parent=self._lp, umid="RTo9MQ")
         assert segnum >= 0
         o = Observer2()
@@ -177,9 +178,11 @@ class Share:
     def loop(self):
         try:
             # if any exceptions occur here, kill the download
-            log.msg("Share(sh%d on %s).loop, reqs=%s" %
+            log.msg("Share(sh%d on %s).loop, reqs=[%s], wanted=%s, requested=%s, received=%s, wanted_blocks=%s" %
                     (self._shnum, self._peerid_s,
-                     ",".join([str(req[0]) for req in self._requested_blocks])),
+                     ",".join([str(req[0]) for req in self._requested_blocks]),
+                     self._wanted.dump(), self._requested.dump(),
+                     self._received.dump(), self._wanted_blocks.dump() ),
                     level=log.NOISY, parent=self._lp, umid="BaL1zw")
             self._do_loop()
         except BaseException:
@@ -266,7 +269,7 @@ class Share:
         return self._satisfy_data_block(segnum, observers)
 
     def _satisfy_offsets(self):
-        version_s = self._received_data.get(0, 4)
+        version_s = self._received.get(0, 4)
         if version_s is None:
             return False
         (version,) = struct.unpack(">L", version_s)
@@ -279,7 +282,7 @@ class Share:
             self._fieldsize = 0x8
             self._fieldstruct = "Q"
         offset_table_size = 6 * self._fieldsize
-        table_s = self._received_data.pop(table_start, offset_table_size)
+        table_s = self._received.pop(table_start, offset_table_size)
         if table_s is None:
             return False
         fields = struct.unpack(">"+6*self._fieldstruct, table_s)
@@ -294,13 +297,13 @@ class Share:
             offsets[field] = fields[i]
         self.actual_offsets = offsets
         log.msg("actual offsets: data=%d, plaintext_hash_tree=%d, crypttext_hash_tree=%d, block_hashes=%d, share_hashes=%d, uri_extension=%d" % tuple(fields))
-        self._received_data.remove(0, 4) # don't need this anymore
+        self._received.remove(0, 4) # don't need this anymore
         return True
 
     def _satisfy_UEB(self):
         o = self.actual_offsets
         fsize = self._fieldsize
-        rdata = self._received_data
+        rdata = self._received
         UEB_length_s = rdata.get(o["uri_extension"], fsize)
         if not UEB_length_s:
             return False
@@ -328,7 +331,7 @@ class Share:
         # exactly where they are. So fetch everything, and parse the results
         # later.
         o = self.actual_offsets
-        rdata = self._received_data
+        rdata = self._received
         hashlen = o["uri_extension"] - o["share_hashes"]
         assert hashlen % (2+HASH_SIZE) == 0
         hashdata = rdata.get(o["share_hashes"], hashlen)
@@ -358,7 +361,7 @@ class Share:
 
     def _satisfy_block_hash_tree(self, needed_hashes):
         o = self.actual_offsets
-        rdata = self._received_data
+        rdata = self._received
         block_hashes = {}
         for hashnum in needed_hashes:
             hashdata = rdata.get(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
@@ -380,7 +383,7 @@ class Share:
 
     def _satisfy_ciphertext_hash_tree(self, needed_hashes):
         start = self.actual_offsets["crypttext_hash_tree"]
-        rdata = self._received_data
+        rdata = self._received
         ciphertext_hashes = {}
         for hashnum in needed_hashes:
             hashdata = rdata.get(start+hashnum*HASH_SIZE, HASH_SIZE)
@@ -406,7 +409,7 @@ class Share:
         if tail:
             blocklen = self._node.tail_block_size
 
-        rdata = self._received_data
+        rdata = self._received
         block = rdata.pop(blockstart, blocklen)
         if not block:
             return False
@@ -478,7 +481,7 @@ class Share:
         # even if that means more roundtrips.
 
         self._wanted.add(0,4)  # version number, always safe
-        version_s = self._received_data.get(0, 4)
+        version_s = self._received.get(0, 4)
         if not version_s:
             return
         (version,) = struct.unpack(">L", version_s)
@@ -493,7 +496,7 @@ class Share:
 
     def _desire_UEB(self, o):
         # UEB data is stored as (length,data).
-        rdata = self._received_data
+        rdata = self._received
         if self._overrun_ok:
             # We can pre-fetch 2kb, which should probably cover it. If it
             # turns out to be larger, we'll come back here later with a known
@@ -518,12 +521,12 @@ class Share:
     def _request_needed(self):
         # send requests for metadata first, to avoid hanging on to large data
         # blocks any longer than necessary.
-        self._send_requests(self._wanted - self._received - self._requested)
+        received = self._received.get_spans()
+        self._send_requests(self._wanted - received - self._requested)
         # then send requests for data blocks. All the hashes should arrive
         # before the blocks, so the blocks can be consumed and released in a
         # single turn.
-        ask = self._wanted_blocks - self._received - self._requested
-        self._send_requests(ask)
+        self._send_requests(self._wanted_blocks - received - self._requested)
 
     def _send_requests(self, needed):
         for (start, length) in needed:
@@ -553,8 +556,7 @@ class Share:
         span = (start, length)
         assert span in self._requested
         self._requested.remove(start, length)
-        self._received.add(start, length)
-        self._received_data.add(start, data)
+        self._received.add(start, data)
 
     def _got_error(self, f, start, length, lp):
         log.msg(format="error requesting %(start)d+%(length)d"
@@ -761,19 +763,29 @@ class SegmentFetcher:
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
             self.stop()
-            format = ("ran out of shares: %(complete)d complete,"
-                      " %(pending)d pending, %(overdue)d overdue,"
-                      " %(unused)d unused, need %(k)d."
-                      " Last failure: %(last_failure)s")
-            args = {"complete": self._count_shnums(COMPLETE),
-                    "pending": self._count_shnums(PENDING),
-                    "overdue": self._count_shnums(OVERDUE),
-                    "unused": self._count_shnums(AVAILABLE), # should be zero
-                    "k": k,
-                    "last_failure": self._last_failure,
-                    }
+
+            if self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) == 0:
+                format = ("no shares (need %(k)d)."
+                          " Last failure: %(last_failure)s")
+                args = { "k": k,
+                         "last_failure": self._last_failure }
+                error = NoSharesError
+            else:
+                format = ("ran out of shares: %(complete)d complete,"
+                          " %(pending)d pending, %(overdue)d overdue,"
+                          " %(unused)d unused, need %(k)d."
+                          " Last failure: %(last_failure)s")
+                args = {"complete": self._count_shnums(COMPLETE),
+                        "pending": self._count_shnums(PENDING),
+                        "overdue": self._count_shnums(OVERDUE),
+                        # 'unused' should be zero
+                        "unused": self._count_shnums(AVAILABLE),
+                        "k": k,
+                        "last_failure": self._last_failure,
+                        }
+                error = NotEnoughSharesError
             log.msg(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
-            e = NotEnoughSharesError(format % args)
+            e = error(format % args)
             f = Failure(e)
             self._node.fetch_failed(self, f)
             return
@@ -1085,7 +1097,7 @@ class Segmentation:
         # we got file[segment_start:segment_start+len(segment)]
         # we want file[self._offset:self._offset+self._size]
         log.msg(format="Segmentation got data:"
-                " wanted [%(wantstart)d-%(wantend)d),"
+                " want [%(wantstart)d-%(wantend)d),"
                 " given [%(segstart)d-%(segend)d), for segnum=%(segnum)d",
                 wantstart=self._offset, wantend=self._offset+self._size,
                 segstart=segment_start, segend=segment_start+len(segment),
@@ -1106,7 +1118,7 @@ class Segmentation:
                            wanted_segnum, self._node._si_prefix),
                         level=log.WEIRD, parent=self._lp, umid="STlIiA")
                 raise BadSegmentError("Despite knowing the segment size,"
-                                      " we were given the wrong data."
+                                      " I was given the wrong data."
                                       " I cannot cope.")
             # we've wasted some bandwidth, but now we can grab the right one,
             # because we should know the segsize by now.
@@ -1245,10 +1257,15 @@ class _Node:
         """I am the main entry point, from which FileNode.read() can get
         data. I feed the consumer with the desired range of ciphertext. I
         return a Deferred that fires (with the consumer) when the read is
-        finished."""
+        finished.
+
+        Note that there is no notion of a 'file pointer': each call to read()
+        uses an independent offset= value."""
         # for concurrent operations: each gets its own Segmentation manager
         if size is None:
-            size = self._verifycap.size - offset
+            size = self._verifycap.size
+        # clip size so offset+size does not go past EOF
+        size = min(size, self._verifycap.size-offset)
         lp = log.msg(format="imm Node(%(si)s.read(%(offset)d, %(size)d)",
                      si=base32.b2a(self._verifycap.storage_index)[:8],
                      offset=offset, size=size,

commit 2ee69e0d0b07fd6775595d4b50b574841346ed89
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 11:44:36 2010 -0700

    DataSpans: rename get_spans to get_chunks, change get_spans() to return Spans
---
 src/allmydata/test/test_util.py |   24 +++++++++++++++---------
 src/allmydata/util/spans.py     |   13 +++++++++++--
 2 files changed, 26 insertions(+), 11 deletions(-)

diff --git a/src/allmydata/test/test_util.py b/src/allmydata/test/test_util.py
index 5f6ce67..de4a8ad 100644
--- a/src/allmydata/test/test_util.py
+++ b/src/allmydata/test/test_util.py
@@ -1827,7 +1827,7 @@ class SimpleDataSpans:
         self.missing = "" # "1" where missing, "0" where found
         self.data = ""
         if other:
-            for (start, data) in other.get_spans():
+            for (start, data) in other.get_chunks():
                 self.add(start, data)
 
     def __len__(self):
@@ -1839,9 +1839,12 @@ class SimpleDataSpans:
         if not m or len(m)<length or int(m):
             return False
         return True
-    def get_spans(self):
+    def get_chunks(self):
         for i in self._dump():
             yield (i, self.data[i])
+    def get_spans(self):
+        return SimpleSpans([(start,len(data))
+                            for (start,data) in self.get_chunks()])
     def get(self, start, length):
         if self._have(start, length):
             return self.data[start:start+length]
@@ -1866,7 +1869,8 @@ class StringSpans(unittest.TestCase):
         ds = klass()
         self.failUnlessEqual(len(ds), 0)
         self.failUnlessEqual(list(ds._dump()), [])
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 0)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_chunks()]), 0)
+        s = ds.get_spans()
         self.failUnlessEqual(ds.get(0, 4), None)
         self.failUnlessEqual(ds.pop(0, 4), None)
         ds.remove(0, 4)
@@ -1874,7 +1878,9 @@ class StringSpans(unittest.TestCase):
         ds.add(2, "four")
         self.failUnlessEqual(len(ds), 4)
         self.failUnlessEqual(list(ds._dump()), [2,3,4,5])
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 4)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_chunks()]), 4)
+        s = ds.get_spans()
+        self.failUnless((2,2) in s)
         self.failUnlessEqual(ds.get(0, 4), None)
         self.failUnlessEqual(ds.pop(0, 4), None)
         self.failUnlessEqual(ds.get(4, 4), None)
@@ -1882,23 +1888,23 @@ class StringSpans(unittest.TestCase):
         ds2 = klass(ds)
         self.failUnlessEqual(len(ds2), 4)
         self.failUnlessEqual(list(ds2._dump()), [2,3,4,5])
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds2.get_spans()]), 4)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds2.get_chunks()]), 4)
         self.failUnlessEqual(ds2.get(0, 4), None)
         self.failUnlessEqual(ds2.pop(0, 4), None)
         self.failUnlessEqual(ds2.pop(2, 3), "fou")
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds2.get_spans()]), 1)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds2.get_chunks()]), 1)
         self.failUnlessEqual(ds2.get(2, 3), None)
         self.failUnlessEqual(ds2.get(5, 1), "r")
         self.failUnlessEqual(ds.get(2, 3), "fou")
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 4)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_chunks()]), 4)
 
         ds.add(0, "23")
         self.failUnlessEqual(len(ds), 6)
         self.failUnlessEqual(list(ds._dump()), [0,1,2,3,4,5])
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 6)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_chunks()]), 6)
         self.failUnlessEqual(ds.get(0, 4), "23fo")
         self.failUnlessEqual(ds.pop(0, 4), "23fo")
-        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 2)
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_chunks()]), 2)
         self.failUnlessEqual(ds.get(0, 4), None)
         self.failUnlessEqual(ds.pop(0, 4), None)
 
diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
index 336fddf..853d207 100755
--- a/src/allmydata/util/spans.py
+++ b/src/allmydata/util/spans.py
@@ -217,7 +217,7 @@ class DataSpans:
     def __init__(self, other=None):
         self.spans = [] # (start, data) tuples, non-overlapping, merged
         if other:
-            for (start, data) in other.get_spans():
+            for (start, data) in other.get_chunks():
                 self.add(start, data)
 
     def __len__(self):
@@ -230,9 +230,18 @@ class DataSpans:
             for i in range(start, start+len(data)):
                 yield i
 
-    def get_spans(self):
+    def dump(self):
+        return "len=%d: %s" % (len(self),
+                               ",".join(["[%d-%d]" % (start,start+len(data)-1)
+                                         for (start,data) in self.spans]) )
+
+    def get_chunks(self):
         return list(self.spans)
 
+    def get_spans(self):
+        """Return a Spans object with a bit set for each byte I hold"""
+        return Spans([(start, len(data)) for (start,data) in self.spans])
+
     def assert_invariants(self):
         if not self.spans:
             return

commit 4992dc30102909607da5bb984e8569793eb5fd53
Author: Brian Warner <warner@lothar.com>
Date:   Thu Apr 29 10:55:20 2010 -0700

    add a reasonable (perhaps large) set of log.msg calls
---
 src/allmydata/immutable/download2.py |  156 +++++++++++++++++++++++-----------
 1 files changed, 105 insertions(+), 51 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 52fda96..3c82a5a 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -47,7 +47,7 @@ class Share:
     # servers. A different backend would use a different class.
 
     def __init__(self, rref, server_version, verifycap, commonshare, node,
-                 peerid, shnum):
+                 peerid, shnum, logparent):
         self._rref = rref
         self._server_version = server_version
         self._node = node # holds share_hash_tree and UEB
@@ -64,7 +64,7 @@ class Share:
 
         self._lp = log.msg(format="Share(%(si)s) on server=%(server)s starting",
                            si=self._si_prefix, server=self._peerid_s,
-                           level=log.NOISY, umid="P7hv2w")
+                           level=log.NOISY, parent=logparent, umid="P7hv2w")
 
         self._wanted = Spans() # desired metadata
         self._wanted_blocks = Spans() # desired block data
@@ -138,6 +138,8 @@ class Share:
          - state=DEAD, f=Failure: the server reported an error, this share
                                   is unusable
         """
+        log.msg("Share.get_block(segnum=%d)" % segnum,
+                level=log.NOISY, parent=self._lp, umid="RTo9MQ")
         assert segnum >= 0
         o = Observer2()
         o.set_canceler(self._cancel_block_request)
@@ -175,6 +177,10 @@ class Share:
     def loop(self):
         try:
             # if any exceptions occur here, kill the download
+            log.msg("Share(sh%d on %s).loop, reqs=%s" %
+                    (self._shnum, self._peerid_s,
+                     ",".join([str(req[0]) for req in self._requested_blocks])),
+                    level=log.NOISY, parent=self._lp, umid="BaL1zw")
             self._do_loop()
         except BaseException:
             self._fail(Failure())
@@ -523,10 +529,11 @@ class Share:
         for (start, length) in needed:
             # TODO: quantize to reasonably-large blocks
             self._requested.add(start, length)
-            lp = log.msg(format="_send_request(%(peerid)s)"
+            lp = log.msg(format="_send_request(sh%(shnum)d-on-%(peerid)s)"
                          " [%(start)d:+%(length)d]",
-                         peerid=self._peerid_s, start=start, length=length,
-                         level=log.NOISY, umid="sgVAyA")
+                         shnum=self._shnum, peerid=self._peerid_s,
+                         start=start, length=length,
+                         level=log.NOISY, parent=self._lp, umid="sgVAyA")
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length, lp)
             d.addErrback(self._got_error, start, length, lp)
@@ -560,6 +567,9 @@ class Share:
         self._fail(f)
 
     def _fail(self, f):
+        log.msg(format="abandoning Share(sh%(shnum)d-on-%(peerid)s",
+                failure=f, shnum=self._shnum, peerid=self._peerid_s,
+                level=log.UNUSUAL, parent=self._lp, umid="JKM2Og")
         self._dead = True
         for (segnum, observers) in self._requested_blocks:
             for o in observers:
@@ -570,7 +580,7 @@ class CommonShare:
     """I hold data that is common across all instances of a single share,
     like sh2 on both servers A and B. This is just the block hash tree.
     """
-    def __init__(self, guessed_numsegs, si_prefix, shnum):
+    def __init__(self, guessed_numsegs, si_prefix, shnum, logparent):
         self.si_prefix = si_prefix
         self.shnum = shnum
         # in the beginning, before we have the real UEB, we can only guess at
@@ -579,6 +589,7 @@ class CommonShare:
         # numsegs for sure, we return a guess.
         self._block_hash_tree = IncompleteHashTree(guessed_numsegs)
         self._know_numsegs = False
+        self._logparent = logparent
 
     def set_numsegs(self, numsegs):
         if self._know_numsegs:
@@ -587,17 +598,20 @@ class CommonShare:
         self._know_numsegs = True
 
     def need_block_hash_root(self):
-        log.msg("need_block_hash_root: %s" % bool(not self._block_hash_tree[0]))
         return bool(not self._block_hash_tree[0])
 
     def set_block_hash_root(self, roothash):
-        log.msg("set_block_hash_root: %s" % repr(roothash))
+        lp = log.msg("CommonShare.set_block_hash_root: %s" % repr(roothash),
+                     level=log.NOISY, parent=self._logparent, umid="wwG5Gw")
         self._block_hash_tree.set_hashes({0: roothash})
-        log.msg("done with set_block_hash_root")
+        log.msg("done with set_block_hash_root",
+                level=log.NOISY, parent=lp, umid="xjWKcw")
 
     def get_needed_block_hashes(self, segnum):
         needed = ",".join([str(n) for n in sorted(self._block_hash_tree.needed_hashes(segnum))])
-        log.msg("segnum=%d needs %s" % (segnum, needed))
+        log.msg("CommonShare.get_needed_block_hashes: segnum=%d needs %s" %
+                (segnum, needed),
+                level=log.NOISY, parent=self._logparent, umid="6qTMnw")
         # XXX: include_leaf=True needs thought: how did the old downloader do
         # it? I think it grabbed *all* block hashes and set them all at once.
         # Since we want to fetch less data, we either need to fetch the leaf
@@ -617,7 +631,7 @@ class CommonShare:
                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                     hashnums=hashnums, shnum=self.shnum,
                     si=self.si_prefix, server=serverid_s, failure=Failure(),
-                    level=log.WEIRD, umid="yNyFdA")
+                    level=log.WEIRD, parent=self._logparent, umid="yNyFdA")
         return False
 
     def check_block(self, segnum, block, serverid_s):
@@ -631,7 +645,7 @@ class CommonShare:
                      " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                      segnum=segnum, shnum=self.shnum, si=self.si_prefix,
                      server=serverid_s, failure=Failure(),
-                     level=log.WEIRD, umid="mZjkqA")
+                     level=log.WEIRD, parent=self._logparent, umid="mZjkqA")
         return False
 
 # all classes are also Services, and the rule is that you don't initiate more
@@ -845,7 +859,7 @@ class RequestToken:
         self.peerid = peerid
 
 class ShareFinder:
-    def __init__(self, storage_broker, verifycap, node,
+    def __init__(self, storage_broker, verifycap, node, logparent=None,
                  max_outstanding_requests=10):
         self.running = True
         self.verifycap = verifycap
@@ -862,8 +876,10 @@ class ShareFinder:
 
         self._storage_index = verifycap.storage_index
         self._si_prefix = base32.b2a_l(self._storage_index[:8], 60)
+        self._node_logparent = logparent
         self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
-                           si=self._si_prefix, level=log.NOISY, umid="2xjj2A")
+                           si=self._si_prefix,
+                           level=log.NOISY, parent=logparent, umid="2xjj2A")
 
     def log(self, *args, **kwargs):
         if "parent" not in kwargs:
@@ -875,8 +891,8 @@ class ShareFinder:
 
     # called by our parent CiphertextDownloader
     def hungry(self):
-        log.msg(format="ShareFinder[si=%(si)s] hungry",
-                si=self._si_prefix, level=log.NOISY, umid="NywYaQ")
+        self.log(format="ShareFinder[si=%(si)s] hungry",
+                 si=self._si_prefix, level=log.NOISY, umid="NywYaQ")
         self._hungry = True
         eventually(self.loop)
 
@@ -887,12 +903,12 @@ class ShareFinder:
                                   for s in self.undelivered_shares])
         pending_s = ",".join([idlib.shortnodeid_b2a(rt.peerid)
                               for rt in self.pending_requests]) # sort?
-        log.msg(format="ShareFinder[si=%(si)s] loop: running=%(running)s"
-                " hungry=%(hungry)s, undelivered=%(undelivered)s,"
-                " pending=%(pending)s",
-                si=self._si_prefix, running=self.running, hungry=self._hungry,
-                undelivered=undelivered_s, pending=pending_s,
-                level=log.NOISY, umid="kRtS4Q")
+        self.log(format="ShareFinder loop: running=%(running)s"
+                 " hungry=%(hungry)s, undelivered=%(undelivered)s,"
+                 " pending=%(pending)s",
+                 running=self.running, hungry=self._hungry,
+                 undelivered=undelivered_s, pending=pending_s,
+                 level=log.NOISY, umid="kRtS4Q")
         if not self.running:
             return
         if not self._hungry:
@@ -901,6 +917,9 @@ class ShareFinder:
             sh = self.undelivered_shares.pop(0)
             # they will call hungry() again if they want more
             self._hungry = False
+            self.log(format="delivering Share(shnum=%(shnum)d, server=%(peerid)s)",
+                     shnum=sh._shnum, peerid=sh._peerid_s,
+                     level=log.NOISY, umid="2n1qQw")
             eventually(self.share_consumer.got_shares, [sh])
             return
         if len(self.pending_requests) >= self.max_outstanding_requests:
@@ -923,8 +942,8 @@ class ShareFinder:
             # them will make progress
             return
 
-        log.msg(format="ShareFinder.loop: no_more_shares",
-                level=log.UNUSUAL, umid="XjQlzg")
+        self.log(format="ShareFinder.loop: no_more_shares, ever",
+                 level=log.UNUSUAL, umid="XjQlzg")
         # we've run out of servers (so we can't send any more requests), and
         # we have nothing in flight. No further progress can be made. They
         # are destined to remain hungry.
@@ -965,7 +984,8 @@ class ShareFinder:
             if shnum in self._commonshares:
                 cs = self._commonshares[shnum]
             else:
-                cs = CommonShare(best_numsegs, self._si_prefix, shnum)
+                cs = CommonShare(best_numsegs, self._si_prefix, shnum,
+                                 self._node_logparent)
                 # Share._get_satisfaction is responsible for updating
                 # CommonShare.set_numsegs after we know the UEB. Alternatives:
                 #  1: d = self.node.get_num_segments()
@@ -981,7 +1001,7 @@ class ShareFinder:
                 #     Yuck.
                 self._commonshares[shnum] = cs
             s = Share(bucket, server_version, self.verifycap, cs, self.node,
-                      peerid, shnum)
+                      peerid, shnum, self._node_logparent)
             self.undelivered_shares.append(s)
 
     def _got_error(self, f, peerid, req, lp):
@@ -999,7 +1019,7 @@ class Segmentation:
     request one segment at a time.
     """
     implements(IPushProducer)
-    def __init__(self, node, offset, size, consumer):
+    def __init__(self, node, offset, size, consumer, logparent=None):
         self._node = node
         self._hungry = True
         self._active_segnum = None
@@ -1009,6 +1029,7 @@ class Segmentation:
         self._offset = offset
         self._size = size
         self._consumer = consumer
+        self._lp = logparent
 
     def start(self):
         self._alive = True
@@ -1041,11 +1062,15 @@ class Segmentation:
         else:
             # this might be a guess
             wanted_segnum = self._offset // segment_size
+        log.msg(format="_fetch_next(offset=%(offset)d) wants segnum=%(segnum)d",
+                offset=self._offset, segnum=wanted_segnum,
+                level=log.NOISY, parent=self._lp, umid="5WfN0w")
         self._active_segnum = wanted_segnum
-        d,c = n.get_segment(wanted_segnum)
+        d,c = n.get_segment(wanted_segnum, self._lp)
         self._cancel_segment_request = c
         d.addBoth(self._request_retired)
-        d.addCallback(self._got_segment, have_actual_segment_size)
+        d.addCallback(self._got_segment, have_actual_segment_size,
+                      wanted_segnum)
         d.addErrback(self._retry_bad_segment, have_actual_segment_size)
         d.addErrback(self._error)
 
@@ -1054,12 +1079,19 @@ class Segmentation:
         self._cancel_segment_request = None
         return res
 
-    def _got_segment(self, (segment_start,segment), had_actual_segment_size):
-        segnum = self._active_segnum
-        self._active_segnum = None
+    def _got_segment(self, (segment_start,segment), had_actual_segment_size,
+                     wanted_segnum):
         self._cancel_segment_request = None
         # we got file[segment_start:segment_start+len(segment)]
         # we want file[self._offset:self._offset+self._size]
+        log.msg(format="Segmentation got data:"
+                " wanted [%(wantstart)d-%(wantend)d),"
+                " given [%(segstart)d-%(segend)d), for segnum=%(segnum)d",
+                wantstart=self._offset, wantend=self._offset+self._size,
+                segstart=segment_start, segend=segment_start+len(segment),
+                segnum=wanted_segnum,
+                level=log.OPERATIONAL, parent=self._lp, umid="32dHcg")
+
         o = overlap(segment_start, len(segment),  self._offset, self._size)
         # the overlap is file[o[0]:o[0]+o[1]]
         if not o or o[0] != self._offset:
@@ -1071,8 +1103,8 @@ class Segmentation:
                         " for si=%s"
                         % (self._offset, self._offset+self._size,
                            segment_start, segment_start+len(segment),
-                           segnum, self._node._si_prefix),
-                        level=log.WEIRD, umid="STlIiA")
+                           wanted_segnum, self._node._si_prefix),
+                        level=log.WEIRD, parent=self._lp, umid="STlIiA")
                 raise BadSegmentError("Despite knowing the segment size,"
                                       " we were given the wrong data."
                                       " I cannot cope.")
@@ -1100,6 +1132,8 @@ class Segmentation:
         return self._maybe_fetch_next()
 
     def _error(self, f):
+        log.msg("Error in Segmentation",
+                level=log.WEIRD, parent=self._lp, umid="EYlXBg")
         self._alive = False
         self._hungry = False
         self._consumer.unregisterProducer()
@@ -1181,7 +1215,20 @@ class _Node:
         self._segment_requests = [] # (segnum, d, cancel_handle)
         self._active_segment = None # a SegmentFetcher, with .segnum
 
-        self._sharefinder = ShareFinder(storage_broker, verifycap, self)
+        # we create one top-level logparent for this _Node, and another one
+        # for each read() call. Segmentation and get_segment() messages are
+        # associated with the read() call, everything else is tied to the
+        # _Node's log entry.
+        lp = log.msg(format="Immutable _Node(%(si)s) created: size=%(size)d,"
+                     " guessed_segsize=%(guessed_segsize)d,"
+                     " guessed_numsegs=%(guessed_numsegs)d",
+                     si=self._si_prefix, size=verifycap.size,
+                     guessed_segsize=self.guessed_segment_size,
+                     guessed_numsegs=self.guessed_num_segments,
+                     level=log.OPERATIONAL, umid="uJ0zAQ")
+        self._lp = lp
+
+        self._sharefinder = ShareFinder(storage_broker, verifycap, self, lp)
         self._shares = set()
 
     def stop(self):
@@ -1202,14 +1249,14 @@ class _Node:
         # for concurrent operations: each gets its own Segmentation manager
         if size is None:
             size = self._verifycap.size - offset
-        log.msg(format="imm Node(%(si)s.read(%(offset)d, %(size)d)",
-                si=base32.b2a(self._verifycap.storage_index)[:8],
-                offset=offset, size=size,
-                level=log.OPERATIONAL, umid="l3j3Ww")
+        lp = log.msg(format="imm Node(%(si)s.read(%(offset)d, %(size)d)",
+                     si=base32.b2a(self._verifycap.storage_index)[:8],
+                     offset=offset, size=size,
+                     level=log.OPERATIONAL, parent=self._lp, umid="l3j3Ww")
         sp = self._history.stats_provider
         sp.count("downloader.files_downloaded", 1) # really read() calls
         sp.count("downloader.bytes_downloaded", size)
-        s = Segmentation(self, offset, size, consumer)
+        s = Segmentation(self, offset, size, consumer, lp)
         # this raises an interesting question: what segments to fetch? if
         # offset=0, always fetch the first segment, and then allow
         # Segmentation to be responsible for pulling the subsequent ones if
@@ -1221,7 +1268,7 @@ class _Node:
         d = s.start()
         return d
 
-    def get_segment(self, segnum):
+    def get_segment(self, segnum, logparent=None):
         """Begin downloading a segment. I return a tuple (d, c): 'd' is a
         Deferred that fires with (offset,data) when the desired segment is
         available, and c is an object on which c.cancel() can be called to
@@ -1239,9 +1286,10 @@ class _Node:
         The Deferred can also errback with other fatal problems, such as
         NotEnoughSharesError, NoSharesError, or BadCiphertextHashError.
         """
-        log.msg(format="imm Node(%(si)s.get_segment(%(segnum)d)",
+        log.msg(format="imm Node(%(si)s).get_segment(%(segnum)d)",
                 si=base32.b2a(self._verifycap.storage_index)[:8],
-                segnum=segnum, level=log.OPERATIONAL, umid="UKFjDQ")
+                segnum=segnum,
+                level=log.OPERATIONAL, parent=logparent, umid="UKFjDQ")
         d = defer.Deferred()
         c = Cancel(self._cancel_request)
         self._segment_requests.append( (segnum, d, c) )
@@ -1274,7 +1322,7 @@ class _Node:
 
     def validate_and_store_UEB(self, UEB_s):
         log.msg("validate_and_store_UEB",
-                level=log.OPERATIONAL, umid="7sTrPw")
+                level=log.OPERATIONAL, parent=self._lp, umid="7sTrPw")
         h = hashutil.uri_extension_hash(UEB_s)
         if h != self._verifycap.uri_extension_hash:
             raise hashutil.BadHashError
@@ -1298,7 +1346,7 @@ class _Node:
 
         log.msg(format="UEB=%(ueb)s, vcap=%(vcap)s",
                 ueb=repr(d), vcap=self._verifycap.to_string(),
-                level=log.NOISY, umid="cVqZnA")
+                level=log.NOISY, parent=self._lp, umid="cVqZnA")
 
         k, N = self._verifycap.needed_shares, self._verifycap.total_shares
 
@@ -1311,7 +1359,7 @@ class _Node:
         self.block_size = r["block_size"]
         self.tail_block_size = r["tail_block_size"]
         log.msg("actual sizes: %s" % (r,),
-                level=log.NOISY, umid="PY6P5Q")
+                level=log.NOISY, parent=self._lp, umid="PY6P5Q")
 
         # zfec.Decode() instantiation is fast, but still, let's use the same
         # codec instance for all but the last segment. 3-of-10 takes 15us on
@@ -1340,7 +1388,7 @@ class _Node:
                 log.msg("ignoring bad-length UEB[crypttext_hash], "
                         "got %d bytes, want %d" % (len(d['crypttext_hash']),
                                                    hashutil.CRYPTO_VAL_SIZE),
-                        umid="oZkGLA", level=log.WEIRD)
+                        level=log.WEIRD, parent=self._lp, umid="oZkGLA")
 
         # Our job is a fast download, not verification, so we ignore any
         # redundant fields. The Verifier uses a different code path which
@@ -1396,7 +1444,7 @@ class _Node:
                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                     hashnums=hashnums, shnum=shnum,
                     si=self._si_prefix, server=serverid_s, failure=Failure(),
-                    level=log.WEIRD, umid="iZI0TA")
+                    level=log.WEIRD, parent=self._lp, umid="iZI0TA")
         return False
 
     # called by our child SegmentFetcher
@@ -1415,14 +1463,19 @@ class _Node:
         d = defer.maybeDeferred(self._decode_blocks, segnum, blocks)
         d.addCallback(self._check_ciphertext_hash, segnum)
         def _deliver(result):
+            log.msg(format="delivering segment(%(segnum)d)",
+                    segnum=segnum,
+                    level=log.OPERATIONAL, parent=self._lp,
+                    umid="j60Ojg")
             for (d,c) in self._extract_requests(segnum):
                 eventually(self._deliver, d, c, result)
             self._active_segment = None
             self._start_new_segment()
         d.addBoth(_deliver)
         d.addErrback(lambda f:
-                     log.err(format="unhandled error during process_blocks",
-                             failure=f, level=log.WEIRD, umid="MkEsCg"))
+                     log.err("unhandled error during process_blocks",
+                             failure=f, level=log.WEIRD,
+                             parent=self._lp, umid="MkEsCg"))
 
     def _decode_blocks(self, segnum, blocks):
         tail = (segnum == self.num_segments-1)
@@ -1470,7 +1523,8 @@ class _Node:
             format = ("hash failure in ciphertext_hash_tree:"
                       " segnum=%(segnum)d, SI=%(si)s")
             log.msg(format=format, segnum=segnum, si=self._si_prefix,
-                    failure=Failure(), level=log.WEIRD, umid="MTwNnw")
+                    failure=Failure(),
+                    level=log.WEIRD, parent=self._lp, umid="MTwNnw")
             # this is especially weird, because we made it past the share
             # hash tree. It implies that we're using the wrong encoding, or
             # that the uploader deliberately constructed a bad UEB.

commit 6e133fa6bbcb24fad4fe428165938c8eead4767b
Author: Brian Warner <warner@lothar.com>
Date:   Wed Apr 28 08:44:11 2010 -0700

    added Checker, temporarily disabled status/history test.
    
    SystemTest.test_filesystem now passes. test_upload_and_download_convergent
    hangs.
---
 src/allmydata/immutable/download2.py |  104 +++++++++++++++++++++++++++++++---
 src/allmydata/test/test_system.py    |    5 +-
 2 files changed, 99 insertions(+), 10 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index e1b3ac3..52fda96 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1,6 +1,7 @@
 
 import binascii
 import struct
+import copy
 from zope.interface import implements
 from twisted.python.failure import Failure
 from twisted.internet import defer
@@ -8,17 +9,20 @@ from twisted.internet.interfaces import IPushProducer, IConsumer
 
 from foolscap.api import eventually
 from allmydata.interfaces import HASH_SIZE, NotEnoughSharesError, \
-     IImmutableFileNode
+     IImmutableFileNode, IUploadResults
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 from allmydata.util import base32, log, hashutil, mathutil, idlib
 from allmydata.util.spans import Spans, DataSpans, overlap
 from allmydata.util.dictutil import DictOfSets
+from allmydata.check_results import CheckResults, CheckAndRepairResults
 from allmydata.codec import CRSDecoder
 from allmydata import uri
 from pycryptopp.cipher.aes import AES
 from download2_util import Observer2, incidentally
 from layout import make_write_bucket_proxy
+from checker import Checker
+from repairer import Repairer
 
 (AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
  ("AVAILABLE", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
@@ -378,8 +382,8 @@ class Share:
                 ciphertext_hashes[hashnum] = hashdata
             else:
                 return False # missing some hashes
-        # note that we don't submit any hashes to the ciphertext_hash_tree
-        # until we've gotten them all
+        # we don't submit any hashes to the ciphertext_hash_tree until we've
+        # gotten them all
         ok = self._node.process_ciphertext_hashes(ciphertext_hashes,
                                                   self._shnum, self._peerid_s)
         if not ok:
@@ -1132,6 +1136,7 @@ class _Node:
                  terminator, history):
         assert isinstance(verifycap, uri.CHKFileVerifierURI)
         self._verifycap = verifycap
+        self._storage_broker = storage_broker
         self._si_prefix = base32.b2a_l(verifycap.storage_index[:8], 60)
         self.running = True
         terminator.register(self) # calls self.stop() at stopService()
@@ -1201,6 +1206,9 @@ class _Node:
                 si=base32.b2a(self._verifycap.storage_index)[:8],
                 offset=offset, size=size,
                 level=log.OPERATIONAL, umid="l3j3Ww")
+        sp = self._history.stats_provider
+        sp.count("downloader.files_downloaded", 1) # really read() calls
+        sp.count("downloader.bytes_downloaded", size)
         s = Segmentation(self, offset, size, consumer)
         # this raises an interesting question: what segments to fetch? if
         # offset=0, always fetch the first segment, and then allow
@@ -1375,15 +1383,15 @@ class _Node:
         self.share_hash_tree.set_hashes(share_hashes)
 
     def get_needed_ciphertext_hashes(self, segnum):
-        return self.ciphertext_hash_tree.needed_hashes(segnum,
-                                                       include_leaf=True)
-    def process_ciphertext_hashes(self, ciphertext_hashes, shnum, serverid_s):
+        cht = self.ciphertext_hash_tree
+        return cht.needed_hashes(segnum, include_leaf=True)
+    def process_ciphertext_hashes(self, hashes, shnum, serverid_s):
         assert self.num_segments is not None
         try:
-            self.ciphertext_hash_tree.set_hashes(ciphertext_hashes)
+            self.ciphertext_hash_tree.set_hashes(hashes)
             return True
         except (BadHashError, NotEnoughHashesError):
-            hashnums = ",".join([str(n) for n in sorted(ciphertext_hashes.keys())])
+            hashnums = ",".join([str(n) for n in sorted(hashes.keys())])
             log.msg(format="hash failure in ciphertext_hashes=(%(hashnums)s),"
                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                     hashnums=hashnums, shnum=shnum,
@@ -1493,6 +1501,76 @@ class _Node:
             self._active_segment = None
             self._start_new_segment()
 
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        verifycap = self._verifycap
+        storage_index = verifycap.storage_index
+        sb = self._storage_broker
+        servers = sb.get_all_servers()
+        sh = self._secret_holder
+
+        c = Checker(verifycap=verifycap, servers=servers,
+                    verify=verify, add_lease=add_lease, secret_holder=sh,
+                    monitor=monitor)
+        d = c.start()
+        def _maybe_repair(cr):
+            crr = CheckAndRepairResults(storage_index)
+            crr.pre_repair_results = cr
+            if cr.is_healthy():
+                crr.post_repair_results = cr
+                return defer.succeed(crr)
+            else:
+                crr.repair_attempted = True
+                crr.repair_successful = False # until proven successful
+                def _gather_repair_results(ur):
+                    assert IUploadResults.providedBy(ur), ur
+                    # clone the cr (check results) to form the basis of the
+                    # prr (post-repair results)
+                    prr = CheckResults(cr.uri, cr.storage_index)
+                    prr.data = copy.deepcopy(cr.data)
+
+                    sm = prr.data['sharemap']
+                    assert isinstance(sm, DictOfSets), sm
+                    sm.update(ur.sharemap)
+                    servers_responding = set(prr.data['servers-responding'])
+                    servers_responding.union(ur.sharemap.iterkeys())
+                    prr.data['servers-responding'] = list(servers_responding)
+                    prr.data['count-shares-good'] = len(sm)
+                    prr.data['count-good-share-hosts'] = len(sm)
+                    is_healthy = bool(len(sm) >= verifycap.total_shares)
+                    is_recoverable = bool(len(sm) >= verifycap.needed_shares)
+                    prr.set_healthy(is_healthy)
+                    prr.set_recoverable(is_recoverable)
+                    crr.repair_successful = is_healthy
+                    prr.set_needs_rebalancing(len(sm) >= verifycap.total_shares)
+
+                    crr.post_repair_results = prr
+                    return crr
+                def _repair_error(f):
+                    # as with mutable repair, I'm not sure if I want to pass
+                    # through a failure or not. TODO
+                    crr.repair_successful = False
+                    crr.repair_failure = f
+                    return f
+                r = Repairer(storage_broker=sb, secret_holder=sh,
+                             verifycap=verifycap, monitor=monitor)
+                d = r.start()
+                d.addCallbacks(_gather_repair_results, _repair_error)
+                return d
+
+        d.addCallback(_maybe_repair)
+        return d
+
+    def check(self, monitor, verify=False, add_lease=False):
+        verifycap = self._verifycap
+        sb = self._storage_broker
+        servers = sb.get_all_servers()
+        sh = self._secret_holder
+
+        v = Checker(verifycap=verifycap, servers=servers,
+                    verify=verify, add_lease=add_lease, secret_holder=sh,
+                    monitor=monitor)
+        return v.start()
+
 class CiphertextFileNode:
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history):
@@ -1528,6 +1606,12 @@ class CiphertextFileNode:
         pass
 
 
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        return self._node.check_and_repair(monitor, verify, add_lease)
+    def check(self, monitor, verify=False, add_lease=False):
+        return self._node.check(monitor, verify, add_lease)
+
+
 class DecryptingConsumer:
     """I sit between a CiphertextDownloader (which acts as a Producer) and
     the real Consumer, decrypting everything that passes by. The real
@@ -1621,6 +1705,10 @@ class ImmutableFileNode:
     def is_allowed_in_immutable_directory(self):
         return True
 
+    def check_and_repair(self, monitor, verify=False, add_lease=False):
+        return self._cnode.check_and_repair(monitor, verify, add_lease)
+    def check(self, monitor, verify=False, add_lease=False):
+        return self._cnode.check(monitor, verify, add_lease)
 
 # TODO: if server1 has all shares, and server2-10 have one each, make the
 # loop stall slightly before requesting all shares from the first server, to
diff --git a/src/allmydata/test/test_system.py b/src/allmydata/test/test_system.py
index 0ad6c35..560ed6e 100644
--- a/src/allmydata/test/test_system.py
+++ b/src/allmydata/test/test_system.py
@@ -9,7 +9,8 @@ from allmydata import uri
 from allmydata.storage.mutable import MutableShareFile
 from allmydata.storage.server import si_a2b
 from allmydata.immutable import offloaded, upload
-from allmydata.immutable.filenode import ImmutableFileNode, LiteralFileNode
+from allmydata.immutable.filenode import LiteralFileNode
+from allmydata.immutable.download2 import ImmutableFileNode
 from allmydata.util import idlib, mathutil
 from allmydata.util import log, base32
 from allmydata.util.consumer import MemoryConsumer, download_to_data
@@ -1175,7 +1176,7 @@ class SystemTest(SystemTestMixin, unittest.TestCase):
         d.addCallback(_got_status)
         def _got_up(res):
             return self.GET("status/down-%d" % self._down_status)
-        d.addCallback(_got_up)
+        #d.addCallback(_got_up)
         def _got_down(res):
             return self.GET("status/mapupdate-%d" % self._update_status)
         d.addCallback(_got_down)

commit 093593c9c4a5a06eebc882bc8235174207fdb913
Author: Brian Warner <warner@lothar.com>
Date:   Mon Apr 26 03:07:18 2010 -0700

    get the first part of test_system to work: now it fails for lack of
    status/history
---
 src/allmydata/immutable/download2.py |   64 +++++++++++++++++++++++++++++++++-
 1 files changed, 63 insertions(+), 1 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 83f8a47..e1b3ac3 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -77,6 +77,16 @@ class Share:
         # 2=offset table, 3=UEB_length and everything else (hashes, block),
         # 4=UEB.
 
+        self._dead = False
+
+    def not_dead(self):
+        # XXX: reconsider. If the share sees a single error, should it remain
+        # dead for all time? Or should the next segment try again? Also,
+        # 'not_dead' is a dorky method name. This DEAD state is stored
+        # elsewhere too (SegmentFetcher per-share states?) and needs to be
+        # consistent.
+        return not self._dead
+
     def _guess_offsets(self, verifycap, guessed_segment_size):
         self.guessed_segment_size = guessed_segment_size
         size = verifycap.size
@@ -235,6 +245,13 @@ class Share:
                 # can't check block without block_hash_tree
                 return False
 
+        # ciphertext_hash_tree
+        needed_hashes = self._node.get_needed_ciphertext_hashes(segnum)
+        if needed_hashes:
+            if not self._satisfy_ciphertext_hash_tree(needed_hashes):
+                # can't check decoded blocks without ciphertext_hash_tree
+                return False
+
         # data blocks
         return self._satisfy_data_block(segnum, observers)
 
@@ -351,6 +368,26 @@ class Share:
             rdata.remove(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
         return True
 
+    def _satisfy_ciphertext_hash_tree(self, needed_hashes):
+        start = self.actual_offsets["crypttext_hash_tree"]
+        rdata = self._received_data
+        ciphertext_hashes = {}
+        for hashnum in needed_hashes:
+            hashdata = rdata.get(start+hashnum*HASH_SIZE, HASH_SIZE)
+            if hashdata:
+                ciphertext_hashes[hashnum] = hashdata
+            else:
+                return False # missing some hashes
+        # note that we don't submit any hashes to the ciphertext_hash_tree
+        # until we've gotten them all
+        ok = self._node.process_ciphertext_hashes(ciphertext_hashes,
+                                                  self._shnum, self._peerid_s)
+        if not ok:
+            return False
+        for hashnum in needed_hashes:
+            rdata.remove(start+hashnum*HASH_SIZE, HASH_SIZE)
+        return True
+
     def _satisfy_data_block(self, segnum, observers):
         tail = (segnum == self._node.num_segments-1)
         datastart = self.actual_offsets["data"]
@@ -405,6 +442,10 @@ class Share:
         for hashnum in commonshare.get_needed_block_hashes(segnum):
             self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
+        # ciphertext hash chain
+        for hashnum in self._node.get_needed_ciphertext_hashes(segnum):
+            self._wanted.add(o["crypttext_hash_tree"]+hashnum*HASH_SIZE, HASH_SIZE)
+
         # data
         r = self._node._calculate_sizes(segsize)
         tail = (segnum == r["num_segments"])
@@ -515,6 +556,7 @@ class Share:
         self._fail(f)
 
     def _fail(self, f):
+        self._dead = True
         for (segnum, observers) in self._requested_blocks:
             for o in observers:
                 o.notify(state=DEAD, f=f)
@@ -1113,6 +1155,9 @@ class _Node:
         self.guessed_segment_size = s
         r = self._calculate_sizes(self.guessed_segment_size)
         self.guessed_num_segments = r["num_segments"]
+        # as with CommonShare, our ciphertext_hash_tree is a stub until we
+        # get the real num_segments
+        self.ciphertext_hash_tree = IncompleteHashTree(self.guessed_num_segments)
 
         # filled in when we parse a valid UEB
         self.have_UEB = False
@@ -1122,7 +1167,7 @@ class _Node:
         self.num_segments = None
         self.block_size = None
         self.tail_block_size = None
-        self.ciphertext_hash_tree = None # size depends on num_segments
+        #self.ciphertext_hash_tree = None # size depends on num_segments
         self.ciphertext_hash = None # flat hash, optional
 
         # things to track callers that want data
@@ -1329,6 +1374,23 @@ class _Node:
     def process_share_hashes(self, share_hashes):
         self.share_hash_tree.set_hashes(share_hashes)
 
+    def get_needed_ciphertext_hashes(self, segnum):
+        return self.ciphertext_hash_tree.needed_hashes(segnum,
+                                                       include_leaf=True)
+    def process_ciphertext_hashes(self, ciphertext_hashes, shnum, serverid_s):
+        assert self.num_segments is not None
+        try:
+            self.ciphertext_hash_tree.set_hashes(ciphertext_hashes)
+            return True
+        except (BadHashError, NotEnoughHashesError):
+            hashnums = ",".join([str(n) for n in sorted(ciphertext_hashes.keys())])
+            log.msg(format="hash failure in ciphertext_hashes=(%(hashnums)s),"
+                    " shnum=%(shnum)d SI=%(si)s server=%(server)s",
+                    hashnums=hashnums, shnum=shnum,
+                    si=self._si_prefix, server=serverid_s, failure=Failure(),
+                    level=log.WEIRD, umid="iZI0TA")
+        return False
+
     # called by our child SegmentFetcher
 
     def want_more_shares(self):

commit 7e4cdb5bf05008c47fce57af4463b75d3e9e74fa
Author: Brian Warner <warner@lothar.com>
Date:   Mon Apr 26 02:39:20 2010 -0700

    lurching towards functionality: add IImmutableFileNode cruft, work on
    multi-segment hash trees
---
 src/allmydata/immutable/download2.py |   91 +++++++++++++++++++++++++++++----
 1 files changed, 80 insertions(+), 11 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 95f88c4..83f8a47 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -7,7 +7,8 @@ from twisted.internet import defer
 from twisted.internet.interfaces import IPushProducer, IConsumer
 
 from foolscap.api import eventually
-from allmydata.interfaces import HASH_SIZE, NotEnoughSharesError
+from allmydata.interfaces import HASH_SIZE, NotEnoughSharesError, \
+     IImmutableFileNode
 from allmydata.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 from allmydata.util import base32, log, hashutil, mathutil, idlib
@@ -205,7 +206,8 @@ class Share:
         # knowing the UEB means knowing num_segments. Despite the redundancy,
         # this is the best place to set this. CommonShare.set_numsegs will
         # ignore duplicate calls.
-        self._commonshare.set_numsegs(self._node.num_segments)
+        cs = self._commonshare
+        cs.set_numsegs(self._node.num_segments)
 
         segnum, observers = self._active_segnum_and_observers()
         if segnum >= self._node.num_segments:
@@ -219,6 +221,10 @@ class Share:
                 # can't check block_hash_tree without a root
                 return False
 
+        if cs.need_block_hash_root():
+            block_hash_root = self._node.share_hash_tree.get_leaf(self._shnum)
+            cs.set_block_hash_root(block_hash_root)
+
         if segnum is None:
             return False # we don't want any particular segment right now
 
@@ -534,21 +540,38 @@ class CommonShare:
         self._block_hash_tree = IncompleteHashTree(numsegs)
         self._know_numsegs = True
 
+    def need_block_hash_root(self):
+        log.msg("need_block_hash_root: %s" % bool(not self._block_hash_tree[0]))
+        return bool(not self._block_hash_tree[0])
+
+    def set_block_hash_root(self, roothash):
+        log.msg("set_block_hash_root: %s" % repr(roothash))
+        self._block_hash_tree.set_hashes({0: roothash})
+        log.msg("done with set_block_hash_root")
+
     def get_needed_block_hashes(self, segnum):
-        return self._block_hash_tree.needed_hashes(segnum)
+        needed = ",".join([str(n) for n in sorted(self._block_hash_tree.needed_hashes(segnum))])
+        log.msg("segnum=%d needs %s" % (segnum, needed))
+        # XXX: include_leaf=True needs thought: how did the old downloader do
+        # it? I think it grabbed *all* block hashes and set them all at once.
+        # Since we want to fetch less data, we either need to fetch the leaf
+        # too, or wait to set the block hashes until we've also received the
+        # block itself, so we can hash it too, and set the chain+leaf all at
+        # the same time.
+        return self._block_hash_tree.needed_hashes(segnum, include_leaf=True)
 
     def process_block_hashes(self, block_hashes, serverid_s):
         assert self._know_numsegs
         try:
-            self._block_hash_tree.add_hashes(block_hashes)
+            self._block_hash_tree.set_hashes(block_hashes)
             return True
         except (BadHashError, NotEnoughHashesError):
-            hashnums = ",".join(sorted(block_hashes.keys()))
-            self.log("hash failure in block_hashes=(%(hashnums)s),"
-                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
-                     hashnums=hashnums, shnum=self.shnum,
-                     si=self.si_prefix, server=serverid_s, failure=Failure(),
-                     level=log.WEIRD, umid="yNyFdA")
+            hashnums = ",".join([str(n) for n in sorted(block_hashes.keys())])
+            log.msg(format="hash failure in block_hashes=(%(hashnums)s),"
+                    " shnum=%(shnum)d SI=%(si)s server=%(server)s",
+                    hashnums=hashnums, shnum=self.shnum,
+                    si=self.si_prefix, server=serverid_s, failure=Failure(),
+                    level=log.WEIRD, umid="yNyFdA")
         return False
 
     def check_block(self, segnum, block, serverid_s):
@@ -558,7 +581,7 @@ class CommonShare:
             self._block_hash_tree.set_hashes(leaves={segnum: h})
             return True
         except (BadHashError, NotEnoughHashesError):
-            self.log("hash failure in block %(segnum)d,"
+            self.log(format="hash failure in block %(segnum)d,"
                      " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                      segnum=segnum, shnum=self.shnum, si=self.si_prefix,
                      server=serverid_s, failure=Failure(),
@@ -1439,6 +1462,9 @@ class CiphertextFileNode:
         """
         return self._node.get_segment(segnum)
 
+    def raise_error(self):
+        pass
+
 
 class DecryptingConsumer:
     """I sit between a CiphertextDownloader (which acts as a Producer) and
@@ -1473,6 +1499,8 @@ class DecryptingConsumer:
         self._consumer.write(plaintext)
 
 class ImmutableFileNode:
+    implements(IImmutableFileNode)
+
     # I wrap a CiphertextFileNode with a decryption key
     def __init__(self, filecap, storage_broker, secret_holder, terminator,
                  history):
@@ -1490,6 +1518,47 @@ class ImmutableFileNode:
         d.addCallback(lambda dc: consumer)
         return d
 
+    def raise_error(self):
+        pass
+
+    def get_write_uri(self):
+        return None
+
+    def get_readonly_uri(self):
+        return self.get_uri()
+
+    def get_uri(self):
+        return self.u.to_string()
+    def get_cap(self):
+        return self.u
+    def get_readcap(self):
+        return self.u.get_readonly()
+    def get_verify_cap(self):
+        return self.u.get_verify_cap()
+    def get_repair_cap(self):
+        # CHK files can be repaired with just the verifycap
+        return self.u.get_verify_cap()
+
+    def get_storage_index(self):
+        return self.u.get_storage_index()
+
+    def get_size(self):
+        return self.u.get_size()
+    def get_current_size(self):
+        return defer.succeed(self.get_size())
+
+    def is_mutable(self):
+        return False
+
+    def is_readonly(self):
+        return True
+
+    def is_unknown(self):
+        return False
+
+    def is_allowed_in_immutable_directory(self):
+        return True
+
 
 # TODO: if server1 has all shares, and server2-10 have one each, make the
 # loop stall slightly before requesting all shares from the first server, to

commit 68a36b888921753825be6552072ba4163cc51fdf
Author: Brian Warner <warner@lothar.com>
Date:   Mon Apr 26 02:01:19 2010 -0700

    start of downloader2 integration
---
 src/allmydata/client.py    |   12 ++++--------
 src/allmydata/nodemaker.py |   14 ++++++--------
 2 files changed, 10 insertions(+), 16 deletions(-)

diff --git a/src/allmydata/client.py b/src/allmydata/client.py
index 12e7473..a1ed272 100644
--- a/src/allmydata/client.py
+++ b/src/allmydata/client.py
@@ -12,7 +12,7 @@ import allmydata
 from allmydata.storage.server import StorageServer
 from allmydata import storage_client
 from allmydata.immutable.upload import Uploader
-from allmydata.immutable.download import Downloader
+from allmydata.immutable.download2_util import Terminator
 from allmydata.immutable.offloaded import Helper
 from allmydata.control import ControlServer
 from allmydata.introducer.client import IntroducerClient
@@ -278,12 +278,9 @@ class Client(node.Node, pollmixin.PollMixin):
 
         self.init_client_storage_broker()
         self.history = History(self.stats_provider)
+        self.terminator = Terminator()
+        self.terminator.setServiceParent(self)
         self.add_service(Uploader(helper_furl, self.stats_provider))
-        download_cachedir = os.path.join(self.basedir,
-                                         "private", "cache", "download")
-        self.download_cache_dirman = cachedir.CacheDirectoryManager(download_cachedir)
-        self.download_cache_dirman.setServiceParent(self)
-        self.downloader = Downloader(self.storage_broker, self.stats_provider)
         self.init_stub_client()
         self.init_nodemaker()
 
@@ -342,8 +339,7 @@ class Client(node.Node, pollmixin.PollMixin):
                                    self._secret_holder,
                                    self.get_history(),
                                    self.getServiceNamed("uploader"),
-                                   self.downloader,
-                                   self.download_cache_dirman,
+                                   self.terminator,
                                    self.get_encoding_parameters(),
                                    self._key_generator)
 
diff --git a/src/allmydata/nodemaker.py b/src/allmydata/nodemaker.py
index 3d0819e..8389301 100644
--- a/src/allmydata/nodemaker.py
+++ b/src/allmydata/nodemaker.py
@@ -1,7 +1,7 @@
 import weakref
 from zope.interface import implements
-from allmydata.interfaces import INodeMaker
-from allmydata.immutable.filenode import ImmutableFileNode, LiteralFileNode
+from allmydata.immutable.filenode import LiteralFileNode
+from allmydata.immutable.download2 import ImmutableFileNode
 from allmydata.immutable.upload import Data
 from allmydata.mutable.filenode import MutableFileNode
 from allmydata.dirnode import DirectoryNode, pack_children
@@ -16,14 +16,13 @@ class NodeMaker:
     implements(INodeMaker)
 
     def __init__(self, storage_broker, secret_holder, history,
-                 uploader, downloader, download_cache_dirman,
+                 uploader, terminator,
                  default_encoding_parameters, key_generator):
         self.storage_broker = storage_broker
         self.secret_holder = secret_holder
         self.history = history
         self.uploader = uploader
-        self.downloader = downloader
-        self.download_cache_dirman = download_cache_dirman
+        self.terminator = terminator
         self.default_encoding_parameters = default_encoding_parameters
         self.key_generator = key_generator
 
@@ -33,8 +32,7 @@ class NodeMaker:
         return LiteralFileNode(cap)
     def _create_immutable(self, cap):
         return ImmutableFileNode(cap, self.storage_broker, self.secret_holder,
-                                 self.downloader, self.history,
-                                 self.download_cache_dirman)
+                                 self.terminator, self.history)
     def _create_mutable(self, cap):
         n = MutableFileNode(self.storage_broker, self.secret_holder,
                             self.default_encoding_parameters,
@@ -47,7 +45,7 @@ class NodeMaker:
         # this returns synchronously. It starts with a "cap string".
         assert isinstance(writecap, (str, type(None))), type(writecap)
         assert isinstance(readcap,  (str, type(None))), type(readcap)
-        
+
         bigcap = writecap or readcap
         if not bigcap:
             # maybe the writecap was hidden because we're in a readonly

commit 71f4fd2e06ea4c6bc0cbe522f058313ae3820ab7
Author: Brian Warner <warner@lothar.com>
Date:   Mon Apr 26 01:59:59 2010 -0700

    make it work! first integration steps are in a separate patch
    
    ripped out some complexity surrounding guessing numsegs
---
 src/allmydata/immutable/download2.py |  242 ++++++++++++++++++++--------------
 1 files changed, 144 insertions(+), 98 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index f7fe205..95f88c4 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -13,8 +13,8 @@ from allmydata.hashtree import IncompleteHashTree, BadHashError, \
 from allmydata.util import base32, log, hashutil, mathutil, idlib
 from allmydata.util.spans import Spans, DataSpans, overlap
 from allmydata.util.dictutil import DictOfSets
-from allmydata.util.observer import OneShotObserverList
-from allmydata import uri, codec
+from allmydata.codec import CRSDecoder
+from allmydata import uri
 from pycryptopp.cipher.aes import AES
 from download2_util import Observer2, incidentally
 from layout import make_write_bucket_proxy
@@ -202,6 +202,11 @@ class Share:
                 # can't check any hashes without the UEB
                 return False
 
+        # knowing the UEB means knowing num_segments. Despite the redundancy,
+        # this is the best place to set this. CommonShare.set_numsegs will
+        # ignore duplicate calls.
+        self._commonshare.set_numsegs(self._node.num_segments)
+
         segnum, observers = self._active_segnum_and_observers()
         if segnum >= self._node.num_segments:
             for o in observers:
@@ -209,7 +214,7 @@ class Share:
             self._requested_blocks.pop(0)
             return True
 
-        if self._node.share_hash_tree.needed_hashes(self.shnum):
+        if self._node.share_hash_tree.needed_hashes(self._shnum):
             if not self._satisfy_share_hash_tree():
                 # can't check block_hash_tree without a root
                 return False
@@ -218,7 +223,7 @@ class Share:
             return False # we don't want any particular segment right now
 
         # block_hash_tree
-        needed_hashes = self._commonshare.block_hash_tree.needed_hashes(segnum)
+        needed_hashes = self._commonshare.get_needed_block_hashes(segnum)
         if needed_hashes:
             if not self._satisfy_block_hash_tree(needed_hashes):
                 # can't check block without block_hash_tree
@@ -235,26 +240,27 @@ class Share:
         if version == 1:
             table_start = 0x0c
             self._fieldsize = 0x4
-            self._fieldstruct = ">L"
+            self._fieldstruct = "L"
         else:
             table_start = 0x14
             self._fieldsize = 0x8
-            self._fieldstruct = ">Q"
+            self._fieldstruct = "Q"
         offset_table_size = 6 * self._fieldsize
         table_s = self._received_data.pop(table_start, offset_table_size)
         if table_s is None:
             return False
-        fields = struct.unpack(6*self._fieldstruct, table_s)
+        fields = struct.unpack(">"+6*self._fieldstruct, table_s)
         offsets = {}
-        for i,field in enumerate('data',
-                                 'plaintext_hash_tree', # UNUSED
-                                 'crypttext_hash_tree',
-                                 'block_hashes',
-                                 'share_hashes',
-                                 'uri_extension',
-                                 ):
+        for i,field in enumerate(['data',
+                                  'plaintext_hash_tree', # UNUSED
+                                  'crypttext_hash_tree',
+                                  'block_hashes',
+                                  'share_hashes',
+                                  'uri_extension',
+                                  ] ):
             offsets[field] = fields[i]
         self.actual_offsets = offsets
+        log.msg("actual offsets: data=%d, plaintext_hash_tree=%d, crypttext_hash_tree=%d, block_hashes=%d, share_hashes=%d, uri_extension=%d" % tuple(fields))
         self._received_data.remove(0, 4) # don't need this anymore
         return True
 
@@ -265,7 +271,7 @@ class Share:
         UEB_length_s = rdata.get(o["uri_extension"], fsize)
         if not UEB_length_s:
             return False
-        UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
+        (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
         UEB_s = rdata.pop(o["uri_extension"]+fsize, UEB_length)
         if not UEB_s:
             return False
@@ -297,7 +303,7 @@ class Share:
             return False
         share_hashes = {}
         for i in range(0, hashlen, 2+HASH_SIZE):
-            hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
+            (hashnum,) = struct.unpack(">H", hashdata[i:i+2])
             hashvalue = hashdata[i+2:i+2+HASH_SIZE]
             share_hashes[hashnum] = hashvalue
         try:
@@ -448,7 +454,7 @@ class Share:
             return
         UEB_length_s = rdata.get(o["uri_extension"], self._fieldsize)
         if UEB_length_s:
-            UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
+            (UEB_length,) = struct.unpack(">"+self._fieldstruct, UEB_length_s)
             # we know the length, so make sure we grab everything
             self._wanted.add(o["uri_extension"]+self._fieldsize, UEB_length)
 
@@ -466,9 +472,13 @@ class Share:
         for (start, length) in needed:
             # TODO: quantize to reasonably-large blocks
             self._requested.add(start, length)
+            lp = log.msg(format="_send_request(%(peerid)s)"
+                         " [%(start)d:+%(length)d]",
+                         peerid=self._peerid_s, start=start, length=length,
+                         level=log.NOISY, umid="sgVAyA")
             d = self._send_request(start, length)
-            d.addCallback(self._got_data, start, length)
-            d.addErrback(self._got_error, start, length)
+            d.addCallback(self._got_data, start, length, lp)
+            d.addErrback(self._got_error, start, length, lp)
             d.addCallback(incidentally, eventually, self.loop)
             d.addErrback(lambda f:
                          log.err(format="unhandled error during send_request",
@@ -478,20 +488,22 @@ class Share:
     def _send_request(self, start, length):
         return self._rref.callRemote("read", start, length)
 
-    def _got_data(self, data, start, length):
+    def _got_data(self, data, start, length, lp):
+        log.msg(format="_got_data [%(start)d:+%(length)d] -> %(datalen)d",
+                start=start, length=length, datalen=len(data),
+                level=log.NOISY, parent=lp, umid="sgVAyA")
         span = (start, length)
         assert span in self._requested
         self._requested.remove(start, length)
         self._received.add(start, length)
         self._received_data.add(start, data)
 
-    def _got_error(self, f, start, length):
+    def _got_error(self, f, start, length, lp):
         log.msg(format="error requesting %(start)d+%(length)d"
                 " from %(server)s for si %(si)s",
                 start=start, length=length,
                 server=self._peerid_s, si=self._si_prefix,
-                failure=f, parent=self._lp,
-                level=log.UNUSUAL, umid="qZu0wg")
+                failure=f, parent=lp, level=log.UNUSUAL, umid="qZu0wg")
         # retire our observers, assuming we won't be able to make any
         # further progress
         self._fail(f)
@@ -506,26 +518,27 @@ class CommonShare:
     """I hold data that is common across all instances of a single share,
     like sh2 on both servers A and B. This is just the block hash tree.
     """
-    def __init__(self, numsegs, si_prefix, shnum, numsegs_is_a_guess):
+    def __init__(self, guessed_numsegs, si_prefix, shnum):
         self.si_prefix = si_prefix
         self.shnum = shnum
         # in the beginning, before we have the real UEB, we can only guess at
         # the number of segments. But we want to ask for block hashes early.
         # So if we're asked for which block hashes are needed before we know
         # numsegs for sure, we return a guess.
-        assert numsegs is not None
-        self._block_hash_tree = IncompleteHashTree(numsegs)
-        self._numsegs_is_a_guess = numsegs_is_a_guess
+        self._block_hash_tree = IncompleteHashTree(guessed_numsegs)
+        self._know_numsegs = False
 
-    def got_numsegs(self, numsegs):
+    def set_numsegs(self, numsegs):
+        if self._know_numsegs:
+            return
         self._block_hash_tree = IncompleteHashTree(numsegs)
-        self._numsegs_is_a_guess = False
+        self._know_numsegs = True
 
     def get_needed_block_hashes(self, segnum):
         return self._block_hash_tree.needed_hashes(segnum)
 
     def process_block_hashes(self, block_hashes, serverid_s):
-        assert not self._numsegs_is_a_guess
+        assert self._know_numsegs
         try:
             self._block_hash_tree.add_hashes(block_hashes)
             return True
@@ -539,7 +552,7 @@ class CommonShare:
         return False
 
     def check_block(self, segnum, block, serverid_s):
-        assert not self._numsegs_is_a_guess
+        assert self._know_numsegs
         h = hashutil.block_hash(block)
         try:
             self._block_hash_tree.set_hashes(leaves={segnum: h})
@@ -600,7 +613,7 @@ class SegmentFetcher:
     def stop(self):
         self._cancel_all_requests()
         self._running = False
-        del self._shares # let GC work # ???
+        del self._shares # let GC work # ??? XXX
 
 
     # called by our parent _Node
@@ -612,12 +625,12 @@ class SegmentFetcher:
         for s in shares:
             self._shares[s] = AVAILABLE
             self._shnums.add(s._shnum, s)
-        eventually(self._loop)
+        eventually(self.loop)
 
     def no_more_shares(self):
         # ShareFinder tells us it's reached the end of its list
         self._no_more_shares = True
-        eventually(self._loop)
+        eventually(self.loop)
 
     # internal methods
 
@@ -630,7 +643,15 @@ class SegmentFetcher:
                 shnums.append(shnum)
         return len(shnums)
 
-    def _loop(self):
+    def loop(self):
+        try:
+            # if any exception occurs here, kill the download
+            self._do_loop()
+        except BaseException:
+            self._node.fetch_failed(self, Failure())
+            raise
+
+    def _do_loop(self):
         k = self._k
         if not self._running:
             return
@@ -656,7 +677,6 @@ class SegmentFetcher:
             self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) < k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
-            print "BOO"
             self.stop()
             format = ("ran out of shares: %(complete)d complete,"
                       " %(pending)d pending, %(overdue)d overdue,"
@@ -748,7 +768,7 @@ class SegmentFetcher:
         elif state is BADSEGNUM:
             self._shares[share] = BADSEGNUM # ???
             self._bad_segnum = True
-        eventually(self._loop)
+        eventually(self.loop)
 
 
 class RequestToken:
@@ -776,14 +796,6 @@ class ShareFinder:
         self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
                            si=self._si_prefix, level=log.NOISY, umid="2xjj2A")
 
-        self._num_segments = None
-        d = self.node.get_num_segments()
-        d.addCallback(self._got_numsegs)
-        def _err_numsegs(f):
-            log.err(format="Unable to get number of segments", failure=f,
-                    parent=self._lp, level=log.UNUSUAL, umid="dh38Xw")
-        d.addErrback(_err_numsegs)
-
     def log(self, *args, **kwargs):
         if "parent" not in kwargs:
             kwargs["parent"] = self._lp
@@ -792,11 +804,6 @@ class ShareFinder:
     def stop(self):
         self.running = False
 
-    def _got_numsegs(self, numsegs):
-        for cs in self._commonshares.values():
-            cs.got_numsegs(numsegs)
-        self._num_segments = numsegs
-
     # called by our parent CiphertextDownloader
     def hungry(self):
         log.msg(format="ShareFinder[si=%(si)s] hungry",
@@ -806,15 +813,16 @@ class ShareFinder:
 
     # internal methods
     def loop(self):
+        undelivered_s = ",".join(["sh%d@%s" %
+                                  (s._shnum, idlib.shortnodeid_b2a(s._peerid))
+                                  for s in self.undelivered_shares])
+        pending_s = ",".join([idlib.shortnodeid_b2a(rt.peerid)
+                              for rt in self.pending_requests]) # sort?
         log.msg(format="ShareFinder[si=%(si)s] loop: running=%(running)s"
                 " hungry=%(hungry)s, undelivered=%(undelivered)s,"
                 " pending=%(pending)s",
-                si=self._si_prefix, running=self._running, hungry=self._hungry,
-                undelivered=",".join(["sh%d@%s" % (s._shnum,
-                                                   idlib.shortnodeid_b2a(s._peerid))
-                                      for s in self.undelivered_shares]),
-                pending=",".join([idlib.shortnodeid_b2a(rt.peerid)
-                                  for rt in self.pending_requests]), # sort?
+                si=self._si_prefix, running=self.running, hungry=self._hungry,
+                undelivered=undelivered_s, pending=pending_s,
                 level=log.NOISY, umid="kRtS4Q")
         if not self.running:
             return
@@ -846,6 +854,8 @@ class ShareFinder:
             # them will make progress
             return
 
+        log.msg(format="ShareFinder.loop: no_more_shares",
+                level=log.UNUSUAL, umid="XjQlzg")
         # we've run out of servers (so we can't send any more requests), and
         # we have nothing in flight. No further progress can be made. They
         # are destined to remain hungry.
@@ -878,14 +888,29 @@ class ShareFinder:
             self.log(format="no shares from [%(peerid)s]",
                      peerid=idlib.shortnodeid_b2a(peerid),
                      level=log.NOISY, parent=lp, umid="U7d4JA")
-        numsegs = self._num_segments or A GUESS
-        numsegs_is_a_guess = MAYBE
+        if self.node.num_segments is None:
+            best_numsegs = self.node.guessed_num_segments
+        else:
+            best_numsegs = self.node.num_segments
         for shnum, bucket in buckets.iteritems():
-            if shnum not in self._commonshares:
-                self._commonshares[shnum] = CommonShare(numsegs,
-                                                        self._si_prefix, shnum,
-                                                        numsegs_is_a_guess)
-            cs = self._commonshares[shnum]
+            if shnum in self._commonshares:
+                cs = self._commonshares[shnum]
+            else:
+                cs = CommonShare(best_numsegs, self._si_prefix, shnum)
+                # Share._get_satisfaction is responsible for updating
+                # CommonShare.set_numsegs after we know the UEB. Alternatives:
+                #  1: d = self.node.get_num_segments()
+                #     d.addCallback(cs.got_numsegs)
+                #   the problem is that the OneShotObserverList I was using
+                #   inserts an eventual-send between _get_satisfaction's
+                #   _satisfy_UEB and _satisfy_block_hash_tree, and the
+                #   CommonShare didn't get the num_segs message before
+                #   being asked to set block hash values. To resolve this
+                #   would require an immediate ObserverList instead of
+                #   an eventual-send -based one
+                #  2: break _get_satisfaction into Deferred-attached pieces.
+                #     Yuck.
+                self._commonshares[shnum] = cs
             s = Share(bucket, server_version, self.verifycap, cs, self.node,
                       peerid, shnum)
             self.undelivered_shares.append(s)
@@ -1063,6 +1088,8 @@ class _Node:
                                    # same place as upload.BaseUploadable
         s = mathutil.next_multiple(min(verifycap.size, max_segment_size), k)
         self.guessed_segment_size = s
+        r = self._calculate_sizes(self.guessed_segment_size)
+        self.guessed_num_segments = r["num_segments"]
 
         # filled in when we parse a valid UEB
         self.have_UEB = False
@@ -1076,8 +1103,7 @@ class _Node:
         self.ciphertext_hash = None # flat hash, optional
 
         # things to track callers that want data
-        self._segsize_observers = OneShotObserverList()
-        self._numsegs_observers = OneShotObserverList()
+
         # _segment_requests can have duplicates
         self._segment_requests = [] # (segnum, d, cancel_handle)
         self._active_segment = None # a SegmentFetcher, with .segnum
@@ -1149,15 +1175,6 @@ class _Node:
     # things called by the Segmentation object used to transform
     # arbitrary-sized read() calls into quantized segment fetches
 
-    def get_segment_size(self):
-        """I return a Deferred that fires with the segment_size used by this
-        file."""
-        return self._segsize_observers.when_fired()
-    def get_num_segments(self):
-        """I return a Deferred that fires with the number of segments used by
-        this file."""
-        return self._numsegs_observers.when_fired()
-
     def _start_new_segment(self):
         if self._active_segment is None and self._segment_requests:
             segnum = self._segment_requests[0][0]
@@ -1180,17 +1197,16 @@ class _Node:
     # things called by our Share instances
 
     def validate_and_store_UEB(self, UEB_s):
+        log.msg("validate_and_store_UEB",
+                level=log.OPERATIONAL, umid="7sTrPw")
         h = hashutil.uri_extension_hash(UEB_s)
         if h != self._verifycap.uri_extension_hash:
             raise hashutil.BadHashError
         UEB_dict = uri.unpack_extension(UEB_s)
-        self._parse_and_store_UEB(self, UEB_dict) # sets self._stuff
+        self._parse_and_store_UEB(UEB_dict) # sets self._stuff
         # TODO: a malformed (but authentic) UEB could throw an assertion in
         # _parse_and_store_UEB, and we should abandon the download.
         self.have_UEB = True
-        self._segsize_observers.fire(self.segment_size)
-        self._numsegs_observers.fire(self.num_segments)
-
 
     def _parse_and_store_UEB(self, d):
         # Note: the UEB contains needed_shares and total_shares. These are
@@ -1204,6 +1220,10 @@ class _Node:
 
         # therefore, we ignore d['total_shares'] and d['needed_shares'].
 
+        log.msg(format="UEB=%(ueb)s, vcap=%(vcap)s",
+                ueb=repr(d), vcap=self._verifycap.to_string(),
+                level=log.NOISY, umid="cVqZnA")
+
         k, N = self._verifycap.needed_shares, self._verifycap.total_shares
 
         self.segment_size = d['segment_size']
@@ -1214,12 +1234,14 @@ class _Node:
         self.num_segments = r["num_segments"]
         self.block_size = r["block_size"]
         self.tail_block_size = r["tail_block_size"]
+        log.msg("actual sizes: %s" % (r,),
+                level=log.NOISY, umid="PY6P5Q")
 
         # zfec.Decode() instantiation is fast, but still, let's use the same
         # codec instance for all but the last segment. 3-of-10 takes 15us on
         # my laptop, 25-of-100 is 900us, 3-of-255 is 97us, 25-of-255 is
         # 2.5ms, worst-case 254-of-255 is 9.3ms
-        self._codec = codec.CRSDecoder()
+        self._codec = CRSDecoder()
         self._codec.set_params(self.segment_size, k, N)
 
 
@@ -1297,52 +1319,70 @@ class _Node:
             eventually(self._deliver, d, c, f)
 
     def process_blocks(self, segnum, blocks):
+        d = defer.maybeDeferred(self._decode_blocks, segnum, blocks)
+        d.addCallback(self._check_ciphertext_hash, segnum)
+        def _deliver(result):
+            for (d,c) in self._extract_requests(segnum):
+                eventually(self._deliver, d, c, result)
+            self._active_segment = None
+            self._start_new_segment()
+        d.addBoth(_deliver)
+        d.addErrback(lambda f:
+                     log.err(format="unhandled error during process_blocks",
+                             failure=f, level=log.WEIRD, umid="MkEsCg"))
+
+    def _decode_blocks(self, segnum, blocks):
         tail = (segnum == self.num_segments-1)
         codec = self._codec
+        block_size = self.block_size
+        decoded_size = self.segment_size
         if tail:
             # account for the padding in the last segment
-            codec = codec.CRSDecoder()
+            codec = CRSDecoder()
             k, N = self._verifycap.needed_shares, self._verifycap.total_shares
             codec.set_params(self.tail_segment_padded, k, N)
+            block_size = self.tail_block_size
+            decoded_size = self.tail_segment_padded
 
         shares = []
         shareids = []
         for (shareid, share) in blocks.iteritems():
+            assert len(share) == block_size
             shareids.append(shareid)
             shares.append(share)
         del blocks
-        segment = codec.decode(shares, shareids)
+
+        d = codec.decode(shares, shareids)   # segment
         del shares
-        if tail:
-            segment = segment[self.tail_segment_size:]
-        self._process_segment(segnum, segment)
+        def _process(buffers):
+            segment = "".join(buffers)
+            assert len(segment) == decoded_size
+            del buffers
+            if tail:
+                segment = segment[:self.tail_segment_size]
+            return segment
+        d.addCallback(_process)
+        return d
 
-    def _process_segment(self, segnum, segment):
+    def _check_ciphertext_hash(self, segment, segnum):
         assert self._active_segment.segnum == segnum
         assert self.segment_size is not None
         offset = segnum * self.segment_size
 
-        h = hashutil.crypttext_hash(segment)
+        h = hashutil.crypttext_segment_hash(segment)
         try:
             self.ciphertext_hash_tree.set_hashes(leaves={segnum: h})
-            result = (offset, segment)
+            return (offset, segment)
         except (BadHashError, NotEnoughHashesError):
             format = ("hash failure in ciphertext_hash_tree:"
                       " segnum=%(segnum)d, SI=%(si)s")
-            self.log(format=format, segnum=segnum, si=self._si_prefix,
-                     failure=Failure(),
-                     level=log.WEIRD, umid="MTwNnw")
+            log.msg(format=format, segnum=segnum, si=self._si_prefix,
+                    failure=Failure(), level=log.WEIRD, umid="MTwNnw")
             # this is especially weird, because we made it past the share
             # hash tree. It implies that we're using the wrong encoding, or
             # that the uploader deliberately constructed a bad UEB.
             msg = format % {"segnum": segnum, "si": self._si_prefix}
-            e = BadCiphertextHashError(msg)
-            result = Failure(e)
-
-        for (d,c) in self._extract_requests(segnum):
-            eventually(self._deliver, d, c, result)
-        self._active_segment = None
-        self._start_new_segment()
+            raise BadCiphertextHashError(msg)
 
     def _deliver(self, d, c, result):
         # this method exists to handle cancel() that occurs between
@@ -1446,7 +1486,9 @@ class ImmutableFileNode:
 
     def read(self, consumer, offset=0, size=None):
         decryptor = DecryptingConsumer(consumer, self._readkey, offset)
-        return self._cnode.read(decryptor, offset, size)
+        d = self._cnode.read(decryptor, offset, size)
+        d.addCallback(lambda dc: consumer)
+        return d
 
 
 # TODO: if server1 has all shares, and server2-10 have one each, make the
@@ -1497,3 +1539,7 @@ class ImmutableFileNode:
 
 # TODO: if offset table is corrupt, attacker could cause us to fetch whole
 # (large) share
+
+# log budget: when downloading at 1MBps (i.e. 8 segments-per-second), 10
+# log.OPERATIONAL per second, 100 log.NOISY per second. With k=3, that's 3
+# log.NOISY per block fetch.

commit f33bc9d6526ddbb00e93cb329f9b7996f27d991d
Author: Brian Warner <warner@lothar.com>
Date:   Mon Apr 26 00:15:24 2010 -0700

    several test-driven fixes. Still broken.
---
 src/allmydata/immutable/download2.py      |  129 ++++++++++++++++++-----------
 src/allmydata/immutable/download2_util.py |    3 +-
 2 files changed, 80 insertions(+), 52 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 06b3dee..f7fe205 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -8,10 +8,10 @@ from twisted.internet.interfaces import IPushProducer, IConsumer
 
 from foolscap.api import eventually
 from allmydata.interfaces import HASH_SIZE, NotEnoughSharesError
+from allmydata.hashtree import IncompleteHashTree, BadHashError, \
+     NotEnoughHashesError
 from allmydata.util import base32, log, hashutil, mathutil, idlib
 from allmydata.util.spans import Spans, DataSpans, overlap
-from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
-     NotEnoughHashesError
 from allmydata.util.dictutil import DictOfSets
 from allmydata.util.observer import OneShotObserverList
 from allmydata import uri, codec
@@ -41,9 +41,10 @@ class Share:
     # this is a specific implementation of IShare for tahoe's native storage
     # servers. A different backend would use a different class.
 
-    def __init__(self, rref, verifycap, commonshare, node, peerid,
-                 storage_index, shnum):
+    def __init__(self, rref, server_version, verifycap, commonshare, node,
+                 peerid, shnum):
         self._rref = rref
+        self._server_version = server_version
         self._node = node # holds share_hash_tree and UEB
         self._guess_offsets(verifycap, node.guessed_segment_size)
         self.actual_offsets = None
@@ -52,8 +53,8 @@ class Share:
         self._commonshare = commonshare # holds block_hash_tree
         self._peerid = peerid
         self._peerid_s = base32.b2a(peerid)[:5]
-        self._storage_index = storage_index
-        self._si_prefix = base32.b2a(storage_index)[:8]
+        self._storage_index = verifycap.storage_index
+        self._si_prefix = base32.b2a(verifycap.storage_index)[:8]
         self._shnum = shnum
 
         self._lp = log.msg(format="Share(%(si)s) on server=%(server)s starting",
@@ -66,7 +67,7 @@ class Share:
         self._received = Spans() # we've received a response for this
         self._received_data = DataSpans() # the response contents, still unused
         self._requested_blocks = [] # (segnum, set(observer2..))
-        ver = rref.version["http://allmydata.org/tahoe/protocols/storage/v1"]
+        ver = server_version["http://allmydata.org/tahoe/protocols/storage/v1"]
         self._overrun_ok = ver["tolerates-immutable-read-overrun"]
         # If _overrun_ok and we guess the offsets correctly, we can get
         # everything in one RTT. If _overrun_ok and we guess wrong, we might
@@ -130,7 +131,7 @@ class Share:
                 observers.add(o)
                 break
         else:
-            self._requested_blocks.append(segnum, set([o]))
+            self._requested_blocks.append( (segnum, set([o])) )
         eventually(self.loop)
         return o
 
@@ -157,8 +158,14 @@ class Share:
         return None, []
 
     def loop(self):
-        # TODO: if any exceptions occur here, kill the download
+        try:
+            # if any exceptions occur here, kill the download
+            self._do_loop()
+        except BaseException:
+            self._fail(Failure())
+            raise
 
+    def _do_loop(self):
         # we are (eventually) called after all state transitions:
         #  new segments added to self._requested_blocks
         #  new data received from servers (responses to our read() calls)
@@ -375,7 +382,7 @@ class Share:
         if not self._node.have_UEB:
             self._desire_UEB(o)
 
-        if self._node.share_hash_tree.needed_hashes(self.shnum):
+        if self._node.share_hash_tree.needed_hashes(self._shnum):
             hashlen = o["uri_extension"] - o["share_hashes"]
             self._wanted.add(o["share_hashes"], hashlen)
 
@@ -383,7 +390,7 @@ class Share:
             return # only need block hashes or blocks for active segments
 
         # block hash chain
-        for hashnum in commonshare.block_hash_tree.needed_hashes(segnum):
+        for hashnum in commonshare.get_needed_block_hashes(segnum):
             self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # data
@@ -462,6 +469,7 @@ class Share:
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length)
             d.addErrback(self._got_error, start, length)
+            d.addCallback(incidentally, eventually, self.loop)
             d.addErrback(lambda f:
                          log.err(format="unhandled error during send_request",
                                  failure=f, parent=self._lp,
@@ -476,7 +484,6 @@ class Share:
         self._requested.remove(start, length)
         self._received.add(start, length)
         self._received_data.add(start, data)
-        eventually(self.loop)
 
     def _got_error(self, f, start, length):
         log.msg(format="error requesting %(start)d+%(length)d"
@@ -490,24 +497,35 @@ class Share:
         self._fail(f)
 
     def _fail(self, f):
-        for (segnum, o) in self._requested_blocks:
-            o.notify(state=DEAD, f=f)
+        for (segnum, observers) in self._requested_blocks:
+            for o in observers:
+                o.notify(state=DEAD, f=f)
 
 
 class CommonShare:
     """I hold data that is common across all instances of a single share,
     like sh2 on both servers A and B. This is just the block hash tree.
     """
-    def __init__(self, numsegs, si_s, shnum):
-        self.si_s = si_s
+    def __init__(self, numsegs, si_prefix, shnum, numsegs_is_a_guess):
+        self.si_prefix = si_prefix
         self.shnum = shnum
-        if numsegs is not None:
-            self._block_hash_tree = IncompleteHashTree(numsegs)
+        # in the beginning, before we have the real UEB, we can only guess at
+        # the number of segments. But we want to ask for block hashes early.
+        # So if we're asked for which block hashes are needed before we know
+        # numsegs for sure, we return a guess.
+        assert numsegs is not None
+        self._block_hash_tree = IncompleteHashTree(numsegs)
+        self._numsegs_is_a_guess = numsegs_is_a_guess
 
     def got_numsegs(self, numsegs):
         self._block_hash_tree = IncompleteHashTree(numsegs)
+        self._numsegs_is_a_guess = False
+
+    def get_needed_block_hashes(self, segnum):
+        return self._block_hash_tree.needed_hashes(segnum)
 
     def process_block_hashes(self, block_hashes, serverid_s):
+        assert not self._numsegs_is_a_guess
         try:
             self._block_hash_tree.add_hashes(block_hashes)
             return True
@@ -516,11 +534,12 @@ class CommonShare:
             self.log("hash failure in block_hashes=(%(hashnums)s),"
                      " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                      hashnums=hashnums, shnum=self.shnum,
-                     si=self.si_s, server=serverid_s, failure=Failure(),
+                     si=self.si_prefix, server=serverid_s, failure=Failure(),
                      level=log.WEIRD, umid="yNyFdA")
         return False
 
     def check_block(self, segnum, block, serverid_s):
+        assert not self._numsegs_is_a_guess
         h = hashutil.block_hash(block)
         try:
             self._block_hash_tree.set_hashes(leaves={segnum: h})
@@ -528,7 +547,7 @@ class CommonShare:
         except (BadHashError, NotEnoughHashesError):
             self.log("hash failure in block %(segnum)d,"
                      " shnum=%(shnum)d SI=%(si)s server=%(server)s",
-                     segnum=segnum, shnum=self.shnum, si=self.si_s,
+                     segnum=segnum, shnum=self.shnum, si=self.si_prefix,
                      server=serverid_s, failure=Failure(),
                      level=log.WEIRD, umid="mZjkqA")
         return False
@@ -584,7 +603,7 @@ class SegmentFetcher:
         del self._shares # let GC work # ???
 
 
-    # called by our parent CiphertextFileNode
+    # called by our parent _Node
 
     def add_shares(self, shares):
         # called when ShareFinder locates a new share, and when a non-initial
@@ -592,12 +611,13 @@ class SegmentFetcher:
         # previous segment
         for s in shares:
             self._shares[s] = AVAILABLE
-            self._shnums[s.shnum].add(s)
+            self._shnums.add(s._shnum, s)
         eventually(self._loop)
 
     def no_more_shares(self):
         # ShareFinder tells us it's reached the end of its list
         self._no_more_shares = True
+        eventually(self._loop)
 
     # internal methods
 
@@ -605,7 +625,7 @@ class SegmentFetcher:
         """shnums for which at least one state is in the following list"""
         shnums = []
         for shnum,shares in self._shnums.iteritems():
-            matches = [s for s in shares if s.state in states]
+            matches = [s for s in shares if self._shares[s] in states]
             if matches:
                 shnums.append(shnum)
         return len(shnums)
@@ -636,10 +656,11 @@ class SegmentFetcher:
             self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) < k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
+            print "BOO"
             self.stop()
             format = ("ran out of shares: %(complete)d complete,"
                       " %(pending)d pending, %(overdue)d overdue,"
-                      " %(unused)d unused, need %(k)k."
+                      " %(unused)d unused, need %(k)d."
                       " Last failure: %(last_failure)s")
             args = {"complete": self._count_shnums(COMPLETE),
                     "pending": self._count_shnums(PENDING),
@@ -648,7 +669,7 @@ class SegmentFetcher:
                     "k": k,
                     "last_failure": self._last_failure,
                     }
-            self.log(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
+            log.msg(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
             e = NotEnoughSharesError(format % args)
             f = Failure(e)
             self._node.fetch_failed(self, f)
@@ -735,13 +756,14 @@ class RequestToken:
         self.peerid = peerid
 
 class ShareFinder:
-    def __init__(self, storage_broker, storage_index, node,
+    def __init__(self, storage_broker, verifycap, node,
                  max_outstanding_requests=10):
         self.running = True
-        s = storage_broker.get_servers_for_index(storage_index)
+        self.verifycap = verifycap
+        s = storage_broker.get_servers_for_index(verifycap.storage_index)
         self._servers = iter(s)
         self.share_consumer = self.node = node
-        self.max_outstanding = max_outstanding_requests
+        self.max_outstanding_requests = max_outstanding_requests
 
         self._hungry = False
 
@@ -749,14 +771,13 @@ class ShareFinder:
         self.undelivered_shares = []
         self.pending_requests = set()
 
-        self._storage_index = storage_index
-        self._si_s = base32.b2a(storage_index)
-        self._si_prefix = base32.b2a_l(storage_index[:8], 60)
+        self._storage_index = verifycap.storage_index
+        self._si_prefix = base32.b2a_l(self._storage_index[:8], 60)
         self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
                            si=self._si_prefix, level=log.NOISY, umid="2xjj2A")
 
         self._num_segments = None
-        d = self.share_consumer.get_num_segments()
+        d = self.node.get_num_segments()
         d.addCallback(self._got_numsegs)
         def _err_numsegs(f):
             log.err(format="Unable to get number of segments", failure=f,
@@ -841,34 +862,38 @@ class ShareFinder:
         d = rref.callRemote("get_buckets", self._storage_index)
         d.addBoth(incidentally, self.pending_requests.discard, req)
         d.addCallbacks(self._got_response, self._got_error,
-                       callbackArgs=(peerid, req, lp))
+                       callbackArgs=(rref.version, peerid, req, lp),
+                       errbackArgs=(peerid, req, lp))
         d.addErrback(log.err, format="error in send_request",
                      level=log.WEIRD, parent=lp, umid="rpdV0w")
         d.addCallback(incidentally, eventually, self.loop)
 
-    def _got_response(self, buckets, peerid, req, lp):
+    def _got_response(self, buckets, server_version, peerid, req, lp):
         if buckets:
             shnums_s = ",".join([str(shnum) for shnum in buckets])
-            self.log(format="got shnums [%s] from [%(peerid)s]" % shnums_s,
-                     peerid=idlib.shortnodeid_b2a(peerid),
+            self.log(format="got shnums [%(shnums)s] from [%(peerid)s]",
+                     shnums=shnums_s, peerid=idlib.shortnodeid_b2a(peerid),
                      level=log.NOISY, parent=lp, umid="0fcEZw")
         else:
             self.log(format="no shares from [%(peerid)s]",
                      peerid=idlib.shortnodeid_b2a(peerid),
                      level=log.NOISY, parent=lp, umid="U7d4JA")
+        numsegs = self._num_segments or A GUESS
+        numsegs_is_a_guess = MAYBE
         for shnum, bucket in buckets.iteritems():
             if shnum not in self._commonshares:
-                self._commonshares[shnum] = CommonShare(self._num_segments,
-                                                        self._si_s, shnum)
+                self._commonshares[shnum] = CommonShare(numsegs,
+                                                        self._si_prefix, shnum,
+                                                        numsegs_is_a_guess)
             cs = self._commonshares[shnum]
-            s = Share(bucket, self.verifycap, cs, self.node,
-                      peerid, self._storage_index, shnum)
+            s = Share(bucket, server_version, self.verifycap, cs, self.node,
+                      peerid, shnum)
             self.undelivered_shares.append(s)
 
-    def _got_error(self, f, peerid, req):
+    def _got_error(self, f, peerid, req, lp):
         self.log(format="got error from [%(peerid)s]",
                  peerid=idlib.shortnodeid_b2a(peerid), failure=f,
-                 level=log.UNUSUAL, umid="zUKdCw")
+                 level=log.UNUSUAL, parent=lp, umid="zUKdCw")
 
 
 
@@ -1057,8 +1082,7 @@ class _Node:
         self._segment_requests = [] # (segnum, d, cancel_handle)
         self._active_segment = None # a SegmentFetcher, with .segnum
 
-        storage_index = verifycap.storage_index
-        self._sharefinder = ShareFinder(storage_broker, storage_index, self)
+        self._sharefinder = ShareFinder(storage_broker, verifycap, self)
         self._shares = set()
 
     def stop(self):
@@ -1079,6 +1103,10 @@ class _Node:
         # for concurrent operations: each gets its own Segmentation manager
         if size is None:
             size = self._verifycap.size - offset
+        log.msg(format="imm Node(%(si)s.read(%(offset)d, %(size)d)",
+                si=base32.b2a(self._verifycap.storage_index)[:8],
+                offset=offset, size=size,
+                level=log.OPERATIONAL, umid="l3j3Ww")
         s = Segmentation(self, offset, size, consumer)
         # this raises an interesting question: what segments to fetch? if
         # offset=0, always fetch the first segment, and then allow
@@ -1109,11 +1137,13 @@ class _Node:
         The Deferred can also errback with other fatal problems, such as
         NotEnoughSharesError, NoSharesError, or BadCiphertextHashError.
         """
+        log.msg(format="imm Node(%(si)s.get_segment(%(segnum)d)",
+                si=base32.b2a(self._verifycap.storage_index)[:8],
+                segnum=segnum, level=log.OPERATIONAL, umid="UKFjDQ")
         d = defer.Deferred()
         c = Cancel(self._cancel_request)
         self._segment_requests.append( (segnum, d, c) )
         self._start_new_segment()
-        eventually(self._loop)
         return (d, c)
 
     # things called by the Segmentation object used to transform
@@ -1261,7 +1291,6 @@ class _Node:
 
     def fetch_failed(self, sf, f):
         assert sf is self._active_segment
-        sf.disownServiceParent()
         self._active_segment = None
         # deliver error upwards
         for (d,c) in self._extract_requests(sf.segnum):
@@ -1391,12 +1420,12 @@ class DecryptingConsumer:
         self._decryptor = AES(readkey, iv=iv)
         self._decryptor.process("\x00"*offset_small)
 
-    def registerProducer(self, producer):
+    def registerProducer(self, producer, streaming):
         # this passes through, so the real consumer can flow-control the real
         # producer. Therefore we don't need to provide any IPushProducer
         # methods. We implement all the IConsumer methods as pass-throughs,
         # and only intercept write() to perform decryption.
-        self._consumer.registerProducer(producer)
+        self._consumer.registerProducer(producer, streaming)
     def unregisterProducer(self):
         self._consumer.unregisterProducer()
     def write(self, ciphertext):
@@ -1405,12 +1434,12 @@ class DecryptingConsumer:
 
 class ImmutableFileNode:
     # I wrap a CiphertextFileNode with a decryption key
-    def __init__(self, filecap, storage_broker, secret_holder, downloader,
+    def __init__(self, filecap, storage_broker, secret_holder, terminator,
                  history):
         assert isinstance(filecap, uri.CHKFileURI)
         verifycap = filecap.get_verify_cap()
         self._cnode = CiphertextFileNode(verifycap, storage_broker,
-                                         secret_holder, downloader, history)
+                                         secret_holder, terminator, history)
         assert isinstance(filecap, uri.CHKFileURI)
         self.u = filecap
         self._readkey = filecap.key
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
index df5d11b..9e20ff4 100755
--- a/src/allmydata/immutable/download2_util.py
+++ b/src/allmydata/immutable/download2_util.py
@@ -15,7 +15,7 @@ class Observer2:
         # we use a weakref to avoid creating a cycle between us and the thing
         # we're observing: they'll be holding a reference to us to compare
         # against the value we pass to their canceler function.
-        self._canceler = weakref(f)
+        self._canceler = weakref.ref(f)
 
     def subscribe(self, observer, **watcher_kwargs):
         self._watcher = (observer, watcher_kwargs)
@@ -56,7 +56,6 @@ def incidentally(res, f, *args, **kwargs):
 
 class Terminator(service.Service):
     def __init__(self):
-        service.Service.__init__(self)
         self._clients = weakref.WeakKeyDictionary()
     def register(self, c):
         self._clients[c] = None

commit 5554eb075e56ba752924da946b541a69c4996b99
Author: Brian Warner <warner@lothar.com>
Date:   Sun Apr 25 21:01:24 2010 -0700

    add DictOfSets.allvalues(), which returns one big set. Not sure it's used.
---
 src/allmydata/util/dictutil.py |    7 +++++++
 1 files changed, 7 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/util/dictutil.py b/src/allmydata/util/dictutil.py
index 8de136f..5809639 100644
--- a/src/allmydata/util/dictutil.py
+++ b/src/allmydata/util/dictutil.py
@@ -57,6 +57,13 @@ class DictOfSets(dict):
         if not self[key]:
             del self[key]
 
+    def allvalues(self):
+        # return a set that merges all value sets
+        r = set()
+        for key in self:
+            r.update(self[key])
+        return r
+
 class UtilDict:
     def __init__(self, initialdata={}):
         self.d = {}

commit 9d35abb05936ede5ebd2570bdcdff89f950d3801
Author: Brian Warner <warner@lothar.com>
Date:   Sun Apr 25 21:01:04 2010 -0700

    fix all XXX omissions, add maybe half the ideal logging, pyflakes-clean
---
 src/allmydata/immutable/download2.py      |  205 ++++++++++++++++++-----------
 src/allmydata/immutable/download2_util.py |   13 +--
 2 files changed, 134 insertions(+), 84 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index a665533..06b3dee 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1,13 +1,34 @@
 
 import binascii
+import struct
+from zope.interface import implements
+from twisted.python.failure import Failure
+from twisted.internet import defer
+from twisted.internet.interfaces import IPushProducer, IConsumer
+
+from foolscap.api import eventually
+from allmydata.interfaces import HASH_SIZE, NotEnoughSharesError
+from allmydata.util import base32, log, hashutil, mathutil, idlib
+from allmydata.util.spans import Spans, DataSpans, overlap
 from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
+from allmydata.util.dictutil import DictOfSets
+from allmydata.util.observer import OneShotObserverList
+from allmydata import uri, codec
+from pycryptopp.cipher.aes import AES
+from download2_util import Observer2, incidentally
+from layout import make_write_bucket_proxy
 
 (AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
  ("AVAILABLE", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
 
+KiB = 1024
 class BadSegmentNumberError(Exception):
     pass
+class BadSegmentError(Exception):
+    pass
+class BadCiphertextHashError(Exception):
+    pass
 
 class Share:
     """I represent a single instance of a single share (e.g. I reference the
@@ -20,7 +41,8 @@ class Share:
     # this is a specific implementation of IShare for tahoe's native storage
     # servers. A different backend would use a different class.
 
-    def __init__(self, rref, verifycap, commonshare, node, peerid, si_s, shnum):
+    def __init__(self, rref, verifycap, commonshare, node, peerid,
+                 storage_index, shnum):
         self._rref = rref
         self._node = node # holds share_hash_tree and UEB
         self._guess_offsets(verifycap, node.guessed_segment_size)
@@ -29,8 +51,9 @@ class Share:
         self._UEB_length = None
         self._commonshare = commonshare # holds block_hash_tree
         self._peerid = peerid
-        self._peerid_s = base32.b2(peerid)[:5]
-        self._si_prefix = si_s[:8]
+        self._peerid_s = base32.b2a(peerid)[:5]
+        self._storage_index = storage_index
+        self._si_prefix = base32.b2a(storage_index)[:8]
         self._shnum = shnum
 
         self._lp = log.msg(format="Share(%(si)s) on server=%(server)s starting",
@@ -57,19 +80,23 @@ class Share:
         size = verifycap.size
         k = verifycap.needed_shares
         N = verifycap.total_shares
-        r = self._node._calculate_sizes(guessed_segment_size, size, k)
-        offsets = {}
-        for i,field in enumerate('data',
-                                 'plaintext_hash_tree', # UNUSED
-                                 'crypttext_hash_tree',
-                                 'block_hashes',
-                                 'share_hashes',
-                                 'uri_extension',
-                                 ):
-            offsets[field] = i # bad guesses are easy :) # XXX stub
-        self.guessed_offsets = offsets
-        self._fieldsize = 4
-        self._fieldstruct = ">L"
+        r = self._node._calculate_sizes(guessed_segment_size)
+        # num_segments, block_size/tail_block_size
+        # guessed_segment_size/tail_segment_size/tail_segment_padded
+        share_size = mathutil.div_ceil(size, k)
+        # share_size is the amount of block data that will be put into each
+        # share, summed over all segments. It does not include hashes, the
+        # UEB, or other overhead.
+
+        # use the upload-side code to get this as accurate as possible
+        ht = IncompleteHashTree(N)
+        num_share_hashes = len(ht.needed_hashes(0, include_leaf=True))
+        wbp = make_write_bucket_proxy(None, share_size, r["block_size"],
+                                      r["num_segments"], num_share_hashes, 0,
+                                      None)
+        self._fieldsize = wbp.fieldsize
+        self._fieldstruct = wbp.fieldstruct
+        self.guessed_offsets = wbp._offsets
 
     # called by our client, the SegmentFetcher
     def get_block(self, segnum):
@@ -241,7 +268,7 @@ class Share:
             self.actual_segment_size = self._node.segment_size
             assert self.actual_segment_size is not None
             return True
-        except hashtree.BadHashError:
+        except BadHashError:
             # TODO: if this UEB was bad, we'll keep trying to validate it
             # over and over again. Only log.err on the first one, or better
             # yet skip all but the first
@@ -271,16 +298,17 @@ class Share:
             # adds to self._node.share_hash_tree
             rdata.remove(o["share_hashes"], hashlen)
             return True
-        except (hashtree.BadHashError, hashtree.NotEnoughHashesError,
-                IndexError):
+        except (BadHashError, NotEnoughHashesError, IndexError):
             f = Failure()
             self._signal_corruption(f, o["share_hashes"], hashlen)
             return False
 
     def _signal_corruption(self, f, start, offset):
         # there was corruption somewhere in the given range
-        print f # XXX
-        pass
+        reason = "corruption in share[%d-%d): %s" % (start, start+offset,
+                                                     str(f.value))
+        self._rref.callRemoteOnly("advise_corrupt_share", "immutable",
+                                  self._storage_index, self._shnum, reason)
 
     def _satisfy_block_hash_tree(self, needed_hashes):
         o = self.actual_offsets
@@ -296,7 +324,8 @@ class Share:
         # we've gotten them all, because the hash tree will throw an
         # exception if we only give it a partial set (which it therefore
         # cannot validate)
-        ok = commonshare.process_block_hashes(block_hashes, serverid_s)
+        commonshare = self._commonshare
+        ok = commonshare.process_block_hashes(block_hashes, self._peerid_s)
         if not ok:
             return False
         for hashnum in needed_hashes:
@@ -311,13 +340,15 @@ class Share:
         if tail:
             blocklen = self._node.tail_block_size
 
+        rdata = self._received_data
         block = rdata.pop(blockstart, blocklen)
         if not block:
             return False
         # this block is being retired, either as COMPLETE or CORRUPT, since
         # no further data reads will help
         assert self._requested_blocks[0][0] == segnum
-        ok = commonshare.check_block(segnum, block)
+        commonshare = self._commonshare
+        ok = commonshare.check_block(segnum, block, self._peerid_s)
         if ok:
             for o in observers:
                 # goes to SegmentFetcher._block_request_activity
@@ -330,8 +361,6 @@ class Share:
 
     def _desire(self):
         segnum, observers = self._active_segnum_and_observers()
-        fsize = self._fieldsize
-        rdata = self._received_data
         commonshare = self._commonshare
 
         if not self.actual_offsets:
@@ -358,7 +387,7 @@ class Share:
             self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # data
-        r = self._node._calculate_sizes(segsize, size, k) # XXX
+        r = self._node._calculate_sizes(segsize)
         tail = (segnum == r["num_segments"])
         datastart = o["data"]
         blockstart = datastart + segnum * r["block_size"]
@@ -394,6 +423,7 @@ class Share:
 
     def _desire_UEB(self, o):
         # UEB data is stored as (length,data).
+        rdata = self._received_data
         if self._overrun_ok:
             # We can pre-fetch 2kb, which should probably cover it. If it
             # turns out to be larger, we'll come back here later with a known
@@ -431,7 +461,7 @@ class Share:
             self._requested.add(start, length)
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length)
-            d.addErrback(self._got_error)
+            d.addErrback(self._got_error, start, length)
             d.addErrback(lambda f:
                          log.err(format="unhandled error during send_request",
                                  failure=f, parent=self._lp,
@@ -448,7 +478,7 @@ class Share:
         self._received_data.add(start, data)
         eventually(self.loop)
 
-    def _got_error(self, f): # XXX
+    def _got_error(self, f, start, length):
         log.msg(format="error requesting %(start)d+%(length)d"
                 " from %(server)s for si %(si)s",
                 start=start, length=length,
@@ -481,23 +511,27 @@ class CommonShare:
         try:
             self._block_hash_tree.add_hashes(block_hashes)
             return True
-        except (hashtree.BadHashError, hashtree.NotEnoughHashesError):
+        except (BadHashError, NotEnoughHashesError):
             hashnums = ",".join(sorted(block_hashes.keys()))
             self.log("hash failure in block_hashes=(%(hashnums)s),"
                      " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                      hashnums=hashnums, shnum=self.shnum,
-                     si=self.si_s, server=serverid_s,
+                     si=self.si_s, server=serverid_s, failure=Failure(),
                      level=log.WEIRD, umid="yNyFdA")
         return False
 
-    def check_block(self, segnum, block):
+    def check_block(self, segnum, block, serverid_s):
         h = hashutil.block_hash(block)
         try:
             self._block_hash_tree.set_hashes(leaves={segnum: h})
-        except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
-            LOG(...)
-            return False
-        return True
+            return True
+        except (BadHashError, NotEnoughHashesError):
+            self.log("hash failure in block %(segnum)d,"
+                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
+                     segnum=segnum, shnum=self.shnum, si=self.si_s,
+                     server=serverid_s, failure=Failure(),
+                     level=log.WEIRD, umid="mZjkqA")
+        return False
 
 # all classes are also Services, and the rule is that you don't initiate more
 # work unless self.running
@@ -615,7 +649,7 @@ class SegmentFetcher:
                     "last_failure": self._last_failure,
                     }
             self.log(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
-            e = NotEnoughShares(format % args)
+            e = NotEnoughSharesError(format % args)
             f = Failure(e)
             self._node.fetch_failed(self, f)
             return
@@ -655,7 +689,7 @@ class SegmentFetcher:
             # here's a candidate. Send a request.
             s = self._find_one(shares, AVAILABLE)
             self._shares[s] = PENDING
-            self._share_observers[s] = o = s.get_block(segnum)
+            self._share_observers[s] = o = s.get_block(self.segnum)
             o.subscribe(self._block_request_activity, share=s, shnum=shnum)
             # TODO: build up a list of candidates, then walk through the
             # list, sending requests to the most desireable servers,
@@ -701,13 +735,12 @@ class RequestToken:
         self.peerid = peerid
 
 class ShareFinder:
-    def __init__(self, storage_broker, storage_index,
-                 share_consumer, max_outstanding_requests=10):
-        # XXX need self.node -> CiphertextFileNode
+    def __init__(self, storage_broker, storage_index, node,
+                 max_outstanding_requests=10):
         self.running = True
         s = storage_broker.get_servers_for_index(storage_index)
         self._servers = iter(s)
-        self.share_consumer = share_consumer
+        self.share_consumer = self.node = node
         self.max_outstanding = max_outstanding_requests
 
         self._hungry = False
@@ -716,13 +749,14 @@ class ShareFinder:
         self.undelivered_shares = []
         self.pending_requests = set()
 
+        self._storage_index = storage_index
         self._si_s = base32.b2a(storage_index)
         self._si_prefix = base32.b2a_l(storage_index[:8], 60)
         self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
                            si=self._si_prefix, level=log.NOISY, umid="2xjj2A")
 
         self._num_segments = None
-        d = share_consumer.get_num_segments()
+        d = self.share_consumer.get_num_segments()
         d.addCallback(self._got_numsegs)
         def _err_numsegs(f):
             log.err(format="Unable to get number of segments", failure=f,
@@ -797,7 +831,6 @@ class ShareFinder:
         self.share_consumer.no_more_shares()
         self.stop()
 
-
     def send_request(self, server):
         peerid, rref = server
         req = RequestToken(peerid)
@@ -829,13 +862,13 @@ class ShareFinder:
                                                         self._si_s, shnum)
             cs = self._commonshares[shnum]
             s = Share(bucket, self.verifycap, cs, self.node,
-                      peerid, self._si_s, shnum)
+                      peerid, self._storage_index, shnum)
             self.undelivered_shares.append(s)
 
     def _got_error(self, f, peerid, req):
         self.log(format="got error from [%(peerid)s]",
                  peerid=idlib.shortnodeid_b2a(peerid), failure=f,
-                 level=log.UNUSUAL, parent=lp, umid="zUKdCw")
+                 level=log.UNUSUAL, umid="zUKdCw")
 
 
 
@@ -861,7 +894,7 @@ class Segmentation:
     def start(self):
         self._alive = True
         self._deferred = defer.Deferred()
-        self._consumer.registerProducer(self) # XXX???
+        self._consumer.registerProducer(self, True)
         self._maybe_fetch_next()
         return self._deferred
 
@@ -903,6 +936,7 @@ class Segmentation:
         return res
 
     def _got_segment(self, (segment_start,segment), had_actual_segment_size):
+        segnum = self._active_segnum
         self._active_segnum = None
         self._cancel_segment_request = None
         # we got file[segment_start:segment_start+len(segment)]
@@ -911,9 +945,18 @@ class Segmentation:
         # the overlap is file[o[0]:o[0]+o[1]]
         if not o or o[0] != self._offset:
             # we didn't get the first byte, so we can't use this segment
-            if have_actual_segment_size:
+            if self._node.segment_size is not None:
                 # and we should have gotten it right. This is big problem.
-                raise SOMETHING
+                log.msg("Segmentation handed wrong data (but we knew better):"
+                        " want [%d-%d), given [%d-%d), for segnum=%d,"
+                        " for si=%s"
+                        % (self._offset, self._offset+self._size,
+                           segment_start, segment_start+len(segment),
+                           segnum, self._node._si_prefix),
+                        level=log.WEIRD, umid="STlIiA")
+                raise BadSegmentError("Despite knowing the segment size,"
+                                      " we were given the wrong data."
+                                      " I cannot cope.")
             # we've wasted some bandwidth, but now we can grab the right one,
             # because we should know the segsize by now.
             assert self._node.segment_size is not None
@@ -925,10 +968,12 @@ class Segmentation:
         self._offset += len(desired_data)
         self._size -= len(desired_data)
         self._consumer.write(desired_data)
+        # the consumer might call our .pauseProducing() inside that write()
+        # call, setting self._hungry=False
         self._maybe_fetch_next()
 
     def _retry_bad_segment(self, f, had_actual_segment_size):
-        f.trap(BadSegmentNumberError): # guessed way wrong, off the end
+        f.trap(BadSegmentNumberError) # guessed way wrong, off the end
         if had_actual_segment_size:
             # but we should have known better, so this is a real error
             return f
@@ -970,8 +1015,9 @@ class _Node:
     # Share._node points to me
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history):
-        assert isinstance(verifycap, CHKFileVerifierURI)
+        assert isinstance(verifycap, uri.CHKFileVerifierURI)
         self._verifycap = verifycap
+        self._si_prefix = base32.b2a_l(verifycap.storage_index[:8], 60)
         self.running = True
         terminator.register(self) # calls self.stop() at stopService()
         # the rules are:
@@ -1059,6 +1105,9 @@ class _Node:
         The Deferred fires with the offset of the first byte of the data
         segment, so that you can call get_segment() before knowing the
         segment size, and still know which data you received.
+
+        The Deferred can also errback with other fatal problems, such as
+        NotEnoughSharesError, NoSharesError, or BadCiphertextHashError.
         """
         d = defer.Deferred()
         c = Cancel(self._cancel_request)
@@ -1091,7 +1140,7 @@ class _Node:
     # called by our child ShareFinder
     def got_shares(self, shares):
         self._shares.update(shares)
-        if self._active_segment
+        if self._active_segment:
             self._active_segment.add_shares(shares)
     def no_more_shares(self):
         self._no_more_shares = True
@@ -1104,7 +1153,7 @@ class _Node:
         h = hashutil.uri_extension_hash(UEB_s)
         if h != self._verifycap.uri_extension_hash:
             raise hashutil.BadHashError
-        UEB_dict = uri.unpack_extension(data)
+        UEB_dict = uri.unpack_extension(UEB_s)
         self._parse_and_store_UEB(self, UEB_dict) # sets self._stuff
         # TODO: a malformed (but authentic) UEB could throw an assertion in
         # _parse_and_store_UEB, and we should abandon the download.
@@ -1126,11 +1175,10 @@ class _Node:
         # therefore, we ignore d['total_shares'] and d['needed_shares'].
 
         k, N = self._verifycap.needed_shares, self._verifycap.total_shares
-        size = self._verifycap.size
 
         self.segment_size = d['segment_size']
 
-        r = self._calculate_sizes(self.segment_size, size, k)
+        r = self._calculate_sizes(self.segment_size)
         self.tail_segment_size = r["tail_segment_size"]
         self.tail_segment_padded = r["tail_segment_padded"]
         self.num_segments = r["num_segments"]
@@ -1170,8 +1218,10 @@ class _Node:
         # redundant fields. The Verifier uses a different code path which
         # does not ignore them.
 
-    def _calculate_sizes(self, segment_size, size, k):
+    def _calculate_sizes(self, segment_size):
         # segments of ciphertext
+        size = self._verifycap.size
+        k = self._verifycap.needed_shares
 
         # this assert matches the one in encode.py:127 inside
         # Encoded._got_all_encoding_parameters, where the UEB is constructed
@@ -1215,13 +1265,7 @@ class _Node:
         self._active_segment = None
         # deliver error upwards
         for (d,c) in self._extract_requests(sf.segnum):
-            eventually(self._deliver_error, d, c, f)
-
-    def _deliver_error(self, d, c, f):
-        # this method exists to handle cancel() that occurs between
-        # _got_segment and _deliver_error
-        if not c.cancelled:
-            d.errback(f)
+            eventually(self._deliver, d, c, f)
 
     def process_blocks(self, segnum, blocks):
         tail = (segnum == self.num_segments-1)
@@ -1245,24 +1289,37 @@ class _Node:
         self._process_segment(segnum, segment)
 
     def _process_segment(self, segnum, segment):
-        h = hashutil.crypttext_hash(segment)
-        try:
-            self.ciphertext_hash_tree.set_hashes(leaves={segnum, h})
-        except SOMETHING:
-            SOMETHING
         assert self._active_segment.segnum == segnum
         assert self.segment_size is not None
         offset = segnum * self.segment_size
+
+        h = hashutil.crypttext_hash(segment)
+        try:
+            self.ciphertext_hash_tree.set_hashes(leaves={segnum: h})
+            result = (offset, segment)
+        except (BadHashError, NotEnoughHashesError):
+            format = ("hash failure in ciphertext_hash_tree:"
+                      " segnum=%(segnum)d, SI=%(si)s")
+            self.log(format=format, segnum=segnum, si=self._si_prefix,
+                     failure=Failure(),
+                     level=log.WEIRD, umid="MTwNnw")
+            # this is especially weird, because we made it past the share
+            # hash tree. It implies that we're using the wrong encoding, or
+            # that the uploader deliberately constructed a bad UEB.
+            msg = format % {"segnum": segnum, "si": self._si_prefix}
+            e = BadCiphertextHashError(msg)
+            result = Failure(e)
+
         for (d,c) in self._extract_requests(segnum):
-            eventually(self._deliver, d, c, offset, segment)
+            eventually(self._deliver, d, c, result)
         self._active_segment = None
         self._start_new_segment()
 
-    def _deliver(self, d, c, offset, segment):
+    def _deliver(self, d, c, result):
         # this method exists to handle cancel() that occurs between
         # _got_segment and _deliver
         if not c.cancelled:
-            d.callback((offset,segment))
+            d.callback(result) # might actually be an errback
 
     def _extract_requests(self, segnum):
         """Remove matching requests and return their (d,c) tuples so that the
@@ -1285,7 +1342,7 @@ class _Node:
 class CiphertextFileNode:
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history):
-        assert isinstance(verifycap, CHKFileVerifierURI)
+        assert isinstance(verifycap, uri.CHKFileVerifierURI)
         self._node = _Node(verifycap, storage_broker, secret_holder,
                            terminator, history)
 
@@ -1350,13 +1407,13 @@ class ImmutableFileNode:
     # I wrap a CiphertextFileNode with a decryption key
     def __init__(self, filecap, storage_broker, secret_holder, downloader,
                  history):
-        assert isinstance(filecap, CHKFileURI)
+        assert isinstance(filecap, uri.CHKFileURI)
         verifycap = filecap.get_verify_cap()
         self._cnode = CiphertextFileNode(verifycap, storage_broker,
                                          secret_holder, downloader, history)
-        assert isinstance(filecap, CHKFileURI)
+        assert isinstance(filecap, uri.CHKFileURI)
         self.u = filecap
-        # XXX self._readkey
+        self._readkey = filecap.key
 
     def read(self, consumer, offset=0, size=None):
         decryptor = DecryptingConsumer(consumer, self._readkey, offset)
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
index 32a0ad4..df5d11b 100755
--- a/src/allmydata/immutable/download2_util.py
+++ b/src/allmydata/immutable/download2_util.py
@@ -1,6 +1,8 @@
-
 import weakref
 
+from twisted.application import service
+from foolscap.api import eventually
+
 class Observer2:
     """A simple class to distribute multiple events to a single subscriber.
     It accepts arbitrary kwargs, but no posargs."""
@@ -37,14 +39,6 @@ class Observer2:
         if f:
             f(self)
 
-class DictOfSets:
-    def add(self, key, value): pass # XXX
-    def values(self): # return set that merges all value sets
-        r = set()
-        for key in self:
-            r.update(self[key])
-        return r
-
 
 def incidentally(res, f, *args, **kwargs):
     """Add me to a Deferred chain like this:
@@ -60,7 +54,6 @@ def incidentally(res, f, *args, **kwargs):
     return res
 
 
-import weakref
 class Terminator(service.Service):
     def __init__(self):
         service.Service.__init__(self)

commit 0628afbf829a708ce879e3a699db0a74c8e9c691
Author: Brian Warner <warner@lothar.com>
Date:   Fri Apr 23 16:17:55 2010 -0700

    cleanups, add a bit of logging, fix attribute access in _Node
---
 src/allmydata/immutable/download2.py      |  320 ++++++++++++++++++++---------
 src/allmydata/immutable/download2_util.py |    2 +-
 2 files changed, 226 insertions(+), 96 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 48f2b59..a665533 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -3,15 +3,13 @@ import binascii
 from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 
-(UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
- ("UNUSED", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
+(AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
+ ("AVAILABLE", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
 
 class BadSegmentNumberError(Exception):
     pass
 
 class Share:
-    # this is a specific implementation of IShare for tahoe's native storage
-    # servers. A different backend would use a different class.
     """I represent a single instance of a single share (e.g. I reference the
     shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
     I am associated with a CommonShare that remembers data that is held in
@@ -19,18 +17,26 @@ class Share:
     associated with a CiphertextFileNode for e.g. SI=abcde (all shares, all
     servers).
     """
+    # this is a specific implementation of IShare for tahoe's native storage
+    # servers. A different backend would use a different class.
 
-    def __init__(self, rref, verifycap, commonshare, node, peerid, shnum):
+    def __init__(self, rref, verifycap, commonshare, node, peerid, si_s, shnum):
         self._rref = rref
+        self._node = node # holds share_hash_tree and UEB
         self._guess_offsets(verifycap, node.guessed_segment_size)
         self.actual_offsets = None
         self.actual_segment_size = None
         self._UEB_length = None
         self._commonshare = commonshare # holds block_hash_tree
-        self._node = node # holds share_hash_tree and UEB
         self._peerid = peerid
+        self._peerid_s = base32.b2(peerid)[:5]
+        self._si_prefix = si_s[:8]
         self._shnum = shnum
 
+        self._lp = log.msg(format="Share(%(si)s) on server=%(server)s starting",
+                           si=self._si_prefix, server=self._peerid_s,
+                           level=log.NOISY, umid="P7hv2w")
+
         self._wanted = Spans() # desired metadata
         self._wanted_blocks = Spans() # desired block data
         self._requested = Spans() # we've sent a request for this
@@ -51,6 +57,7 @@ class Share:
         size = verifycap.size
         k = verifycap.needed_shares
         N = verifycap.total_shares
+        r = self._node._calculate_sizes(guessed_segment_size, size, k)
         offsets = {}
         for i,field in enumerate('data',
                                  'plaintext_hash_tree', # UNUSED
@@ -76,7 +83,19 @@ class Share:
         o.subscribe(), which gives me a place to send state changes and
         eventually the data block. The second is o.cancel(), which removes
         the request (if it is still active).
+
+        I will distribute the following events through my Observer2:
+         - state=OVERDUE: ?? I believe I should have had an answer by now.
+                          You may want to ask another share instead.
+         - state=BADSEGNUM: the segnum you asked for is too large. I must
+                            fetch a valid UEB before I can determine this,
+                            so the notification is asynchronous
+         - state=COMPLETE, block=data: here is a valid block
+         - state=CORRUPT: this share contains corrupted data
+         - state=DEAD, f=Failure: the server reported an error, this share
+                                  is unusable
         """
+        assert segnum >= 0
         o = Observer2()
         o.set_canceler(self._cancel_block_request)
         for i,(segnum0,observers) in enumerate(self._requested_blocks):
@@ -144,13 +163,13 @@ class Share:
                 # can't even look at anything without the offset table
                 return False
 
-        if self._node.UEB is None:
+        if not self._node.have_UEB:
             if not self._satisfy_UEB():
                 # can't check any hashes without the UEB
                 return False
 
         segnum, observers = self._active_segnum_and_observers()
-        if segnum >= self._node.UEB.num_segments:
+        if segnum >= self._node.num_segments:
             for o in observers:
                 o.notify(state=BADSEGNUM)
             self._requested_blocks.pop(0)
@@ -252,7 +271,8 @@ class Share:
             # adds to self._node.share_hash_tree
             rdata.remove(o["share_hashes"], hashlen)
             return True
-        except IndexError, hashtree.BadHashError, hashtree.NotEnoughHashesError:
+        except (hashtree.BadHashError, hashtree.NotEnoughHashesError,
+                IndexError):
             f = Failure()
             self._signal_corruption(f, o["share_hashes"], hashlen)
             return False
@@ -276,7 +296,7 @@ class Share:
         # we've gotten them all, because the hash tree will throw an
         # exception if we only give it a partial set (which it therefore
         # cannot validate)
-        ok = commonshare.process_block_hashes(block_hashes)
+        ok = commonshare.process_block_hashes(block_hashes, serverid_s)
         if not ok:
             return False
         for hashnum in needed_hashes:
@@ -284,19 +304,13 @@ class Share:
         return True
 
     def _satisfy_data_block(self, segnum, observers):
-        o = self.actual_offsets
-        segsize = self._node.UEB["segment_size"]
-        needed_shares = self._node.UEB["needed_shares"]
-        sharesize = mathutil.div_ceil(self._node.UEB["size"],
-                                      needed_shares)
-        blocksize = mathutil.div_ceil(segsize, needed_shares) # XXX
-        blockstart = o["data"] + segnum * blocksize
-        if blocknum < NUM_BLOCKS-1:
-            blocklen = blocksize
-        else:
-            blocklen = sharesize % blocksize
-            if blocklen == 0:
-                blocklen = blocksize
+        tail = (segnum == self._node.num_segments-1)
+        datastart = self.actual_offsets["data"]
+        blockstart = datastart + segnum * self._node.block_size
+        blocklen = self._node.block_size
+        if tail:
+            blocklen = self._node.tail_block_size
+
         block = rdata.pop(blockstart, blocklen)
         if not block:
             return False
@@ -305,12 +319,12 @@ class Share:
         assert self._requested_blocks[0][0] == segnum
         ok = commonshare.check_block(segnum, block)
         if ok:
-            state = COMPLETE
+            for o in observers:
+                # goes to SegmentFetcher._block_request_activity
+                o.notify(state=COMPLETE, block=block)
         else:
-            state = CORRUPT
-        for o in observers:
-            # goes to SegmentFetcher._block_request_activity
-            o.notify(state=state, block=block)
+            for o in observers:
+                o.notify(state=CORRUPT)
         self._requested_blocks.pop(0) # retired
         return True # got satisfaction
 
@@ -329,7 +343,7 @@ class Share:
 
         o = self.actual_offsets or self.guessed_offsets
         segsize = self.actual_segment_size or self.guessed_segment_size
-        if self._node.UEB is None:
+        if not self._node.have_UEB:
             self._desire_UEB(o)
 
         if self._node.share_hash_tree.needed_hashes(self.shnum):
@@ -344,7 +358,13 @@ class Share:
             self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # data
-        blockstart, blocklen = COMPUTE(segnum, segsize, etc) # XXX
+        r = self._node._calculate_sizes(segsize, size, k) # XXX
+        tail = (segnum == r["num_segments"])
+        datastart = o["data"]
+        blockstart = datastart + segnum * r["block_size"]
+        blocklen = r["block_size"]
+        if tail:
+            blocklen = r["tail_block_size"]
         self._wanted_blocks.add(blockstart, blocklen)
 
 
@@ -402,7 +422,8 @@ class Share:
         # then send requests for data blocks. All the hashes should arrive
         # before the blocks, so the blocks can be consumed and released in a
         # single turn.
-        self._send_requests(self._wanted_blocks - self._received - self._requested
+        ask = self._wanted_blocks - self._received - self._requested
+        self._send_requests(ask)
 
     def _send_requests(self, needed):
         for (start, length) in needed:
@@ -411,7 +432,10 @@ class Share:
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length)
             d.addErrback(self._got_error)
-            d.addErrback(log.err, ...) # XXX
+            d.addErrback(lambda f:
+                         log.err(format="unhandled error during send_request",
+                                 failure=f, parent=self._lp,
+                                 level=log.WEIRD, umid="qZu0wg"))
 
     def _send_request(self, start, length):
         return self._rref.callRemote("read", start, length)
@@ -425,21 +449,35 @@ class Share:
         eventually(self.loop)
 
     def _got_error(self, f): # XXX
-        ...
+        log.msg(format="error requesting %(start)d+%(length)d"
+                " from %(server)s for si %(si)s",
+                start=start, length=length,
+                server=self._peerid_s, si=self._si_prefix,
+                failure=f, parent=self._lp,
+                level=log.UNUSUAL, umid="qZu0wg")
+        # retire our observers, assuming we won't be able to make any
+        # further progress
+        self._fail(f)
+
+    def _fail(self, f):
+        for (segnum, o) in self._requested_blocks:
+            o.notify(state=DEAD, f=f)
 
 
 class CommonShare:
     """I hold data that is common across all instances of a single share,
     like sh2 on both servers A and B. This is just the block hash tree.
     """
-    def __init__(self, numsegs):
+    def __init__(self, numsegs, si_s, shnum):
+        self.si_s = si_s
+        self.shnum = shnum
         if numsegs is not None:
             self._block_hash_tree = IncompleteHashTree(numsegs)
 
     def got_numsegs(self, numsegs):
         self._block_hash_tree = IncompleteHashTree(numsegs)
 
-    def process_block_hashes(self, block_hashes):
+    def process_block_hashes(self, block_hashes, serverid_s):
         try:
             self._block_hash_tree.add_hashes(block_hashes)
             return True
@@ -448,7 +486,7 @@ class CommonShare:
             self.log("hash failure in block_hashes=(%(hashnums)s),"
                      " shnum=%(shnum)d SI=%(si)s server=%(server)s",
                      hashnums=hashnums, shnum=self.shnum,
-                     si=self.si_s, server=serverid_s, # XXX
+                     si=self.si_s, server=serverid_s,
                      level=log.WEIRD, umid="yNyFdA")
         return False
 
@@ -485,13 +523,13 @@ class SegmentFetcher:
     method to have me shut down early."""
 
     def __init__(self, node, segnum, k):
-        self._node = node # CiphertextFileNode
+        self._node = node # _Node
         self.segnum = segnum
         self._k = k
         self._shares = {} # maps non-dead Share instance to a state, one of
-                          # (UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT).
+                          # (AVAILABLE, PENDING, OVERDUE, COMPLETE, CORRUPT).
                           # State transition map is:
-                          #  UNUSED -(send-read)-> PENDING
+                          #  AVAILABLE -(send-read)-> PENDING
                           #  PENDING -(timer)-> OVERDUE
                           #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
                           #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
@@ -503,6 +541,7 @@ class SegmentFetcher:
         self._blocks = {} # maps shnum to validated block data
         self._no_more_shares = False
         self._bad_segnum = False
+        self._last_failure = None
         self._running = True
 
     def stop(self):
@@ -518,7 +557,7 @@ class SegmentFetcher:
         # segment fetch is started and we already know about shares from the
         # previous segment
         for s in shares:
-            self._shares[s] = UNUSED
+            self._shares[s] = AVAILABLE
             self._shnums[s.shnum].add(s)
         eventually(self._loop)
 
@@ -538,6 +577,7 @@ class SegmentFetcher:
         return len(shnums)
 
     def _loop(self):
+        k = self._k
         if not self._running:
             return
         if self._bad_segnum:
@@ -551,7 +591,7 @@ class SegmentFetcher:
             return
 
         # are we done?
-        if self._count_shnums(COMPLETE) >= self._k:
+        if self._count_shnums(COMPLETE) >= k:
             # yay!
             self.stop()
             self._node.process_blocks(self.segnum, self._blocks)
@@ -559,11 +599,23 @@ class SegmentFetcher:
 
         # we may have exhausted everything
         if (self._no_more_shares and
-            self._count_shnums(UNUSED, PENDING, OVERDUE, COMPLETE) < self._k):
+            self._count_shnums(AVAILABLE, PENDING, OVERDUE, COMPLETE) < k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
             self.stop()
-            e = NotEnoughShares("...") # XXX
+            format = ("ran out of shares: %(complete)d complete,"
+                      " %(pending)d pending, %(overdue)d overdue,"
+                      " %(unused)d unused, need %(k)k."
+                      " Last failure: %(last_failure)s")
+            args = {"complete": self._count_shnums(COMPLETE),
+                    "pending": self._count_shnums(PENDING),
+                    "overdue": self._count_shnums(OVERDUE),
+                    "unused": self._count_shnums(AVAILABLE), # should be zero
+                    "k": k,
+                    "last_failure": self._last_failure,
+                    }
+            self.log(format=format, level=log.UNUSUAL, umid="1DsnTg", **args)
+            e = NotEnoughShares(format % args)
             f = Failure(e)
             self._node.fetch_failed(self, f)
             return
@@ -571,7 +623,7 @@ class SegmentFetcher:
         # nope, not done. Are we "block-hungry" (i.e. do we want to send out
         # more read requests, or do we think we have enough in flight
         # already?)
-        while self._count_shnums(PENDING, COMPLETE) < self._k:
+        while self._count_shnums(PENDING, COMPLETE) < k:
             # we're hungry.. are there any unused shares?
             sent = self._send_new_request()
             if not sent:
@@ -579,22 +631,29 @@ class SegmentFetcher:
 
         # ok, now are we "share-hungry" (i.e. do we have enough known shares
         # to make us happy, or should we ask the ShareFinder to get us more?)
-        if self._count_shnums(UNUSED, PENDING, COMPLETE) < self._k:
+        if self._count_shnums(AVAILABLE, PENDING, COMPLETE) < k:
             # we're hungry for more shares
             self._node.want_more_shares()
             # that will trigger the ShareFinder to keep looking
 
+    def _find_one(self, shares, state):
+        # TODO could choose fastest
+        for s in shares:
+            if self._shares[s] == state:
+                return s
+        raise IndexError("shouldn't get here")
+
     def _send_new_request(self):
         for shnum,shares in self._shnums.iteritems():
             states = [self._shares[s] for s in shares]
             if COMPLETE in states or PENDING in states:
                 # don't send redundant requests
                 continue
-            if UNUSED not in states:
+            if AVAILABLE not in states:
                 # no candidates for this shnum, move on
                 continue
             # here's a candidate. Send a request.
-            s = find_one(shares, UNUSED) # XXX could choose fastest
+            s = self._find_one(shares, AVAILABLE)
             self._shares[s] = PENDING
             self._share_observers[s] = o = s.get_block(segnum)
             o.subscribe(self._block_request_activity, share=s, shnum=shnum)
@@ -612,7 +671,7 @@ class SegmentFetcher:
             o.cancel()
         self._share_observers = {}
 
-    def _block_request_activity(self, share, shnum, state, block=None):
+    def _block_request_activity(self, share, shnum, state, block=None, f=None):
         # called by Shares, in response to our s.send_request() calls.
         # COMPLETE, CORRUPT, DEAD, BADSEGNUM are terminal.
         if state in (COMPLETE, CORRUPT, DEAD, BADSEGNUM):
@@ -630,6 +689,7 @@ class SegmentFetcher:
         elif state is DEAD:
             del self._shares[share]
             self._shnums[shnum].remove(share)
+            self._last_failure = f
         elif state is BADSEGNUM:
             self._shares[share] = BADSEGNUM # ???
             self._bad_segnum = True
@@ -643,6 +703,7 @@ class RequestToken:
 class ShareFinder:
     def __init__(self, storage_broker, storage_index,
                  share_consumer, max_outstanding_requests=10):
+        # XXX need self.node -> CiphertextFileNode
         self.running = True
         s = storage_broker.get_servers_for_index(storage_index)
         self._servers = iter(s)
@@ -655,6 +716,7 @@ class ShareFinder:
         self.undelivered_shares = []
         self.pending_requests = set()
 
+        self._si_s = base32.b2a(storage_index)
         self._si_prefix = base32.b2a_l(storage_index[:8], 60)
         self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
                            si=self._si_prefix, level=log.NOISY, umid="2xjj2A")
@@ -662,7 +724,10 @@ class ShareFinder:
         self._num_segments = None
         d = share_consumer.get_num_segments()
         d.addCallback(self._got_numsegs)
-        d.addErrback(log.err, ...) # XXX
+        def _err_numsegs(f):
+            log.err(format="Unable to get number of segments", failure=f,
+                    parent=self._lp, level=log.UNUSUAL, umid="dh38Xw")
+        d.addErrback(_err_numsegs)
 
     def log(self, *args, **kwargs):
         if "parent" not in kwargs:
@@ -760,10 +825,11 @@ class ShareFinder:
                      level=log.NOISY, parent=lp, umid="U7d4JA")
         for shnum, bucket in buckets.iteritems():
             if shnum not in self._commonshares:
-                self._commonshares[shnum] = CommonShare(self._num_segments)
+                self._commonshares[shnum] = CommonShare(self._num_segments,
+                                                        self._si_s, shnum)
             cs = self._commonshares[shnum]
             s = Share(bucket, self.verifycap, cs, self.node,
-                      peerid, shnum)
+                      peerid, self._si_s, shnum)
             self.undelivered_shares.append(s)
 
     def _got_error(self, f, peerid, req):
@@ -815,8 +881,8 @@ class Segmentation:
             self._deferred.callback(self._consumer)
             return
         n = self._node
-        have_actual_segment_size = n.actual_segment_size is not None
-        segment_size = n.actual_segment_size or n.guessed_segment_size
+        have_actual_segment_size = n.segment_size is not None
+        segment_size = n.segment_size or n.guessed_segment_size
         if self._offset == 0:
             # great! we want segment0 for sure
             wanted_segnum = 0
@@ -824,7 +890,7 @@ class Segmentation:
             # this might be a guess
             wanted_segnum = self._offset // segment_size
         self._active_segnum = wanted_segnum
-        d,c = self._node.get_segment(wanted_segnum)
+        d,c = n.get_segment(wanted_segnum)
         self._cancel_segment_request = c
         d.addBoth(self._request_retired)
         d.addCallback(self._got_segment, have_actual_segment_size)
@@ -850,7 +916,7 @@ class Segmentation:
                 raise SOMETHING
             # we've wasted some bandwidth, but now we can grab the right one,
             # because we should know the segsize by now.
-            assert self._node.actual_segment_size is not None
+            assert self._node.segment_size is not None
             self._maybe_fetch_next()
             return
         offset_in_segment = self._offset - segment_start
@@ -897,29 +963,31 @@ class Cancel:
             self.cancelled = True
             self._f(self)
 
-class CiphertextFileNode:
+class _Node:
+    """Internal class which manages downloads and holds state. External
+    callers use CiphertextFileNode instead."""
+
     # Share._node points to me
     def __init__(self, verifycap, storage_broker, secret_holder,
                  terminator, history):
         assert isinstance(verifycap, CHKFileVerifierURI)
-        self.u = verifycap
-        storage_index = verifycap.storage_index
-        self._needed_shares = verifycap.needed_shares
-        self._total_shares = verifycap.total_shares
+        self._verifycap = verifycap
         self.running = True
         terminator.register(self) # calls self.stop() at stopService()
-        # the rule is: only send network requests if you're active
-        # (self.running is True). You can do eventual-sends any time. This
-        # rule should mean that once stopService()+flushEventualQueue()
-        # fires, everything will be done.
+        # the rules are:
+        # 1: Only send network requests if you're active (self.running is True)
+        # 2: Use TimerService, not reactor.callLater
+        # 3: You can do eventual-sends any time.
+        # These rules should mean that once
+        # stopService()+flushEventualQueue() fires, everything will be done.
         self._secret_holder = secret_holder
         self._history = history
 
-        self.share_hash_tree = IncompleteHashTree(self.u.total_shares)
+        k, N = self._verifycap.needed_shares, self._verifycap.total_shares
+        self.share_hash_tree = IncompleteHashTree(N)
 
         # we guess the segment size, so Segmentation can pull non-initial
         # segments in a single roundtrip
-        k = verifycap.needed_shares
         max_segment_size = 128*KiB # TODO: pull from elsewhere, maybe the
                                    # same place as upload.BaseUploadable
         s = mathutil.next_multiple(min(verifycap.size, max_segment_size), k)
@@ -927,12 +995,12 @@ class CiphertextFileNode:
 
         # filled in when we parse a valid UEB
         self.have_UEB = False
-        self.num_segments = None
         self.segment_size = None
-        self.tail_data_size = None
         self.tail_segment_size = None
+        self.tail_segment_padded = None
+        self.num_segments = None
         self.block_size = None
-        self.share_size = None
+        self.tail_block_size = None
         self.ciphertext_hash_tree = None # size depends on num_segments
         self.ciphertext_hash = None # flat hash, optional
 
@@ -943,6 +1011,7 @@ class CiphertextFileNode:
         self._segment_requests = [] # (segnum, d, cancel_handle)
         self._active_segment = None # a SegmentFetcher, with .segnum
 
+        storage_index = verifycap.storage_index
         self._sharefinder = ShareFinder(storage_broker, storage_index, self)
         self._shares = set()
 
@@ -953,8 +1022,8 @@ class CiphertextFileNode:
             self._active_segment = None
         self._sharefinder.stop()
 
-    # things called by our client, either a filenode user or an
-    # ImmutableFileNode wrapper
+    # things called by outside callers, via CiphertextFileNode. get_segment()
+    # may also be called by Segmentation.
 
     def read(self, consumer, offset=0, size=None):
         """I am the main entry point, from which FileNode.read() can get
@@ -963,7 +1032,7 @@ class CiphertextFileNode:
         finished."""
         # for concurrent operations: each gets its own Segmentation manager
         if size is None:
-            size = self._size - offset
+            size = self._verifycap.size - offset
         s = Segmentation(self, offset, size, consumer)
         # this raises an interesting question: what segments to fetch? if
         # offset=0, always fetch the first segment, and then allow
@@ -1013,8 +1082,8 @@ class CiphertextFileNode:
     def _start_new_segment(self):
         if self._active_segment is None and self._segment_requests:
             segnum = self._segment_requests[0][0]
-            self._active_segment = fetcher = SegmentFetcher(self, segnum,
-                                                            self._needed_shares)
+            k = self._verifycap.needed_shares
+            self._active_segment = fetcher = SegmentFetcher(self, segnum, k)
             active_shares = [s for s in self._shares if s.not_dead()]
             fetcher.add_shares(active_shares) # this triggers the loop
 
@@ -1056,30 +1125,24 @@ class CiphertextFileNode:
 
         # therefore, we ignore d['total_shares'] and d['needed_shares'].
 
-        self.share_size = mathutil.div_ceil(self._verifycap.size,
-                                            self._needed_shares)
+        k, N = self._verifycap.needed_shares, self._verifycap.total_shares
+        size = self._verifycap.size
 
         self.segment_size = d['segment_size']
-        for r in self._readers:
-            r.set_segment_size(self.segment_size)
-
-        self.block_size = mathutil.div_ceil(self._segsize, self._needed_shares)
-        self.num_segments = mathutil.div_ceil(self._size, self.segment_size)
 
-        self.tail_data_size = self._size % self.segment_size
-        if self.tail_data_size == 0:
-            self.tail_data_size = self.segment_size
-        # padding for erasure code
-        self.tail_segment_size = mathutil.next_multiple(self.tail_data_size,
-                                                        self._needed_shares)
+        r = self._calculate_sizes(self.segment_size, size, k)
+        self.tail_segment_size = r["tail_segment_size"]
+        self.tail_segment_padded = r["tail_segment_padded"]
+        self.num_segments = r["num_segments"]
+        self.block_size = r["block_size"]
+        self.tail_block_size = r["tail_block_size"]
 
         # zfec.Decode() instantiation is fast, but still, let's use the same
-        # codec for anything we can. 3-of-10 takes 15us on my laptop,
-        # 25-of-100 is 900us, 3-of-255 is 97us, 25-of-255 is 2.5ms,
-        # worst-case 254-of-255 is 9.3ms
+        # codec instance for all but the last segment. 3-of-10 takes 15us on
+        # my laptop, 25-of-100 is 900us, 3-of-255 is 97us, 25-of-255 is
+        # 2.5ms, worst-case 254-of-255 is 9.3ms
         self._codec = codec.CRSDecoder()
-        self._codec.set_params(self.segment_size,
-                               self._needed_shares, self._total_shares)
+        self._codec.set_params(self.segment_size, k, N)
 
 
         # Ciphertext hash tree root is mandatory, so that there is at most
@@ -1107,6 +1170,36 @@ class CiphertextFileNode:
         # redundant fields. The Verifier uses a different code path which
         # does not ignore them.
 
+    def _calculate_sizes(self, segment_size, size, k):
+        # segments of ciphertext
+
+        # this assert matches the one in encode.py:127 inside
+        # Encoded._got_all_encoding_parameters, where the UEB is constructed
+        assert segment_size % k == 0
+
+        # the last segment is usually short. We don't store a whole segsize,
+        # but we do pad the segment up to a multiple of k, because the
+        # encoder requires that.
+        tail_segment_size = size % segment_size
+        if tail_segment_size == 0:
+            tail_segment_size = segment_size
+        padded = mathutil.next_multiple(tail_segment_size, k)
+        tail_segment_padded = padded
+
+        num_segments = mathutil.div_ceil(size, segment_size)
+
+        # each segment is turned into N blocks. All but the last are of size
+        # block_size, and the last is of size tail_block_size
+        block_size = segment_size / k
+        tail_block_size = tail_segment_padded / k
+
+        return { "tail_segment_size": tail_segment_size,
+                 "tail_segment_padded": tail_segment_padded,
+                 "num_segments": num_segments,
+                 "block_size": block_size,
+                 "tail_block_size": tail_block_size,
+                 }
+
 
     def process_share_hashes(self, share_hashes):
         self.share_hash_tree.set_hashes(share_hashes)
@@ -1131,11 +1224,13 @@ class CiphertextFileNode:
             d.errback(f)
 
     def process_blocks(self, segnum, blocks):
+        tail = (segnum == self.num_segments-1)
         codec = self._codec
-        if segnum == self.num_segments-1:
+        if tail:
+            # account for the padding in the last segment
             codec = codec.CRSDecoder()
-            k, N = self._needed_shares, self._total_shares
-            codec.set_params(self.tail_segment_size, k, N)
+            k, N = self._verifycap.needed_shares, self._verifycap.total_shares
+            codec.set_params(self.tail_segment_padded, k, N)
 
         shares = []
         shareids = []
@@ -1145,6 +1240,8 @@ class CiphertextFileNode:
         del blocks
         segment = codec.decode(shares, shareids)
         del shares
+        if tail:
+            segment = segment[self.tail_segment_size:]
         self._process_segment(segnum, segment)
 
     def _process_segment(self, segnum, segment):
@@ -1185,6 +1282,38 @@ class CiphertextFileNode:
             self._active_segment = None
             self._start_new_segment()
 
+class CiphertextFileNode:
+    def __init__(self, verifycap, storage_broker, secret_holder,
+                 terminator, history):
+        assert isinstance(verifycap, CHKFileVerifierURI)
+        self._node = _Node(verifycap, storage_broker, secret_holder,
+                           terminator, history)
+
+    def read(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data. I feed the consumer with the desired range of ciphertext. I
+        return a Deferred that fires (with the consumer) when the read is
+        finished."""
+        return self._node.read(consumer, offset, size)
+
+    def get_segment(self, segnum):
+        """Begin downloading a segment. I return a tuple (d, c): 'd' is a
+        Deferred that fires with (offset,data) when the desired segment is
+        available, and c is an object on which c.cancel() can be called to
+        disavow interest in the segment (after which 'd' will never fire).
+
+        You probably need to know the segment size before calling this,
+        unless you want the first few bytes of the file. If you ask for a
+        segment number which turns out to be too large, the Deferred will
+        errback with BadSegmentNumberError.
+
+        The Deferred fires with the offset of the first byte of the data
+        segment, so that you can call get_segment() before knowing the
+        segment size, and still know which data you received.
+        """
+        return self._node.get_segment(segnum)
+
+
 class DecryptingConsumer:
     """I sit between a CiphertextDownloader (which acts as a Producer) and
     the real Consumer, decrypting everything that passes by. The real
@@ -1227,6 +1356,7 @@ class ImmutableFileNode:
                                          secret_holder, downloader, history)
         assert isinstance(filecap, CHKFileURI)
         self.u = filecap
+        # XXX self._readkey
 
     def read(self, consumer, offset=0, size=None):
         decryptor = DecryptingConsumer(consumer, self._readkey, offset)
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
index 48f2f0a..32a0ad4 100755
--- a/src/allmydata/immutable/download2_util.py
+++ b/src/allmydata/immutable/download2_util.py
@@ -38,7 +38,7 @@ class Observer2:
             f(self)
 
 class DictOfSets:
-    def add(self, key, value): pass
+    def add(self, key, value): pass # XXX
     def values(self): # return set that merges all value sets
         r = set()
         for key in self:

commit da0faec64c7ad424e4344550fa4cbfbeaf0d91dd
Author: Brian Warner <warner@lothar.com>
Date:   Wed Apr 14 12:12:34 2010 -0700

    more
---
 src/allmydata/immutable/download2.py |   26 ++++++++++++++++++--------
 1 files changed, 18 insertions(+), 8 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 440459c..48f2b59 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -218,7 +218,7 @@ class Share:
             return False
         rdata.remove(o["uri_extension"], fsize)
         try:
-            self._node.validate_UEB(UEB_s) # stores in self._node.UEB # XXX
+            self._node.validate_and_store_UEB(UEB_s)
             self.actual_segment_size = self._node.segment_size
             assert self.actual_segment_size is not None
             return True
@@ -276,7 +276,7 @@ class Share:
         # we've gotten them all, because the hash tree will throw an
         # exception if we only give it a partial set (which it therefore
         # cannot validate)
-        ok = commonshare.process_block_hashes(block_hashes) # XXX
+        ok = commonshare.process_block_hashes(block_hashes)
         if not ok:
             return False
         for hashnum in needed_hashes:
@@ -440,8 +440,18 @@ class CommonShare:
         self._block_hash_tree = IncompleteHashTree(numsegs)
 
     def process_block_hashes(self, block_hashes):
-        self._block_hash_tree.add_hashes(block_hashes)
-        return True
+        try:
+            self._block_hash_tree.add_hashes(block_hashes)
+            return True
+        except (hashtree.BadHashError, hashtree.NotEnoughHashesError):
+            hashnums = ",".join(sorted(block_hashes.keys()))
+            self.log("hash failure in block_hashes=(%(hashnums)s),"
+                     " shnum=%(shnum)d SI=%(si)s server=%(server)s",
+                     hashnums=hashnums, shnum=self.shnum,
+                     si=self.si_s, server=serverid_s, # XXX
+                     level=log.WEIRD, umid="yNyFdA")
+        return False
+
     def check_block(self, segnum, block):
         h = hashutil.block_hash(block)
         try:
@@ -1021,20 +1031,20 @@ class CiphertextFileNode:
 
     # things called by our Share instances
 
-    def validate_UEB(self, UEB_s):
+    def validate_and_store_UEB(self, UEB_s):
         h = hashutil.uri_extension_hash(UEB_s)
         if h != self._verifycap.uri_extension_hash:
             raise hashutil.BadHashError
         UEB_dict = uri.unpack_extension(data)
-        self._parse_UEB(self, UEB_dict) # sets self._stuff
+        self._parse_and_store_UEB(self, UEB_dict) # sets self._stuff
         # TODO: a malformed (but authentic) UEB could throw an assertion in
-        # _parse_UEB, and we should abandon the download.
+        # _parse_and_store_UEB, and we should abandon the download.
         self.have_UEB = True
         self._segsize_observers.fire(self.segment_size)
         self._numsegs_observers.fire(self.num_segments)
 
 
-    def _parse_UEB(self, d):
+    def _parse_and_store_UEB(self, d):
         # Note: the UEB contains needed_shares and total_shares. These are
         # redundant and inferior (the filecap contains the authoritative
         # values). However, because it is possible to encode the same file in

commit 08fb07e61dee4d9420813e65d4f33f9f46088bde
Author: Brian Warner <warner@lothar.com>
Date:   Wed Mar 10 10:59:07 2010 -0800

    notes
---
 src/allmydata/immutable/download2.py |    5 +++++
 1 files changed, 5 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 5aae1e8..440459c 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -406,6 +406,7 @@ class Share:
 
     def _send_requests(self, needed):
         for (start, length) in needed:
+            # TODO: quantize to reasonably-large blocks
             self._requested.add(start, length)
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length)
@@ -1266,3 +1267,7 @@ class ImmutableFileNode:
 # CORRUPT). The SegmentFetcher it then responsible for shutting down, and
 # informing its parent (the CiphertextFileNode) of the BadSegmentNumberError,
 # which is then passed to the client of get_segment().
+
+
+# TODO: if offset table is corrupt, attacker could cause us to fetch whole
+# (large) share

commit ec53ca28691e938ce5c1f47e1d6521ad6a5d5597
Author: Brian Warner <warner@lothar.com>
Date:   Wed Mar 3 11:54:09 2010 -0800

    add comments to address Nathan+Zooko's concerns
---
 src/allmydata/immutable/download2.py |   15 +++++++++++++++
 1 files changed, 15 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 67f6dd7..5aae1e8 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -379,6 +379,10 @@ class Share:
             # turns out to be larger, we'll come back here later with a known
             # length and fetch the rest.
             self._wanted.add(o["uri_extension"], 2048)
+            # now, while that is probably enough to fetch the whole UEB, it
+            # might not be, so we need to do the next few steps as well. In
+            # most cases, the following steps will not actually add anything
+            # to self._wanted
 
         self._wanted.add(o["uri_extension"], self._fieldsize)
         # only use a length if we're sure it's correct, otherwise we'll
@@ -1030,6 +1034,17 @@ class CiphertextFileNode:
 
 
     def _parse_UEB(self, d):
+        # Note: the UEB contains needed_shares and total_shares. These are
+        # redundant and inferior (the filecap contains the authoritative
+        # values). However, because it is possible to encode the same file in
+        # multiple ways, and the encoders might choose (poorly) to use the
+        # same key for both (therefore getting the same SI), we might
+        # encounter shares for both types. The UEB hashes will be different,
+        # however, and we'll disregard the "other" encoding's shares as
+        # corrupted.
+
+        # therefore, we ignore d['total_shares'] and d['needed_shares'].
+
         self.share_size = mathutil.div_ceil(self._verifycap.size,
                                             self._needed_shares)
 

commit 1b5bd98ce6dc8b38bc4696f54103011c7f77470e
Author: Brian Warner <warner@lothar.com>
Date:   Mon Feb 15 22:06:37 2010 -0800

    more work
---
 src/allmydata/immutable/download2.py |    7 +++++++
 1 files changed, 7 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 9468c14..67f6dd7 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1213,6 +1213,13 @@ class ImmutableFileNode:
 # Or, don't bother, let the first block all come from one server, and take
 # comfort in the fact that we'll learn about the other servers by the time we
 # fetch the second block.
+#
+# davidsarah points out that we could use sequential (instead of parallel)
+# fetching of multiple block from a single server: by the time the first
+# block arrives, we'll hopefully have heard about other shares. This would
+# induce some RTT delays (i.e. lose pipelining) in the case that this server
+# has the only shares, but that seems tolerable. We could rig it to only use
+# sequential requests on the first segment.
 
 # as a query gets later, we're more willing to duplicate work.
 

commit c589da57cf6170d3d5a72793cae8f5c758172601
Author: Brian Warner <warner@lothar.com>
Date:   Thu Feb 11 22:51:54 2010 -0800

    small cleanups, start adding logging
---
 src/allmydata/immutable/download2.py |   38 +++++++++++++++++++++++++++------
 1 files changed, 31 insertions(+), 7 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 8902508..9468c14 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -20,7 +20,7 @@ class Share:
     servers).
     """
 
-    def __init__(self, rref, verifycap, commonshare, node):
+    def __init__(self, rref, verifycap, commonshare, node, peerid, shnum):
         self._rref = rref
         self._guess_offsets(verifycap, node.guessed_segment_size)
         self.actual_offsets = None
@@ -28,6 +28,9 @@ class Share:
         self._UEB_length = None
         self._commonshare = commonshare # holds block_hash_tree
         self._node = node # holds share_hash_tree and UEB
+        self._peerid = peerid
+        self._shnum = shnum
+
         self._wanted = Spans() # desired metadata
         self._wanted_blocks = Spans() # desired block data
         self._requested = Spans() # we've sent a request for this
@@ -618,8 +621,9 @@ class SegmentFetcher:
         eventually(self._loop)
 
 
-class Token:
-    pass
+class RequestToken:
+    def __init__(self, peerid):
+        self.peerid = peerid
 
 class ShareFinder:
     def __init__(self, storage_broker, storage_index,
@@ -636,12 +640,19 @@ class ShareFinder:
         self.undelivered_shares = []
         self.pending_requests = set()
 
+        self._si_prefix = base32.b2a_l(storage_index[:8], 60)
+        self._lp = log.msg(format="ShareFinder[si=%(si)s] starting",
+                           si=self._si_prefix, level=log.NOISY, umid="2xjj2A")
+
         self._num_segments = None
         d = share_consumer.get_num_segments()
         d.addCallback(self._got_numsegs)
         d.addErrback(log.err, ...) # XXX
 
-    def log(self, **kwargs):
+    def log(self, *args, **kwargs):
+        if "parent" not in kwargs:
+            kwargs["parent"] = self._lp
+        return log.msg(*args, **kwargs)
 
     def stop(self):
         self.running = False
@@ -653,11 +664,23 @@ class ShareFinder:
 
     # called by our parent CiphertextDownloader
     def hungry(self):
+        log.msg(format="ShareFinder[si=%(si)s] hungry",
+                si=self._si_prefix, level=log.NOISY, umid="NywYaQ")
         self._hungry = True
         eventually(self.loop)
 
     # internal methods
     def loop(self):
+        log.msg(format="ShareFinder[si=%(si)s] loop: running=%(running)s"
+                " hungry=%(hungry)s, undelivered=%(undelivered)s,"
+                " pending=%(pending)s",
+                si=self._si_prefix, running=self._running, hungry=self._hungry,
+                undelivered=",".join(["sh%d@%s" % (s._shnum,
+                                                   idlib.shortnodeid_b2a(s._peerid))
+                                      for s in self.undelivered_shares]),
+                pending=",".join([idlib.shortnodeid_b2a(rt.peerid)
+                                  for rt in self.pending_requests]), # sort?
+                level=log.NOISY, umid="kRtS4Q")
         if not self.running:
             return
         if not self._hungry:
@@ -696,9 +719,9 @@ class ShareFinder:
 
 
     def send_request(self, server):
-        req = Token()
-        self.pending_requests.add(req)
         peerid, rref = server
+        req = RequestToken(peerid)
+        self.pending_requests.add(req)
         lp = self.log(format="sending DYHB to [%(peerid)s]",
                       peerid=idlib.shortnodeid_b2a(peerid),
                       level=log.NOISY, umid="Io7pyg")
@@ -724,7 +747,8 @@ class ShareFinder:
             if shnum not in self._commonshares:
                 self._commonshares[shnum] = CommonShare(self._num_segments)
             cs = self._commonshares[shnum]
-            s = Share(bucket, self.verifycap, cs, self.node)
+            s = Share(bucket, self.verifycap, cs, self.node,
+                      peerid, shnum)
             self.undelivered_shares.append(s)
 
     def _got_error(self, f, peerid, req):

commit 53c827bf10f44249b08d007dd18e31877fe8e894
Author: Brian Warner <warner@lothar.com>
Date:   Thu Feb 11 22:51:28 2010 -0800

    move Terminator into download2_util.py
---
 src/allmydata/immutable/download2.py      |   16 ++--------------
 src/allmydata/immutable/download2_util.py |   13 +++++++++++++
 2 files changed, 15 insertions(+), 14 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 5d2ceb7..8902508 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -3,20 +3,8 @@ import binascii
 from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 
-import weakref
-class Terminator(service.Service):
-    def __init__(self):
-        service.Service.__init__(self)
-        self._clients = weakref.WeakKeyDictionary()
-    def register(self, c):
-        self._clients[c] = None
-    def stopService(self):
-        for c in self._clients:
-            c.stop()
-        return service.Service.stopService(self)
-
-# TODO: just use strings
-(UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = range(7)
+(UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = \
+ ("UNUSED", "PENDING", "OVERDUE", "COMPLETE", "CORRUPT", "DEAD", "BADSEGNUM")
 
 class BadSegmentNumberError(Exception):
     pass
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
index 3ddcd1d..48f2f0a 100755
--- a/src/allmydata/immutable/download2_util.py
+++ b/src/allmydata/immutable/download2_util.py
@@ -58,3 +58,16 @@ def incidentally(res, f, *args, **kwargs):
     """
     f(*args, **kwargs)
     return res
+
+
+import weakref
+class Terminator(service.Service):
+    def __init__(self):
+        service.Service.__init__(self)
+        self._clients = weakref.WeakKeyDictionary()
+    def register(self, c):
+        self._clients[c] = None
+    def stopService(self):
+        for c in self._clients:
+            c.stop()
+        return service.Service.stopService(self)

commit b3408d6a1a001734efafbe804816d08a4ebd5992
Author: Brian Warner <warner@lothar.com>
Date:   Thu Feb 11 22:21:55 2010 -0800

    spans: implement DataSpans and extensive tests
---
 src/allmydata/test/test_util.py |  267 +++++++++++++++++++++++++++++++++++++--
 src/allmydata/util/spans.py     |  199 +++++++++++++++++++++++++++--
 2 files changed, 446 insertions(+), 20 deletions(-)

diff --git a/src/allmydata/test/test_util.py b/src/allmydata/test/test_util.py
index caf6c36..5f6ce67 100644
--- a/src/allmydata/test/test_util.py
+++ b/src/allmydata/test/test_util.py
@@ -14,7 +14,7 @@ from allmydata.util import assertutil, fileutil, deferredutil, abbreviate
 from allmydata.util import limiter, time_format, pollmixin, cachedir
 from allmydata.util import statistics, dictutil, pipeline
 from allmydata.util import log as tahoe_log
-from allmydata.util.spans import Spans, overlap, adjacent
+from allmydata.util.spans import Spans, overlap, DataSpans
 
 class Base32(unittest.TestCase):
     def test_b2a_matches_Pythons(self):
@@ -1709,7 +1709,6 @@ class ByteSpans(unittest.TestCase):
             ns1 = S1(); ns2 = S2()
             for i in range(10):
                 what = md5(subseed+str(i)).hexdigest()
-                op = what[0]
                 start = int(what[2:4], 16)
                 length = max(1,int(what[5:6], 16))
                 ns1.add(start, length); ns2.add(start, length)
@@ -1779,20 +1778,19 @@ class ByteSpans(unittest.TestCase):
     # bool(s)  (__len__)
     # s = s1+s2, s1-s2, +=s1, -=s1
 
-class ByteSpans_Overlap(unittest.TestCase):
     def test_overlap(self):
         for a in range(20):
             for b in range(10):
                 for c in range(20):
                     for d in range(10):
-                        self._test(a,b,c,d)
+                        self._test_overlap(a,b,c,d)
 
-    def _test(self, a, b, c, d):
+    def _test_overlap(self, a, b, c, d):
         s1 = set(range(a,a+b))
         s2 = set(range(c,c+d))
         #print "---"
-        #self._show(s1, "1")
-        #self._show(s2, "2")
+        #self._show_overlap(s1, "1")
+        #self._show_overlap(s2, "2")
         o = overlap(a,b,c,d)
         expected = s1.intersection(s2)
         if not expected:
@@ -1803,7 +1801,7 @@ class ByteSpans_Overlap(unittest.TestCase):
             #self._show(so, "o")
             self.failUnlessEqual(so, expected)
 
-    def _show(self, s, c):
+    def _show_overlap(self, s, c):
         import sys
         out = sys.stdout
         if s:
@@ -1813,3 +1811,256 @@ class ByteSpans_Overlap(unittest.TestCase):
                 else:
                     out.write(" ")
         out.write("\n")
+
+def extend(s, start, length, fill):
+    if len(s) >= start+length:
+        return s
+    assert len(fill) == 1
+    return s + fill*(start+length-len(s))
+
+def replace(s, start, data):
+    assert len(s) >= start+len(data)
+    return s[:start] + data + s[start+len(data):]
+
+class SimpleDataSpans:
+    def __init__(self, other=None):
+        self.missing = "" # "1" where missing, "0" where found
+        self.data = ""
+        if other:
+            for (start, data) in other.get_spans():
+                self.add(start, data)
+
+    def __len__(self):
+        return len(self.missing.translate(None, "1"))
+    def _dump(self):
+        return [i for (i,c) in enumerate(self.missing) if c == "0"]
+    def _have(self, start, length):
+        m = self.missing[start:start+length]
+        if not m or len(m)<length or int(m):
+            return False
+        return True
+    def get_spans(self):
+        for i in self._dump():
+            yield (i, self.data[i])
+    def get(self, start, length):
+        if self._have(start, length):
+            return self.data[start:start+length]
+        return None
+    def pop(self, start, length):
+        data = self.get(start, length)
+        if data:
+            self.remove(start, length)
+        return data
+    def remove(self, start, length):
+        self.missing = replace(extend(self.missing, start, length, "1"),
+                               start, "1"*length)
+    def add(self, start, data):
+        self.missing = replace(extend(self.missing, start, len(data), "1"),
+                               start, "0"*len(data))
+        self.data = replace(extend(self.data, start, len(data), " "),
+                            start, data)
+
+
+class StringSpans(unittest.TestCase):
+    def do_basic(self, klass):
+        ds = klass()
+        self.failUnlessEqual(len(ds), 0)
+        self.failUnlessEqual(list(ds._dump()), [])
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 0)
+        self.failUnlessEqual(ds.get(0, 4), None)
+        self.failUnlessEqual(ds.pop(0, 4), None)
+        ds.remove(0, 4)
+
+        ds.add(2, "four")
+        self.failUnlessEqual(len(ds), 4)
+        self.failUnlessEqual(list(ds._dump()), [2,3,4,5])
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 4)
+        self.failUnlessEqual(ds.get(0, 4), None)
+        self.failUnlessEqual(ds.pop(0, 4), None)
+        self.failUnlessEqual(ds.get(4, 4), None)
+
+        ds2 = klass(ds)
+        self.failUnlessEqual(len(ds2), 4)
+        self.failUnlessEqual(list(ds2._dump()), [2,3,4,5])
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds2.get_spans()]), 4)
+        self.failUnlessEqual(ds2.get(0, 4), None)
+        self.failUnlessEqual(ds2.pop(0, 4), None)
+        self.failUnlessEqual(ds2.pop(2, 3), "fou")
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds2.get_spans()]), 1)
+        self.failUnlessEqual(ds2.get(2, 3), None)
+        self.failUnlessEqual(ds2.get(5, 1), "r")
+        self.failUnlessEqual(ds.get(2, 3), "fou")
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 4)
+
+        ds.add(0, "23")
+        self.failUnlessEqual(len(ds), 6)
+        self.failUnlessEqual(list(ds._dump()), [0,1,2,3,4,5])
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 6)
+        self.failUnlessEqual(ds.get(0, 4), "23fo")
+        self.failUnlessEqual(ds.pop(0, 4), "23fo")
+        self.failUnlessEqual(sum([len(d) for (s,d) in ds.get_spans()]), 2)
+        self.failUnlessEqual(ds.get(0, 4), None)
+        self.failUnlessEqual(ds.pop(0, 4), None)
+
+        ds = klass()
+        ds.add(2, "four")
+        ds.add(3, "ea")
+        self.failUnlessEqual(ds.get(2, 4), "fear")
+
+    def do_scan(self, klass):
+        # do a test with gaps and spans of size 1 and 2
+        #  left=(1,11) * right=(1,11) * gapsize=(1,2)
+        # 111, 112, 121, 122, 211, 212, 221, 222
+        #    211
+        #      121
+        #         112
+        #            212
+        #               222
+        #                   221
+        #                      111
+        #                        122
+        #  11 1  1 11 11  11  1 1  111
+        # 0123456789012345678901234567
+        # abcdefghijklmnopqrstuvwxyz-=
+        pieces = [(1, "bc"),
+                  (4, "e"),
+                  (7, "h"),
+                  (9, "jk"),
+                  (12, "mn"),
+                  (16, "qr"),
+                  (20, "u"),
+                  (22, "w"),
+                  (25, "z-="),
+                  ]
+        p_elements = set([1,2,4,7,9,10,12,13,16,17,20,22,25,26,27])
+        S = "abcdefghijklmnopqrstuvwxyz-="
+        # TODO: when adding data, add capital letters, to make sure we aren't
+        # just leaving the old data in place
+        l = len(S)
+        def base():
+            ds = klass()
+            for start, data in pieces:
+                ds.add(start, data)
+            return ds
+        def dump(s):
+            p = set(s._dump())
+            # wow, this is the first time I've ever wanted ?: in python
+            # note: this requires python2.5
+            d = "".join([(S[i] if i in p else " ") for i in range(l)])
+            assert len(d) == l
+            return d
+        DEBUG = False
+        for start in range(0, l):
+            for end in range(start+1, l):
+                # add [start-end) to the baseline
+                which = "%d-%d" % (start, end-1)
+                p_added = set(range(start, end))
+                b = base()
+                if DEBUG:
+                    print
+                    print dump(b), which
+                    add = klass(); add.add(start, S[start:end])
+                    print dump(add)
+                b.add(start, S[start:end])
+                if DEBUG:
+                    print dump(b)
+                # check that the new span is there
+                d = b.get(start, end-start)
+                self.failUnlessEqual(d, S[start:end], which)
+                # check that all the original pieces are still there
+                for t_start, t_data in pieces:
+                    t_len = len(t_data)
+                    self.failUnlessEqual(b.get(t_start, t_len),
+                                         S[t_start:t_start+t_len],
+                                         "%s %d+%d" % (which, t_start, t_len))
+                # check that a lot of subspans are mostly correct
+                for t_start in range(l):
+                    for t_len in range(1,4):
+                        d = b.get(t_start, t_len)
+                        if d is not None:
+                            which2 = "%s+(%d-%d)" % (which, t_start,
+                                                     t_start+t_len-1)
+                            self.failUnlessEqual(d, S[t_start:t_start+t_len],
+                                                 which2)
+                        # check that removing a subspan gives the right value
+                        b2 = klass(b)
+                        b2.remove(t_start, t_len)
+                        removed = set(range(t_start, t_start+t_len))
+                        for i in range(l):
+                            exp = (((i in p_elements) or (i in p_added))
+                                   and (i not in removed))
+                            which2 = "%s-(%d-%d)" % (which, t_start,
+                                                     t_start+t_len-1)
+                            self.failUnlessEqual(bool(b2.get(i, 1)), exp,
+                                                 which2+" %d" % i)
+
+    def test_test(self):
+        self.do_basic(SimpleDataSpans)
+        self.do_scan(SimpleDataSpans)
+
+    def test_basic(self):
+        self.do_basic(DataSpans)
+        self.do_scan(DataSpans)
+
+    def test_random(self):
+        # attempt to increase coverage of corner cases by comparing behavior
+        # of a simple-but-slow model implementation against the
+        # complex-but-fast actual implementation, in a large number of random
+        # operations
+        S1 = SimpleDataSpans
+        S2 = DataSpans
+        s1 = S1(); s2 = S2()
+        seed = ""
+        def _randstr(length, seed):
+            created = 0
+            pieces = []
+            while created < length:
+                piece = md5(seed + str(created)).hexdigest()
+                pieces.append(piece)
+                created += len(piece)
+            return "".join(pieces)[:length]
+        def _create(subseed):
+            ns1 = S1(); ns2 = S2()
+            for i in range(10):
+                what = md5(subseed+str(i)).hexdigest()
+                start = int(what[2:4], 16)
+                length = max(1,int(what[5:6], 16))
+                ns1.add(start, _randstr(length, what[7:9]));
+                ns2.add(start, _randstr(length, what[7:9]))
+            return ns1, ns2
+
+        #print
+        for i in range(1000):
+            what = md5(seed+str(i)).hexdigest()
+            op = what[0]
+            subop = what[1]
+            start = int(what[2:4], 16)
+            length = max(1,int(what[5:6], 16))
+            #print what
+            if op in "0":
+                if subop in "0123456":
+                    s1 = S1(); s2 = S2()
+                else:
+                    s1, s2 = _create(what[7:11])
+                #print "s2 = %s" % list(s2._dump())
+            elif op in "123456":
+                #print "s2.add(%d,%d)" % (start, length)
+                s1.add(start, _randstr(length, what[7:9]));
+                s2.add(start, _randstr(length, what[7:9]))
+            elif op in "789abc":
+                #print "s2.remove(%d,%d)" % (start, length)
+                s1.remove(start, length); s2.remove(start, length)
+            else:
+                #print "s2.pop(%d,%d)" % (start, length)
+                d1 = s1.pop(start, length); d2 = s2.pop(start, length)
+                self.failUnlessEqual(d1, d2)
+            #print "s1 now %s" % list(s1._dump())
+            #print "s2 now %s" % list(s2._dump())
+            self.failUnlessEqual(len(s1), len(s2))
+            self.failUnlessEqual(list(s1._dump()), list(s2._dump()))
+            for j in range(100):
+                what = md5(what[12:14]+str(j)).hexdigest()
+                start = int(what[2:4], 16)
+                length = max(1, int(what[5:6], 16))
+                d1 = s1.get(start, length); d2 = s2.get(start, length)
+                self.failUnlessEqual(d1, d2, "%d+%d" % (start, length))
diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
index 39ee55f..336fddf 100755
--- a/src/allmydata/util/spans.py
+++ b/src/allmydata/util/spans.py
@@ -33,12 +33,11 @@ class Spans:
 
     def _check(self):
         assert sorted(self._spans) == self._spans
-        prev_start = prev_end = None
+        prev_end = None
         try:
             for (start,length) in self._spans:
                 if prev_end is not None:
                     assert start > prev_end
-                prev_start = start
                 prev_end = start+length
         except AssertionError:
             print "BAD:", self.dump()
@@ -84,7 +83,6 @@ class Spans:
         assert start >= 0
         assert length > 0
         #print " REMOVE [%d+%d -%d) from %s" % (start, length, start+length, self.dump())
-        end = start+length
         first_complete_overlap = last_complete_overlap = None
         for i,(s_start,s_length) in enumerate(self._spans):
             s_end = s_start + s_length
@@ -209,31 +207,208 @@ def adjacent(start0, length0, start1, length1):
         return True
     return False
 
-class SimpleDataSpans:
+class DataSpans:
     """I represent portions of a large string. Equivalently, I can be said to
     maintain a large array of characters (with gaps of empty elements). I can
     be used to manage access to a remote share, where some pieces have been
     retrieved, some have been requested, and others have not been read.
     """
 
-    def __init__(self):
-        pass
+    def __init__(self, other=None):
+        self.spans = [] # (start, data) tuples, non-overlapping, merged
+        if other:
+            for (start, data) in other.get_spans():
+                self.add(start, data)
+
+    def __len__(self):
+        # return number of bytes we're holding
+        return sum([len(data) for (start,data) in self.spans])
+
+    def _dump(self):
+        # return iterator of sorted list of offsets, one per byte
+        for (start,data) in self.spans:
+            for i in range(start, start+len(data)):
+                yield i
+
+    def get_spans(self):
+        return list(self.spans)
+
+    def assert_invariants(self):
+        if not self.spans:
+            return
+        prev_start = self.spans[0][0]
+        prev_end = prev_start + len(self.spans[0][1])
+        for start, data in self.spans[1:]:
+            if not start > prev_end:
+                # adjacent or overlapping: bad
+                print "ASSERTION FAILED", self.spans
+                raise AssertionError
 
     def get(self, start, length):
         # returns a string of LENGTH, or None
-        pass
+        #print "get", start, length, self.spans
+        end = start+length
+        for (s_start,s_data) in self.spans:
+            s_end = s_start+len(s_data)
+            #print " ",s_start,s_end
+            if s_start <= start < s_end:
+                # we want some data from this span. Because we maintain
+                # strictly merged and non-overlapping spans, everything we
+                # want must be in this span.
+                offset = start - s_start
+                if offset + length > len(s_data):
+                    #print " None, span falls short"
+                    return None # span falls short
+                #print " some", s_data[offset:offset+length]
+                return s_data[offset:offset+length]
+            if s_start >= end:
+                # we've gone too far: no further spans will overlap
+                #print " None, gone too far"
+                return None
+        #print " None, ran out of spans"
+        return None
 
     def add(self, start, data):
-        pass
+        # first: walk through existing spans, find overlap, modify-in-place
+        #  create list of new spans
+        #  add new spans
+        #  sort
+        #  merge adjacent spans
+        #print "add", start, data, self.spans
+        end = start + len(data)
+        i = 0
+        while len(data):
+            #print " loop", start, data, i, len(self.spans), self.spans
+            if i >= len(self.spans):
+                #print " append and done"
+                # append a last span
+                self.spans.append( (start, data) )
+                break
+            (s_start,s_data) = self.spans[i]
+            # five basic cases:
+            #  a: OLD  b:OLDD  c1:OLD  c2:OLD   d1:OLDD  d2:OLD  e: OLLDD
+            #    NEW     NEW      NEW     NEWW      NEW      NEW     NEW
+            #
+            # we handle A by inserting a new segment (with "N") and looping,
+            # turning it into B or C. We handle B by replacing a prefix and
+            # terminating. We handle C (both c1 and c2) by replacing the
+            # segment (and, for c2, looping, turning it into A). We handle D
+            # by replacing a suffix (and, for d2, looping, turning it into
+            # A). We handle E by replacing the middle and terminating.
+            if start < s_start:
+                # case A: insert a new span, then loop with the remainder
+                #print " insert new psan"
+                s_len = s_start-start
+                self.spans.insert(i, (start, data[:s_len]))
+                i += 1
+                start = s_start
+                data = data[s_len:]
+                continue
+            s_len = len(s_data)
+            s_end = s_start+s_len
+            if s_start <= start < s_end:
+                #print " modify this span", s_start, start, s_end
+                # we want to modify some data in this span: a prefix, a
+                # suffix, or the whole thing
+                if s_start == start:
+                    if s_end <= end:
+                        #print " replace whole segment"
+                        # case C: replace this segment
+                        self.spans[i] = (s_start, data[:s_len])
+                        i += 1
+                        start += s_len
+                        data = data[s_len:]
+                        # C2 is where len(data)>0
+                        continue
+                    # case B: modify the prefix, retain the suffix
+                    #print " modify prefix"
+                    self.spans[i] = (s_start, data + s_data[len(data):])
+                    break
+                if start > s_start and end < s_end:
+                    # case E: modify the middle
+                    #print " modify middle"
+                    prefix_len = start - s_start # we retain this much
+                    suffix_len = s_end - end # and retain this much
+                    newdata = s_data[:prefix_len] + data + s_data[-suffix_len:]
+                    self.spans[i] = (s_start, newdata)
+                    break
+                # case D: retain the prefix, modify the suffix
+                #print " modify suffix"
+                prefix_len = start - s_start # we retain this much
+                suffix_len = s_len - prefix_len # we replace this much
+                #print "  ", s_data, prefix_len, suffix_len, s_len, data
+                self.spans[i] = (s_start,
+                                 s_data[:prefix_len] + data[:suffix_len])
+                i += 1
+                start += suffix_len
+                data = data[suffix_len:]
+                #print "  now", start, data
+                # D2 is where len(data)>0
+                continue
+            # else we're not there yet
+            #print " still looking"
+            i += 1
+            continue
+        # now merge adjacent spans
+        #print " merging", self.spans
+        newspans = []
+        for (s_start,s_data) in self.spans:
+            if newspans and adjacent(newspans[-1][0], len(newspans[-1][1]),
+                                     s_start, len(s_data)):
+                newspans[-1] = (newspans[-1][0], newspans[-1][1] + s_data)
+            else:
+                newspans.append( (s_start, s_data) )
+        self.spans = newspans
+        self.assert_invariants()
+        #print " done", self.spans
 
     def remove(self, start, length):
-        pass
+        i = 0
+        end = start + length
+        #print "remove", start, length, self.spans
+        while i < len(self.spans):
+            (s_start,s_data) = self.spans[i]
+            if s_start >= end:
+                # this segment is entirely right of the removed region, and
+                # all further segments are even further right. We're done.
+                break
+            s_len = len(s_data)
+            s_end = s_start + s_len
+            o = overlap(start, length, s_start, s_len)
+            if not o:
+                i += 1
+                continue
+            o_start, o_len = o
+            o_end = o_start + o_len
+            if o_len == s_len:
+                # remove the whole segment
+                del self.spans[i]
+                continue
+            if o_start == s_start:
+                # remove a prefix, leaving the suffix from o_end to s_end
+                prefix_len = o_end - o_start
+                self.spans[i] = (o_end, s_data[prefix_len:])
+                i += 1
+                continue
+            elif o_end == s_end:
+                # remove a suffix, leaving the prefix from s_start to o_start
+                prefix_len = o_start - s_start
+                self.spans[i] = (s_start, s_data[:prefix_len])
+                i += 1
+                continue
+            # remove the middle, creating a new segment
+            # left is s_start:o_start, right is o_end:s_end
+            left_len = o_start - s_start
+            left = s_data[:left_len]
+            right_len = s_end - o_end
+            right = s_data[-right_len:]
+            self.spans[i] = (s_start, left)
+            self.spans.insert(i+1, (o_end, right))
+            break
+        #print " done", self.spans
 
     def pop(self, start, length):
         data = self.get(start, length)
         if data:
             self.remove(start, length)
         return data
-
-    def get_what(self, wanted):
-        pass

commit 6285e17234984461c681c9098fe8215e909e35c2
Author: Brian Warner <warner@lothar.com>
Date:   Sat Feb 6 17:01:54 2010 -0800

    more work
---
 src/allmydata/immutable/download2.py      |  432 ++++++++++++++++++-----------
 src/allmydata/immutable/download2_off.py  |   13 -
 src/allmydata/immutable/download2_util.py |   29 ++-
 3 files changed, 301 insertions(+), 173 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index e2b1c01..5d2ceb7 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -4,7 +4,7 @@ from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 
 import weakref
-class Inhibitor(service.Service):
+class Terminator(service.Service):
     def __init__(self):
         service.Service.__init__(self)
         self._clients = weakref.WeakKeyDictionary()
@@ -45,7 +45,7 @@ class Share:
         self._requested = Spans() # we've sent a request for this
         self._received = Spans() # we've received a response for this
         self._received_data = DataSpans() # the response contents, still unused
-        self._requested_blocks = [] # (segnum, [observer2s])
+        self._requested_blocks = [] # (segnum, set(observer2..))
         ver = rref.version["http://allmydata.org/tahoe/protocols/storage/v1"]
         self._overrun_ok = ver["tolerates-immutable-read-overrun"]
         # If _overrun_ok and we guess the offsets correctly, we can get
@@ -73,6 +73,40 @@ class Share:
         self._fieldsize = 4
         self._fieldstruct = ">L"
 
+    # called by our client, the SegmentFetcher
+    def get_block(self, segnum):
+        """Add a block number to the list of requests. This will eventually
+        result in a fetch of the data necessary to validate the block, then
+        the block itself. The fetch order is generally
+        first-come-first-served, but requests may be answered out-of-order if
+        data becomes available sooner.
+
+        I return an Observer2, which has two uses. The first is to call
+        o.subscribe(), which gives me a place to send state changes and
+        eventually the data block. The second is o.cancel(), which removes
+        the request (if it is still active).
+        """
+        o = Observer2()
+        o.set_canceler(self._cancel_block_request)
+        for i,(segnum0,observers) in enumerate(self._requested_blocks):
+            if segnum0 == segnum:
+                observers.add(o)
+                break
+        else:
+            self._requested_blocks.append(segnum, set([o]))
+        eventually(self.loop)
+        return o
+
+    def _cancel_block_request(self, o):
+        new_requests = []
+        for e in self._requested_blocks:
+            (segnum0, observers) = e
+            observers.discard(o)
+            if observers:
+                new_requests.append(e)
+        self._requested_blocks = new_requests
+
+    # internal methods
     def _active_segnum(self):
         if self._requested_blocks:
             return self._requested_blocks[0]
@@ -86,6 +120,8 @@ class Share:
         return None, []
 
     def loop(self):
+        # TODO: if any exceptions occur here, kill the download
+
         # we are (eventually) called after all state transitions:
         #  new segments added to self._requested_blocks
         #  new data received from servers (responses to our read() calls)
@@ -190,15 +226,18 @@ class Share:
         if not UEB_s:
             return False
         rdata.remove(o["uri_extension"], fsize)
-        ok = self._node.validate_UEB(UEB_s) # stores in self._node.UEB # XXX
-        if not ok:
+        try:
+            self._node.validate_UEB(UEB_s) # stores in self._node.UEB # XXX
+            self.actual_segment_size = self._node.segment_size
+            assert self.actual_segment_size is not None
+            return True
+        except hashtree.BadHashError:
+            # TODO: if this UEB was bad, we'll keep trying to validate it
+            # over and over again. Only log.err on the first one, or better
+            # yet skip all but the first
+            f = Failure()
+            self._signal_corruption(f, o["uri_extension"], fsize+UEB_length)
             return False
-        # TODO: if this UEB was bad, we'll keep trying to
-        # validate it over and over again. Only log.err on
-        # the first one, or better yet skip all but the first
-        self.actual_segment_size = self._node.segment_size
-        assert self.actual_segment_size is not None
-        return True
 
     def _satisfy_share_hash_tree(self):
         # the share hash chain is stored as (hashnum,hash) tuples, so you
@@ -217,12 +256,20 @@ class Share:
             hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
             hashvalue = hashdata[i+2:i+2+HASH_SIZE]
             share_hashes[hashnum] = hashvalue
-        ok = self._node.process_share_hashes(share_hashes)
-        # adds to self._node.share_hash_tree
-        if ok:
+        try:
+            self._node.process_share_hashes(share_hashes)
+            # adds to self._node.share_hash_tree
             rdata.remove(o["share_hashes"], hashlen)
             return True
-        return False
+        except IndexError, hashtree.BadHashError, hashtree.NotEnoughHashesError:
+            f = Failure()
+            self._signal_corruption(f, o["share_hashes"], hashlen)
+            return False
+
+    def _signal_corruption(self, f, start, offset):
+        # there was corruption somewhere in the given range
+        print f # XXX
+        pass
 
     def _satisfy_block_hash_tree(self, needed_hashes):
         o = self.actual_offsets
@@ -385,18 +432,6 @@ class Share:
         ...
 
 
-    # called by our client, the SegmentFetcher
-    def get_block(self, segnum):
-        o = Observer2()
-        for i,(segnum0,observers) in enumerate(self._requested_blocks):
-            if segnum0 == segnum:
-                observers.append(o)
-                break
-        else:
-            self._requested_blocks.append(segnum, [o])
-        eventually(self.loop)
-        return o
-
 class CommonShare:
     """I hold data that is common across all instances of a single share,
     like sh2 on both servers A and B. This is just the block hash tree.
@@ -429,145 +464,181 @@ class CommonShare:
 # download is complete.
 
 class SegmentFetcher:
-    """I am responsible for acquiring blocks for a single segment."""
-    def __init__(self, parent, segnum):
-        self.parent = parent # CiphertextFileNode
+    """I am responsible for acquiring blocks for a single segment. I will use
+    the Share instances passed to my add_shares() method to locate, retrieve,
+    and validate those blocks. I expect my parent node to call my
+    no_more_shares() method when there are no more shares available. I will
+    call my parent's want_more_shares() method when I want more: I expect to
+    see at least one call to add_shares or no_more_shares afterwards.
+
+    When I have enough validated blocks, I will call my parent's
+    process_blocks() method with a dictionary that maps shnum to blockdata.
+    If I am unable to provide enough blocks, I will call my parent's
+    fetch_failed() method with (self, f). After either of these events, I
+    will shut down and do no further work. My parent can also call my stop()
+    method to have me shut down early."""
+
+    def __init__(self, node, segnum, k):
+        self._node = node # CiphertextFileNode
         self.segnum = segnum
-        self.shares = {} # maps non-dead Share instance to a state, one of
-                         # (UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT).
-                         # State transition map is:
-                         #  UNUSED -(send-read)-> PENDING
-                         #  PENDING -(timer)-> OVERDUE
-                         #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD
-                         #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD
-                         # If a share becomes DEAD, it is removed from the
-                         # dict. Shares can also signal BADSEGNUM.
-        self.shnums = DictOfSets() # maps shnum to the shares that provide it
-        self.blocks = {} # maps shnum to validated block data
+        self._k = k
+        self._shares = {} # maps non-dead Share instance to a state, one of
+                          # (UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT).
+                          # State transition map is:
+                          #  UNUSED -(send-read)-> PENDING
+                          #  PENDING -(timer)-> OVERDUE
+                          #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
+                          #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD, BADSEGNUM
+                          # If a share becomes DEAD, it is removed from the
+                          # dict. If it becomes BADSEGNUM, the whole fetch is
+                          # terminated.
+        self._share_observers = {} # maps Share to Observer2 for active ones
+        self._shnums = DictOfSets() # maps shnum to the shares that provide it
+        self._blocks = {} # maps shnum to validated block data
         self._no_more_shares = False
         self._bad_segnum = False
-        self.running = True
+        self._running = True
 
     def stop(self):
         self._cancel_all_requests()
-        self.running = False
+        self._running = False
+        del self._shares # let GC work # ???
+
+
+    # called by our parent CiphertextFileNode
 
     def add_shares(self, shares):
         # called when ShareFinder locates a new share, and when a non-initial
         # segment fetch is started and we already know about shares from the
         # previous segment
         for s in shares:
-            self.shares[s] = UNUSED
-            self.shnums[s.shnum].add(s)
-        eventually(self.loop)
+            self._shares[s] = UNUSED
+            self._shnums[s.shnum].add(s)
+        eventually(self._loop)
+
+    def no_more_shares(self):
+        # ShareFinder tells us it's reached the end of its list
+        self._no_more_shares = True
+
+    # internal methods
 
-    def count_shnums(self, *states):
+    def _count_shnums(self, *states):
         """shnums for which at least one state is in the following list"""
         shnums = []
-        for shnum,shares in self.shnums.iteritems():
+        for shnum,shares in self._shnums.iteritems():
             matches = [s for s in shares if s.state in states]
             if matches:
                 shnums.append(shnum)
         return len(shnums)
 
-    def no_more_shares(self):
-        # ShareFinder tells us it's reached the end of its list
-        self._no_more_shares = True
-
-    def loop(self):
-        if not self.running:
+    def _loop(self):
+        if not self._running:
             return
         if self._bad_segnum:
             # oops, we were asking for a segment number beyond the end of the
             # file. This is an error.
+            self.stop()
             e = BadSegmentNumberError("%d > %d" % (self.segnum,
-                                                   self.parent.num_segments))
+                                                   self._node.num_segments))
             f = Failure(e)
-            self.parent.fetch_failed(self, f)
-            self.stop()
+            self._node.fetch_failed(self, f)
+            return
+
         # are we done?
-        if self.count_shnums(COMPLETE) >= self.k:
+        if self._count_shnums(COMPLETE) >= self._k:
             # yay!
-            self.parent.process_blocks(self.segnum, self.blocks)
-            return self.stop()
+            self.stop()
+            self._node.process_blocks(self.segnum, self._blocks)
+            return
 
         # we may have exhausted everything
         if (self._no_more_shares and
-            self.count_shnums(UNUSED, PENDING, OVERDUE, COMPLETE) < k):
+            self._count_shnums(UNUSED, PENDING, OVERDUE, COMPLETE) < self._k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
+            self.stop()
             e = NotEnoughShares("...") # XXX
             f = Failure(e)
-            self.parent.fetch_failed(self, f)
-            return self.stop()
+            self._node.fetch_failed(self, f)
+            return
 
         # nope, not done. Are we "block-hungry" (i.e. do we want to send out
         # more read requests, or do we think we have enough in flight
-        # already)
-        while self.count_shnums(PENDING, COMPLETE) < k:
+        # already?)
+        while self._count_shnums(PENDING, COMPLETE) < self._k:
             # we're hungry.. are there any unused shares?
-            progress = self._send_some_requests()
-            if not progress:
+            sent = self._send_new_request()
+            if not sent:
                 break
 
         # ok, now are we "share-hungry" (i.e. do we have enough known shares
         # to make us happy, or should we ask the ShareFinder to get us more?)
-        if self.count_shnums(UNUSED, PENDING, COMPLETE) < k:
+        if self._count_shnums(UNUSED, PENDING, COMPLETE) < self._k:
             # we're hungry for more shares
-            self.parent.want_more_shares()
+            self._node.want_more_shares()
             # that will trigger the ShareFinder to keep looking
 
-    def _send_some_requests(self):
-        for shnum,shares in self.shnums.iteritems():
-            states = [s.state for s in shares]
+    def _send_new_request(self):
+        for shnum,shares in self._shnums.iteritems():
+            states = [self._shares[s] for s in shares]
             if COMPLETE in states or PENDING in states:
+                # don't send redundant requests
                 continue
-            if UNUSED in states:
-                # here's a candidate. Send a request.
-                s = find_one(shares, UNUSED)
-                self.shares[s] = PENDING
-                o = s.get_block(segnum)
-                o.subscribe(self._block_request_activity, share=s, shnum=shnum)
-                # TODO: build up a list of candidates, then walk through
-                # the list, sending requests to the most desireable
-                # servers, re-checking our block-hunger each time. For
-                # non-initial segment fetches, this would let us stick
-                # with faster servers.
-                return True # we made progress!
-        return False # no progress made
+            if UNUSED not in states:
+                # no candidates for this shnum, move on
+                continue
+            # here's a candidate. Send a request.
+            s = find_one(shares, UNUSED) # XXX could choose fastest
+            self._shares[s] = PENDING
+            self._share_observers[s] = o = s.get_block(segnum)
+            o.subscribe(self._block_request_activity, share=s, shnum=shnum)
+            # TODO: build up a list of candidates, then walk through the
+            # list, sending requests to the most desireable servers,
+            # re-checking our block-hunger each time. For non-initial segment
+            # fetches, this would let us stick with faster servers.
+            return True
+        # nothing was sent: don't call us again until you have more shares to
+        # work with, or one of the existing shares has been declared OVERDUE
+        return False
 
     def _cancel_all_requests(self):
-        for shnum,shares in self.shnums.iteritems():
-            for s in shares:
-                if s.state == PENDING:
-                    s.SOMETHING.cancel(SOMETHING) # XXX
+        for o in self._share_observers.values():
+            o.cancel()
+        self._share_observers = {}
 
     def _block_request_activity(self, share, shnum, state, block=None):
-        # called by Shares, in response to our s.send_request() calls
+        # called by Shares, in response to our s.send_request() calls.
+        # COMPLETE, CORRUPT, DEAD, BADSEGNUM are terminal.
+        if state in (COMPLETE, CORRUPT, DEAD, BADSEGNUM):
+            del self._share_observers[share]
         if state is COMPLETE:
             # 'block' is fully validated
-            self.shares[share] = COMPLETE
-            self.blocks[shnum] = block
+            self._shares[share] = COMPLETE
+            self._blocks[shnum] = block
         elif state is OVERDUE:
-            self.shares[share] = OVERDUE
+            self._shares[share] = OVERDUE
             # OVERDUE is not terminal: it will eventually transition to
             # COMPLETE, CORRUPT, or DEAD.
         elif state is CORRUPT:
-            self.shares[share] = CORRUPT
+            self._shares[share] = CORRUPT
         elif state is DEAD:
-            del self.shares[share]
-            self.shnums[shnum].remove(share)
+            del self._shares[share]
+            self._shnums[shnum].remove(share)
         elif state is BADSEGNUM:
-            self.shares[share] = BADSEGNUM # ???
+            self._shares[share] = BADSEGNUM # ???
             self._bad_segnum = True
-        eventually(self.loop)
+        eventually(self._loop)
 
 
-class ShareFinder(service.MultiService):
+class Token:
+    pass
+
+class ShareFinder:
     def __init__(self, storage_broker, storage_index,
                  share_consumer, max_outstanding_requests=10):
-        service.MultiService.__init__(self)
-        self._servers = storage_broker.get_servers_for_index(storage_index)
+        self.running = True
+        s = storage_broker.get_servers_for_index(storage_index)
+        self._servers = iter(s)
         self.share_consumer = share_consumer
         self.max_outstanding = max_outstanding_requests
 
@@ -582,6 +653,11 @@ class ShareFinder(service.MultiService):
         d.addCallback(self._got_numsegs)
         d.addErrback(log.err, ...) # XXX
 
+    def log(self, **kwargs):
+
+    def stop(self):
+        self.running = False
+
     def _got_numsegs(self, numsegs):
         for cs in self._commonshares.values():
             cs.got_numsegs(numsegs)
@@ -594,6 +670,8 @@ class ShareFinder(service.MultiService):
 
     # internal methods
     def loop(self):
+        if not self.running:
+            return
         if not self._hungry:
             return
         if self.undelivered_shares:
@@ -613,30 +691,58 @@ class ShareFinder(service.MultiService):
         except StopIteration:
             self._servers = None
 
-        if not server and not self.pending_requests:
-            # we've run out of servers (so we can't send any more requests),
-            # and we have nothing in flight. No further progress can be made.
-            # They are destined to remain hungry.
-            self.share_consumer.no_more_shares()
-            self.stopService()
-            # TODO: stop running
+        if server:
+            self.send_request(server)
             return
 
-        self.pending_requests.add(self.send_request(server))
-        return
+        if self.pending_requests:
+            # no server, but there are still requests in flight: maybe one of
+            # them will make progress
+            return
 
-    def send_request(self, server):
-        ...
+        # we've run out of servers (so we can't send any more requests), and
+        # we have nothing in flight. No further progress can be made. They
+        # are destined to remain hungry.
+        self.share_consumer.no_more_shares()
+        self.stop()
 
-    def got_response(self, response, req, server):
-        self.pending_requests.discard(req)
-        if good:
-            rref = response # the BucketReader
+
+    def send_request(self, server):
+        req = Token()
+        self.pending_requests.add(req)
+        peerid, rref = server
+        lp = self.log(format="sending DYHB to [%(peerid)s]",
+                      peerid=idlib.shortnodeid_b2a(peerid),
+                      level=log.NOISY, umid="Io7pyg")
+        d = rref.callRemote("get_buckets", self._storage_index)
+        d.addBoth(incidentally, self.pending_requests.discard, req)
+        d.addCallbacks(self._got_response, self._got_error,
+                       callbackArgs=(peerid, req, lp))
+        d.addErrback(log.err, format="error in send_request",
+                     level=log.WEIRD, parent=lp, umid="rpdV0w")
+        d.addCallback(incidentally, eventually, self.loop)
+
+    def _got_response(self, buckets, peerid, req, lp):
+        if buckets:
+            shnums_s = ",".join([str(shnum) for shnum in buckets])
+            self.log(format="got shnums [%s] from [%(peerid)s]" % shnums_s,
+                     peerid=idlib.shortnodeid_b2a(peerid),
+                     level=log.NOISY, parent=lp, umid="0fcEZw")
+        else:
+            self.log(format="no shares from [%(peerid)s]",
+                     peerid=idlib.shortnodeid_b2a(peerid),
+                     level=log.NOISY, parent=lp, umid="U7d4JA")
+        for shnum, bucket in buckets.iteritems():
             if shnum not in self._commonshares:
                 self._commonshares[shnum] = CommonShare(self._num_segments)
-            s = Share(rref, verifycap, self._commonshares[shnum], node)
+            cs = self._commonshares[shnum]
+            s = Share(bucket, self.verifycap, cs, self.node)
             self.undelivered_shares.append(s)
-            eventually(self.loop)
+
+    def _got_error(self, f, peerid, req):
+        self.log(format="got error from [%(peerid)s]",
+                 peerid=idlib.shortnodeid_b2a(peerid), failure=f,
+                 level=log.UNUSUAL, parent=lp, umid="zUKdCw")
 
 
 
@@ -756,24 +862,25 @@ class Segmentation:
         eventually(self._maybe_fetch_next)
 
 class Cancel:
-    def __init__(self, parent):
-        self._parent = parent
-        self._cancelled = False
+    def __init__(self, f):
+        self._f = f
+        self.cancelled = False
     def cancel(self):
-        self._cancelled = True
-        self._parent._cancel(self)
+        if not self.cancelled:
+            self.cancelled = True
+            self._f(self)
 
 class CiphertextFileNode:
     # Share._node points to me
     def __init__(self, verifycap, storage_broker, secret_holder,
-                 inhibitor, history):
+                 terminator, history):
         assert isinstance(verifycap, CHKFileVerifierURI)
         self.u = verifycap
         storage_index = verifycap.storage_index
         self._needed_shares = verifycap.needed_shares
         self._total_shares = verifycap.total_shares
         self.running = True
-        inhibitor.register(self) # calls self.stop() at stopService()
+        terminator.register(self) # calls self.stop() at stopService()
         # the rule is: only send network requests if you're active
         # (self.running is True). You can do eventual-sends any time. This
         # rule should mean that once stopService()+flushEventualQueue()
@@ -813,10 +920,11 @@ class CiphertextFileNode:
         self._shares = set()
 
     def stop(self):
+        # called by the Terminator at shutdown, mostly for tests
         if self._active_segment:
             self._active_segment.stop()
+            self._active_segment = None
         self._sharefinder.stop()
-        ...
 
     # things called by our client, either a filenode user or an
     # ImmutableFileNode wrapper
@@ -857,7 +965,7 @@ class CiphertextFileNode:
         segment size, and still know which data you received.
         """
         d = defer.Deferred()
-        c = Cancel(self)
+        c = Cancel(self._cancel_request)
         self._segment_requests.append( (segnum, d, c) )
         self._start_new_segment()
         eventually(self._loop)
@@ -878,7 +986,8 @@ class CiphertextFileNode:
     def _start_new_segment(self):
         if self._active_segment is None and self._segment_requests:
             segnum = self._segment_requests[0][0]
-            self._active_segment = fetcher = SegmentFetcher(self, segnum)
+            self._active_segment = fetcher = SegmentFetcher(self, segnum,
+                                                            self._needed_shares)
             active_shares = [s for s in self._shares if s.not_dead()]
             fetcher.add_shares(active_shares) # this triggers the loop
 
@@ -897,18 +1006,15 @@ class CiphertextFileNode:
 
     def validate_UEB(self, UEB_s):
         h = hashutil.uri_extension_hash(UEB_s)
-        try:
-            if h != self._verifycap.uri_extension_hash:
-                ERR()
-                return False
-            d = uri.unpack_extension(data) # could raise parse error
-            self._parse_UEB(self, d) # sets self._stuff, raise error
-            self.have_UEB = True
-            self._segsize_observers.fire(self.segment_size)
-            self._numsegs_observers.fire(self.num_segments)
-            return True
-        except (errors..):
-            return False
+        if h != self._verifycap.uri_extension_hash:
+            raise hashutil.BadHashError
+        UEB_dict = uri.unpack_extension(data)
+        self._parse_UEB(self, UEB_dict) # sets self._stuff
+        # TODO: a malformed (but authentic) UEB could throw an assertion in
+        # _parse_UEB, and we should abandon the download.
+        self.have_UEB = True
+        self._segsize_observers.fire(self.segment_size)
+        self._numsegs_observers.fire(self.num_segments)
 
 
     def _parse_UEB(self, d):
@@ -965,12 +1071,7 @@ class CiphertextFileNode:
 
 
     def process_share_hashes(self, share_hashes):
-        try:
-            self.share_hash_tree.set_hashes(share_hashes)
-        except IndexError, hashtree.BadHashError, hashtree.NotEnoughHashesError:
-            STUFF # XXX
-            return False
-        return True
+        self.share_hash_tree.set_hashes(share_hashes)
 
     # called by our child SegmentFetcher
 
@@ -981,7 +1082,15 @@ class CiphertextFileNode:
         assert sf is self._active_segment
         sf.disownServiceParent()
         self._active_segment = None
-        ... # deliver error upwards
+        # deliver error upwards
+        for (d,c) in self._extract_requests(sf.segnum):
+            eventually(self._deliver_error, d, c, f)
+
+    def _deliver_error(self, d, c, f):
+        # this method exists to handle cancel() that occurs between
+        # _got_segment and _deliver_error
+        if not c.cancelled:
+            d.errback(f)
 
     def process_blocks(self, segnum, blocks):
         codec = self._codec
@@ -1007,32 +1116,37 @@ class CiphertextFileNode:
         except SOMETHING:
             SOMETHING
         assert self._active_segment.segnum == segnum
-        retire = [(d,c) for (segnum0, d, c) in self._segment_requests
-                  if segnum0 == segnum]
-        self._segment_requests = [t for t in self._segment_requests
-                                  if t[0] != segnum]
         assert self.segment_size is not None
         offset = segnum * self.segment_size
-        for (d,c) in retire:
+        for (d,c) in self._extract_requests(segnum):
             eventually(self._deliver, d, c, offset, segment)
         self._active_segment = None
         self._start_new_segment()
 
-    def _cancel(self, c):
+    def _deliver(self, d, c, offset, segment):
+        # this method exists to handle cancel() that occurs between
+        # _got_segment and _deliver
+        if not c.cancelled:
+            d.callback((offset,segment))
+
+    def _extract_requests(self, segnum):
+        """Remove matching requests and return their (d,c) tuples so that the
+        caller can retire them."""
+        retire = [(d,c) for (segnum0, d, c) in self._segment_requests
+                  if segnum0 == segnum]
+        self._segment_requests = [t for t in self._segment_requests
+                                  if t[0] != segnum]
+        return retire
+
+    def _cancel_request(self, c):
         self._segment_requests = [t for t in self._segment_requests
                                   if t[2] != c]
-        segnums =  [segnum for (segnum,d,c) in self._segment_requests]
+        segnums = [segnum for (segnum,d,c) in self._segment_requests]
         if self._active_segment.segnum not in segnums:
             self._active_segment.stop()
             self._active_segment = None
             self._start_new_segment()
 
-    def _deliver(self, d, c, offset, segment):
-        # this method exists to handle cancel() that occurs between
-        # _got_segment and _deliver
-        if not c._cancelled:
-            d.callback((offset,segment))
-
 class DecryptingConsumer:
     """I sit between a CiphertextDownloader (which acts as a Producer) and
     the real Consumer, decrypting everything that passes by. The real
diff --git a/src/allmydata/immutable/download2_off.py b/src/allmydata/immutable/download2_off.py
index 3871ec5..d2b8b99 100755
--- a/src/allmydata/immutable/download2_off.py
+++ b/src/allmydata/immutable/download2_off.py
@@ -116,19 +116,6 @@ class Server:
         self._remote_buckets = r
         return set(r.keys())
 
-def incidentally(res, f, *args, **kwargs):
-    """Add me to a Deferred chain like this:
-     d.addBoth(incidentally, func, arg)
-    and I'll behave as if you'd added the following function:
-     def _(res):
-         func(arg)
-         return res
-    This is useful if you want to execute an expression when the Deferred
-    fires, but don't care about its value.
-    """
-    f(*args, **kwargs)
-    return res
-
 class ShareOnAServer:
     """I represent one instance of a share, known to live on a specific
     server. I am created every time a server responds affirmatively to a
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
index 9786662..3ddcd1d 100755
--- a/src/allmydata/immutable/download2_util.py
+++ b/src/allmydata/immutable/download2_util.py
@@ -1,4 +1,5 @@
-#! /usr/bin/python
+
+import weakref
 
 class Observer2:
     """A simple class to distribute multiple events to a single subscriber.
@@ -6,6 +7,13 @@ class Observer2:
     def __init__(self):
         self._watcher = None
         self._undelivered_results = []
+        self._canceler = None
+
+    def set_canceler(self, f):
+        # we use a weakref to avoid creating a cycle between us and the thing
+        # we're observing: they'll be holding a reference to us to compare
+        # against the value we pass to their canceler function.
+        self._canceler = weakref(f)
 
     def subscribe(self, observer, **watcher_kwargs):
         self._watcher = (observer, watcher_kwargs)
@@ -24,6 +32,11 @@ class Observer2:
         kwargs.update(watcher_kwargs)
         eventually(o, **kwargs)
 
+    def cancel(self):
+        f = self._canceler()
+        if f:
+            f(self)
+
 class DictOfSets:
     def add(self, key, value): pass
     def values(self): # return set that merges all value sets
@@ -31,3 +44,17 @@ class DictOfSets:
         for key in self:
             r.update(self[key])
         return r
+
+
+def incidentally(res, f, *args, **kwargs):
+    """Add me to a Deferred chain like this:
+     d.addBoth(incidentally, func, arg)
+    and I'll behave as if you'd added the following function:
+     def _(res):
+         func(arg)
+         return res
+    This is useful if you want to execute an expression when the Deferred
+    fires, but don't care about its value.
+    """
+    f(*args, **kwargs)
+    return res

commit ab766fc9d63ed7b6296b2423b3341ae6915bc08a
Author: Brian Warner <warner@lothar.com>
Date:   Wed Feb 3 10:25:13 2010 -0800

    more WIP, pass BadSegmentNumberError sensibly
---
 src/allmydata/immutable/download2.py |  183 +++++++++++++++++++++-------------
 1 files changed, 115 insertions(+), 68 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 4651e1c..e2b1c01 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -15,6 +15,12 @@ class Inhibitor(service.Service):
             c.stop()
         return service.Service.stopService(self)
 
+# TODO: just use strings
+(UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT, DEAD, BADSEGNUM) = range(7)
+
+class BadSegmentNumberError(Exception):
+    pass
+
 class Share:
     # this is a specific implementation of IShare for tahoe's native storage
     # servers. A different backend would use a different class.
@@ -90,12 +96,15 @@ class Share:
         while self._get_satisfaction():
             pass
 
-        # When we get no satisfaction, it's time to find out what we desire.
-        # The number of segments is finite, so I can't get no satisfaction
+        # When we get no satisfaction (from the data we've received so far),
+        # we determine what data we desire (to satisfy more requests). The
+        # number of segments is finite, so I can't get no satisfaction
         # forever.
         self._desire()
 
-        # finally send out requests for whatever we need
+        # finally send out requests for whatever we need (desire minus have).
+        # You can't always get what you want, but, sometimes, you get what
+        # you need.
         self._request_needed() # express desire
 
     def _get_satisfaction(self):
@@ -113,12 +122,18 @@ class Share:
                 # can't check any hashes without the UEB
                 return False
 
+        segnum, observers = self._active_segnum_and_observers()
+        if segnum >= self._node.UEB.num_segments:
+            for o in observers:
+                o.notify(state=BADSEGNUM)
+            self._requested_blocks.pop(0)
+            return True
+
         if self._node.share_hash_tree.needed_hashes(self.shnum):
             if not self._satisfy_share_hash_tree():
                 # can't check block_hash_tree without a root
                 return False
 
-        segnum, observers = self._active_segnum_and_observers()
         if segnum is None:
             return False # we don't want any particular segment right now
 
@@ -416,7 +431,7 @@ class CommonShare:
 class SegmentFetcher:
     """I am responsible for acquiring blocks for a single segment."""
     def __init__(self, parent, segnum):
-        self.parent = parent
+        self.parent = parent # CiphertextFileNode
         self.segnum = segnum
         self.shares = {} # maps non-dead Share instance to a state, one of
                          # (UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT).
@@ -426,13 +441,15 @@ class SegmentFetcher:
                          #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD
                          #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD
                          # If a share becomes DEAD, it is removed from the
-                         # dict.
+                         # dict. Shares can also signal BADSEGNUM.
         self.shnums = DictOfSets() # maps shnum to the shares that provide it
         self.blocks = {} # maps shnum to validated block data
         self._no_more_shares = False
+        self._bad_segnum = False
         self.running = True
 
     def stop(self):
+        self._cancel_all_requests()
         self.running = False
 
     def add_shares(self, shares):
@@ -460,27 +477,36 @@ class SegmentFetcher:
     def loop(self):
         if not self.running:
             return
+        if self._bad_segnum:
+            # oops, we were asking for a segment number beyond the end of the
+            # file. This is an error.
+            e = BadSegmentNumberError("%d > %d" % (self.segnum,
+                                                   self.parent.num_segments))
+            f = Failure(e)
+            self.parent.fetch_failed(self, f)
+            self.stop()
         # are we done?
         if self.count_shnums(COMPLETE) >= self.k:
             # yay!
             self.parent.process_blocks(self.segnum, self.blocks)
-            return stop_running()
+            return self.stop()
 
         # we may have exhausted everything
         if (self._no_more_shares and
             self.count_shnums(UNUSED, PENDING, OVERDUE, COMPLETE) < k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
-            f = ... # XXX
+            e = NotEnoughShares("...") # XXX
+            f = Failure(e)
             self.parent.fetch_failed(self, f)
-            return stop_running()
+            return self.stop()
 
         # nope, not done. Are we "block-hungry" (i.e. do we want to send out
         # more read requests, or do we think we have enough in flight
         # already)
         while self.count_shnums(PENDING, COMPLETE) < k:
             # we're hungry.. are there any unused shares?
-            progress = self.send_some_requests()
+            progress = self._send_some_requests()
             if not progress:
                 break
 
@@ -491,7 +517,7 @@ class SegmentFetcher:
             self.parent.want_more_shares()
             # that will trigger the ShareFinder to keep looking
 
-    def send_some_requests(self):
+    def _send_some_requests(self):
         for shnum,shares in self.shnums.iteritems():
             states = [s.state for s in shares]
             if COMPLETE in states or PENDING in states:
@@ -510,6 +536,12 @@ class SegmentFetcher:
                 return True # we made progress!
         return False # no progress made
 
+    def _cancel_all_requests(self):
+        for shnum,shares in self.shnums.iteritems():
+            for s in shares:
+                if s.state == PENDING:
+                    s.SOMETHING.cancel(SOMETHING) # XXX
+
     def _block_request_activity(self, share, shnum, state, block=None):
         # called by Shares, in response to our s.send_request() calls
         if state is COMPLETE:
@@ -525,6 +557,9 @@ class SegmentFetcher:
         elif state is DEAD:
             del self.shares[share]
             self.shnums[shnum].remove(share)
+        elif state is BADSEGNUM:
+            self.shares[share] = BADSEGNUM # ???
+            self._bad_segnum = True
         eventually(self.loop)
 
 
@@ -694,7 +729,7 @@ class Segmentation:
         self._maybe_fetch_next()
 
     def _retry_bad_segment(self, f, had_actual_segment_size):
-        f.trap(BadSegmentNumber): # guessed way wrong, off the end
+        f.trap(BadSegmentNumberError): # guessed way wrong, off the end
         if had_actual_segment_size:
             # but we should have known better, so this is a real error
             return f
@@ -765,35 +800,21 @@ class CiphertextFileNode:
         self.block_size = None
         self.share_size = None
         self.ciphertext_hash_tree = None # size depends on num_segments
-        self.ciphertext_hash = None # flat hash
+        self.ciphertext_hash = None # flat hash, optional
 
         # things to track callers that want data
         self._segsize_observers = OneShotObserverList()
         self._numsegs_observers = OneShotObserverList()
         # _segment_requests can have duplicates
         self._segment_requests = [] # (segnum, d, cancel_handle)
-        self._active_segnum = None
-        self._active_fetcher = None
+        self._active_segment = None # a SegmentFetcher, with .segnum
 
         self._sharefinder = ShareFinder(storage_broker, storage_index, self)
         self._shares = set()
 
-        self._numsegs_observers.when_fired(self._halt_bad_segnum_requests)
-
-    def _halt_bad_segnum_requests(self, ign):
-        bad = [(segnum,d) for (segnum,d,c) in self._segment_requests
-               if segnum >= self._num_segments]
-        self._segment_requests = [t for t in self._segment_requests
-                                  if t[0] < self._num_segments]
-        for (segnum,d) in bad:
-            e = BadSegmentNumber("%d > %d" % (segnum, self._num_segments))
-            eventually(d.errback(e))
-        if self._active_segnum >= self._num_segments:
-            ...
-
     def stop(self):
-        if self._active_fetcher:
-            self._active_fetcher.stop()
+        if self._active_segment:
+            self._active_segment.stop()
         self._sharefinder.stop()
         ...
 
@@ -802,8 +823,10 @@ class CiphertextFileNode:
 
     def read(self, consumer, offset=0, size=None):
         """I am the main entry point, from which FileNode.read() can get
-        data."""
-        # tolerate concurrent operations: each gets its own Reader
+        data. I feed the consumer with the desired range of ciphertext. I
+        return a Deferred that fires (with the consumer) when the read is
+        finished."""
+        # for concurrent operations: each gets its own Segmentation manager
         if size is None:
             size = self._size - offset
         s = Segmentation(self, offset, size, consumer)
@@ -818,18 +841,6 @@ class CiphertextFileNode:
         d = s.start()
         return d
 
-    # things called by the Segmentation object used to transform
-    # arbitrary-sized read() calls into quantized segment fetches
-
-    def get_segment_size(self):
-        """I return a Deferred that fires with the segment_size used by this
-        file."""
-        return self._segsize_observers.when_fired()
-    def get_num_segments(self):
-        """I return a Deferred that fires with the number of segments used by
-        this file."""
-        return self._numsegs_observers.when_fired()
-
     def get_segment(self, segnum):
         """Begin downloading a segment. I return a tuple (d, c): 'd' is a
         Deferred that fires with (offset,data) when the desired segment is
@@ -837,7 +848,9 @@ class CiphertextFileNode:
         disavow interest in the segment (after which 'd' will never fire).
 
         You probably need to know the segment size before calling this,
-        unless you want the first few bytes of the file.
+        unless you want the first few bytes of the file. If you ask for a
+        segment number which turns out to be too large, the Deferred will
+        errback with BadSegmentNumberError.
 
         The Deferred fires with the offset of the first byte of the data
         segment, so that you can call get_segment() before knowing the
@@ -850,23 +863,35 @@ class CiphertextFileNode:
         eventually(self._loop)
         return (d, c)
 
+    # things called by the Segmentation object used to transform
+    # arbitrary-sized read() calls into quantized segment fetches
+
+    def get_segment_size(self):
+        """I return a Deferred that fires with the segment_size used by this
+        file."""
+        return self._segsize_observers.when_fired()
+    def get_num_segments(self):
+        """I return a Deferred that fires with the number of segments used by
+        this file."""
+        return self._numsegs_observers.when_fired()
+
     def _start_new_segment(self):
-        if self._active_segnum is None and self._segment_requests:
-            self._active_segnum = self._segment_requests[0][0]
-            self._active_fetcher = sf = SegmentFetcher(self, segnum)
+        if self._active_segment is None and self._segment_requests:
+            segnum = self._segment_requests[0][0]
+            self._active_segment = fetcher = SegmentFetcher(self, segnum)
             active_shares = [s for s in self._shares if s.not_dead()]
-            sf.add_shares(active_shares) # this triggers the loop
+            fetcher.add_shares(active_shares) # this triggers the loop
 
 
     # called by our child ShareFinder
     def got_shares(self, shares):
         self._shares.update(shares)
-        if self._active_fetcher:
-            self._active_fetcher.add_shares(shares)
+        if self._active_segment
+            self._active_segment.add_shares(shares)
     def no_more_shares(self):
         self._no_more_shares = True
-        if self._active_fetcher:
-            self._active_fetcher.no_more_shares()
+        if self._active_segment:
+            self._active_segment.no_more_shares()
 
     # things called by our Share instances
 
@@ -890,8 +915,6 @@ class CiphertextFileNode:
         self.share_size = mathutil.div_ceil(self._verifycap.size,
                                             self._needed_shares)
 
-        # First, things that we really need to learn from the UEB:
-        # segment_size, crypttext_root_hash, and share_root_hash.
         self.segment_size = d['segment_size']
         for r in self._readers:
             r.set_segment_size(self.segment_size)
@@ -908,7 +931,8 @@ class CiphertextFileNode:
 
         # zfec.Decode() instantiation is fast, but still, let's use the same
         # codec for anything we can. 3-of-10 takes 15us on my laptop,
-        # 25-of-100 is 900us, 25-of-255 is 2.5ms, 3-of-255 is 97us
+        # 25-of-100 is 900us, 3-of-255 is 97us, 25-of-255 is 2.5ms,
+        # worst-case 254-of-255 is 9.3ms
         self._codec = codec.CRSDecoder()
         self._codec.set_params(self.segment_size,
                                self._needed_shares, self._total_shares)
@@ -924,8 +948,8 @@ class CiphertextFileNode:
 
         self.share_hash_tree.set_hashes({0: d['share_root_hash']})
 
-        # Next: things that are optional and not redundant: crypttext_hash.
-        # We only pull this from the first UEB that we see.
+        # crypttext_hash is optional. We only pull this from the first UEB
+        # that we see.
         if 'crypttext_hash' in d:
             if len(d["crypttext_hash"]) == hashutil.CRYPTO_VAL_SIZE:
                 self.ciphertext_hash = d['crypttext_hash']
@@ -935,8 +959,9 @@ class CiphertextFileNode:
                                                    hashutil.CRYPTO_VAL_SIZE),
                         umid="oZkGLA", level=log.WEIRD)
 
-        # we ignore all of the redundant fields when downloading. The
-        # Verifier uses a different code path which does not ignore them.
+        # Our job is a fast download, not verification, so we ignore any
+        # redundant fields. The Verifier uses a different code path which
+        # does not ignore them.
 
 
     def process_share_hashes(self, share_hashes):
@@ -953,9 +978,9 @@ class CiphertextFileNode:
         self._sharefinder.hungry()
 
     def fetch_failed(self, sf, f):
-        assert sf is self._active_fetcher
+        assert sf is self._active_segment
         sf.disownServiceParent()
-        self._active_fetcher = None
+        self._active_segment = None
         ... # deliver error upwards
 
     def process_blocks(self, segnum, blocks):
@@ -981,7 +1006,7 @@ class CiphertextFileNode:
             self.ciphertext_hash_tree.set_hashes(leaves={segnum, h})
         except SOMETHING:
             SOMETHING
-        assert self._active_segnum == segnum
+        assert self._active_segment.segnum == segnum
         retire = [(d,c) for (segnum0, d, c) in self._segment_requests
                   if segnum0 == segnum]
         self._segment_requests = [t for t in self._segment_requests
@@ -990,17 +1015,16 @@ class CiphertextFileNode:
         offset = segnum * self.segment_size
         for (d,c) in retire:
             eventually(self._deliver, d, c, offset, segment)
-        self._active_segnum = None
+        self._active_segment = None
         self._start_new_segment()
 
     def _cancel(self, c):
         self._segment_requests = [t for t in self._segment_requests
                                   if t[2] != c]
         segnums =  [segnum for (segnum,d,c) in self._segment_requests]
-        if self._active_segnum not in segnums:
-            self._active_fetcher.stop()
-            self._active_fetcher = None
-            self._active_segnum = None
+        if self._active_segment.segnum not in segnums:
+            self._active_segment.stop()
+            self._active_segment = None
             self._start_new_segment()
 
     def _deliver(self, d, c, offset, segment):
@@ -1071,3 +1095,26 @@ class ImmutableFileNode:
 # where shnums=[] means all shares, and the return value is a dict of
 # # shnum->ta (like with mutable files). The DYHB query should also fetch the
 # offset table, since everything else can be located once we have that.
+
+
+# ImmutableFileNode
+#    DecryptingConsumer
+#  CiphertextFileNode
+#    Segmentation
+#   ShareFinder
+#   SegmentFetcher[segnum] (one at a time)
+#   CommonShare[shnum]
+#   Share[shnum,server]
+
+# TODO: when we learn numsegs, any get_segment() calls for bad blocknumbers
+# should be failed with BadSegmentNumberError. But should this be the
+# responsibility of CiphertextFileNode, or SegmentFetcher? The knowledge will
+# first appear when a Share receives a valid UEB and calls
+# CiphertextFileNode.validate_UEB, then _parse_UEB. The SegmentFetcher is
+# expecting to hear from the Share, via the _block_request_activity observer.
+
+# make it the responsibility of the SegmentFetcher. Each Share that gets a
+# valid UEB will tell the SegmentFetcher BADSEGNUM (instead of COMPLETE or
+# CORRUPT). The SegmentFetcher it then responsible for shutting down, and
+# informing its parent (the CiphertextFileNode) of the BadSegmentNumberError,
+# which is then passed to the client of get_segment().

commit 20c1bd066a396ec7ce132205724bae12fc97869a
Author: Brian Warner <warner@lothar.com>
Date:   Wed Feb 3 08:45:26 2010 -0800

    more WIP
---
 src/allmydata/immutable/download2.py |  206 ++++++++++++++++++++++------------
 1 files changed, 133 insertions(+), 73 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index adf9675..4651e1c 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -28,8 +28,9 @@ class Share:
 
     def __init__(self, rref, verifycap, commonshare, node):
         self._rref = rref
-        self._guess_offsets(filesize, filecap_parms, my_default_segsize)
+        self._guess_offsets(verifycap, node.guessed_segment_size)
         self.actual_offsets = None
+        self.actual_segment_size = None
         self._UEB_length = None
         self._commonshare = commonshare # holds block_hash_tree
         self._node = node # holds share_hash_tree and UEB
@@ -48,7 +49,11 @@ class Share:
         # 2=offset table, 3=UEB_length and everything else (hashes, block),
         # 4=UEB.
 
-    def _guess_offsets(self, filesize, filecap_parms, my_default_segsize):
+    def _guess_offsets(self, verifycap, guessed_segment_size):
+        self.guessed_segment_size = guessed_segment_size
+        size = verifycap.size
+        k = verifycap.needed_shares
+        N = verifycap.total_shares
         offsets = {}
         for i,field in enumerate('data',
                                  'plaintext_hash_tree', # UNUSED
@@ -176,6 +181,8 @@ class Share:
         # TODO: if this UEB was bad, we'll keep trying to
         # validate it over and over again. Only log.err on
         # the first one, or better yet skip all but the first
+        self.actual_segment_size = self._node.segment_size
+        assert self.actual_segment_size is not None
         return True
 
     def _satisfy_share_hash_tree(self):
@@ -268,7 +275,7 @@ class Share:
             return # must wait for the offsets to arrive
 
         o = self.actual_offsets or self.guessed_offsets
-        segsize = self.actual_segsize or self.guessed_segsize
+        segsize = self.actual_segment_size or self.guessed_segment_size
         if self._node.UEB is None:
             self._desire_UEB(o)
 
@@ -464,7 +471,8 @@ class SegmentFetcher:
             self.count_shnums(UNUSED, PENDING, OVERDUE, COMPLETE) < k):
             # no more new shares are coming, and the remaining hopeful shares
             # aren't going to be enough. boo!
-            self.parent.fetch_failed(self)
+            f = ... # XXX
+            self.parent.fetch_failed(self, f)
             return stop_running()
 
         # nope, not done. Are we "block-hungry" (i.e. do we want to send out
@@ -607,7 +615,11 @@ class Segmentation:
     implements(IPushProducer)
     def __init__(self, node, offset, size, consumer):
         self._node = node
-        self._hungry = False
+        self._hungry = True
+        self._active_segnum = None
+        self._cancel_segment_request = None
+        # these are updated as we deliver data. At any given time, we still
+        # want to download file[offset:offset+size]
         self._offset = offset
         self._size = size
         self._consumer = consumer
@@ -615,72 +627,98 @@ class Segmentation:
     def start(self):
         self._alive = True
         self._deferred = defer.Deferred()
-        self._node.get_segment_size().addCallbacks(self._got_segment_size,
-                                                   self._got_segsize_error)
-        # the process doesn't actually start until we get the segment size
+        self._consumer.registerProducer(self) # XXX???
+        self._maybe_fetch_next()
         return self._deferred
 
-    def _got_segment_size(self, segment_size):
-        # now we really start
-        self._needed = self._compute_segnums(segment_size)
-        self._loop()
-
-    def _compute_segnums(self, segsize):
-        # now that we know the file's segsize, what segments (and which
-        # ranges of each) will we need?
-        size = self._size
-        offset = self._offset
-        while size:
-            assert size >= 0
-            this_seg_num = int(offset / segsize)
-            this_seg_offset = offset - (seg_num*segsize)
-            this_seg_size = min(size, segsize-seg_offset)
-            size -= this_seg_size
-            if size:
-                offset += this_seg_size
-            yield (this_seg_num, this_seg_offset, this_seg_size)
-
-    def _loop(self):
-        try:
-            (segnum, segoff, segsize) = self._needed.next()
-        except StopIteration:
+    def _maybe_fetch_next(self):
+        if not self._alive or not self._hungry:
+            return
+        if self._active_segnum is not None:
+            return
+        self._fetch_next()
+
+    def _fetch_next(self):
+        if self._size == 0:
             # done!
             self._alive = False
             self._hungry = False
             self._consumer.unregisterProducer()
             self._deferred.callback(self._consumer)
             return
-        (d,self._cancel) = self._node.get_segment(segnum)
-        d.addCallback(self._got_segment, segoff, segsize)
-        d.addErrback(self._got_segment_error)
-
-    def _got_segment(self, segment, segoff, segsize):
-        assert segoff+segsize < len(segment)
-        if segoff != 0 or segsize != len(segment):
-            segment = segment[segoff:segoff+segsize]
-        self._consumer.write(segment)
-        eventually(self._loop)
+        n = self._node
+        have_actual_segment_size = n.actual_segment_size is not None
+        segment_size = n.actual_segment_size or n.guessed_segment_size
+        if self._offset == 0:
+            # great! we want segment0 for sure
+            wanted_segnum = 0
+        else:
+            # this might be a guess
+            wanted_segnum = self._offset // segment_size
+        self._active_segnum = wanted_segnum
+        d,c = self._node.get_segment(wanted_segnum)
+        self._cancel_segment_request = c
+        d.addBoth(self._request_retired)
+        d.addCallback(self._got_segment, have_actual_segment_size)
+        d.addErrback(self._retry_bad_segment, have_actual_segment_size)
+        d.addErrback(self._error)
+
+    def _request_retired(self, res):
+        self._active_segnum = None
+        self._cancel_segment_request = None
+        return res
 
-    def _got_segment_error(self, f):
-        self.error(f)
+    def _got_segment(self, (segment_start,segment), had_actual_segment_size):
+        self._active_segnum = None
+        self._cancel_segment_request = None
+        # we got file[segment_start:segment_start+len(segment)]
+        # we want file[self._offset:self._offset+self._size]
+        o = overlap(segment_start, len(segment),  self._offset, self._size)
+        # the overlap is file[o[0]:o[0]+o[1]]
+        if not o or o[0] != self._offset:
+            # we didn't get the first byte, so we can't use this segment
+            if have_actual_segment_size:
+                # and we should have gotten it right. This is big problem.
+                raise SOMETHING
+            # we've wasted some bandwidth, but now we can grab the right one,
+            # because we should know the segsize by now.
+            assert self._node.actual_segment_size is not None
+            self._maybe_fetch_next()
+            return
+        offset_in_segment = self._offset - segment_start
+        desired_data = segment[offset_in_segment:offset_in_segment+o[1]]
+
+        self._offset += len(desired_data)
+        self._size -= len(desired_data)
+        self._consumer.write(desired_data)
+        self._maybe_fetch_next()
+
+    def _retry_bad_segment(self, f, had_actual_segment_size):
+        f.trap(BadSegmentNumber): # guessed way wrong, off the end
+        if had_actual_segment_size:
+            # but we should have known better, so this is a real error
+            return f
+        # we didn't know better: try again with more information
+        return self._maybe_fetch_next()
+
+    def _error(self, f):
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
 
     def stopProducing(self):
         self._hungry = False
         self._alive = False
         # cancel any outstanding segment request
-        if self._cancel:
-            self._cancel()
-            self._cancel = None
+        if self._cancel_segment_request:
+            self._cancel_segment_request()
+            self._cancel_segment_request = None
     def pauseProducing(self):
         self._hungry = False
     def resumeProducing(self):
         self._hungry = True
-
-    def error(self, f):
-        self._alive = False
-        self._hungry = False
-        self._consumer.unregisterProducer()
-        self._deferred.errback(f)
+        eventually(self._maybe_fetch_next)
 
 class Cancel:
     def __init__(self, parent):
@@ -710,6 +748,14 @@ class CiphertextFileNode:
 
         self.share_hash_tree = IncompleteHashTree(self.u.total_shares)
 
+        # we guess the segment size, so Segmentation can pull non-initial
+        # segments in a single roundtrip
+        k = verifycap.needed_shares
+        max_segment_size = 128*KiB # TODO: pull from elsewhere, maybe the
+                                   # same place as upload.BaseUploadable
+        s = mathutil.next_multiple(min(verifycap.size, max_segment_size), k)
+        self.guessed_segment_size = s
+
         # filled in when we parse a valid UEB
         self.have_UEB = False
         self.num_segments = None
@@ -732,6 +778,19 @@ class CiphertextFileNode:
         self._sharefinder = ShareFinder(storage_broker, storage_index, self)
         self._shares = set()
 
+        self._numsegs_observers.when_fired(self._halt_bad_segnum_requests)
+
+    def _halt_bad_segnum_requests(self, ign):
+        bad = [(segnum,d) for (segnum,d,c) in self._segment_requests
+               if segnum >= self._num_segments]
+        self._segment_requests = [t for t in self._segment_requests
+                                  if t[0] < self._num_segments]
+        for (segnum,d) in bad:
+            e = BadSegmentNumber("%d > %d" % (segnum, self._num_segments))
+            eventually(d.errback(e))
+        if self._active_segnum >= self._num_segments:
+            ...
+
     def stop(self):
         if self._active_fetcher:
             self._active_fetcher.stop()
@@ -773,12 +832,16 @@ class CiphertextFileNode:
 
     def get_segment(self, segnum):
         """Begin downloading a segment. I return a tuple (d, c): 'd' is a
-        Deferred that fires with the data when the desired segment is
+        Deferred that fires with (offset,data) when the desired segment is
         available, and c is an object on which c.cancel() can be called to
         disavow interest in the segment (after which 'd' will never fire).
 
         You probably need to know the segment size before calling this,
         unless you want the first few bytes of the file.
+
+        The Deferred fires with the offset of the first byte of the data
+        segment, so that you can call get_segment() before knowing the
+        segment size, and still know which data you received.
         """
         d = defer.Deferred()
         c = Cancel(self)
@@ -805,22 +868,6 @@ class CiphertextFileNode:
         if self._active_fetcher:
             self._active_fetcher.no_more_shares()
 
-    # called by our child SegmentFetcher
-    def got_blocks(self, sf, blocks):
-        assert sf is self._active_fetcher
-        sf.disownServiceParent()
-        self._active_fetcher = None
-        ... # decode, deliver
-
-    def fetch_failed(self, sf):
-        assert sf is self._active_fetcher
-        sf.disownServiceParent()
-        self._active_fetcher = None
-        ... # deliver error upwards
-
-    def want_more_shares(self):
-        self._sharefinder.hungry()
-
     # things called by our Share instances
 
     def validate_UEB(self, UEB_s):
@@ -900,6 +947,17 @@ class CiphertextFileNode:
             return False
         return True
 
+    # called by our child SegmentFetcher
+
+    def want_more_shares(self):
+        self._sharefinder.hungry()
+
+    def fetch_failed(self, sf, f):
+        assert sf is self._active_fetcher
+        sf.disownServiceParent()
+        self._active_fetcher = None
+        ... # deliver error upwards
+
     def process_blocks(self, segnum, blocks):
         codec = self._codec
         if segnum == self.num_segments-1:
@@ -928,8 +986,10 @@ class CiphertextFileNode:
                   if segnum0 == segnum]
         self._segment_requests = [t for t in self._segment_requests
                                   if t[0] != segnum]
+        assert self.segment_size is not None
+        offset = segnum * self.segment_size
         for (d,c) in retire:
-            eventually(self._deliver, d, c, segment)
+            eventually(self._deliver, d, c, offset, segment)
         self._active_segnum = None
         self._start_new_segment()
 
@@ -943,11 +1003,11 @@ class CiphertextFileNode:
             self._active_segnum = None
             self._start_new_segment()
 
-    def _deliver(self, d, c, segment):
+    def _deliver(self, d, c, offset, segment):
         # this method exists to handle cancel() that occurs between
         # _got_segment and _deliver
         if not c._cancelled:
-            d.callback(segment)
+            d.callback((offset,segment))
 
 class DecryptingConsumer:
     """I sit between a CiphertextDownloader (which acts as a Producer) and

commit c651be349c44a0451cff24821abe3755ef4d4f80
Author: Brian Warner <warner@lothar.com>
Date:   Tue Feb 2 18:21:06 2010 -0800

    util.spans: move overlap/adjacent out to top-level functions
---
 src/allmydata/test/test_util.py |    4 +-
 src/allmydata/util/spans.py     |   48 +++++++++++++++++++-------------------
 2 files changed, 26 insertions(+), 26 deletions(-)

diff --git a/src/allmydata/test/test_util.py b/src/allmydata/test/test_util.py
index 5d59e5b..caf6c36 100644
--- a/src/allmydata/test/test_util.py
+++ b/src/allmydata/test/test_util.py
@@ -14,7 +14,7 @@ from allmydata.util import assertutil, fileutil, deferredutil, abbreviate
 from allmydata.util import limiter, time_format, pollmixin, cachedir
 from allmydata.util import statistics, dictutil, pipeline
 from allmydata.util import log as tahoe_log
-from allmydata.util.spans import Spans
+from allmydata.util.spans import Spans, overlap, adjacent
 
 class Base32(unittest.TestCase):
     def test_b2a_matches_Pythons(self):
@@ -1793,7 +1793,7 @@ class ByteSpans_Overlap(unittest.TestCase):
         #print "---"
         #self._show(s1, "1")
         #self._show(s2, "2")
-        o = Spans()._overlap(a,b,c,d)
+        o = overlap(a,b,c,d)
         expected = s1.intersection(s2)
         if not expected:
             self.failUnlessEqual(o, None)
diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
index 41b2215..39ee55f 100755
--- a/src/allmydata/util/spans.py
+++ b/src/allmydata/util/spans.py
@@ -44,34 +44,15 @@ class Spans:
             print "BAD:", self.dump()
             raise
 
-    def _overlap(self, start0, length0, start1, length1):
-        # return start2,length2 of the overlapping region, or None
-        #  00      00   000   0000  00  00 000  00   00  00      00
-        #     11    11   11    11   111 11 11  1111 111 11    11
-        left = max(start0, start1)
-        right = min(start0+length0, start1+length1)
-        # if there is overlap, 'left' will be its start, and right-1 will
-        # be the end'
-        if left < right:
-            return (left, right-left)
-        return None
-
-    def _adjacent(self, start0, length0, start1, length1):
-        if (start0 < start1) and start0+length0 == start1:
-            return True
-        elif (start1 < start0) and start1+length1 == start0:
-            return True
-        return False
-
     def add(self, start, length):
         assert start >= 0
         assert length > 0
         #print " ADD [%d+%d -%d) to %s" % (start, length, start+length, self.dump())
         first_overlap = last_overlap = None
         for i,(s_start,s_length) in enumerate(self._spans):
-            #print "  (%d+%d)-> overlap=%s adjacent=%s" % (s_start,s_length, self._overlap(s_start, s_length, start, length), self._adjacent(s_start, s_length, start, length))
-            if (self._overlap(s_start, s_length, start, length)
-                or self._adjacent(s_start, s_length, start, length)):
+            #print "  (%d+%d)-> overlap=%s adjacent=%s" % (s_start,s_length, overlap(s_start, s_length, start, length), adjacent(s_start, s_length, start, length))
+            if (overlap(s_start, s_length, start, length)
+                or adjacent(s_start, s_length, start, length)):
                 last_overlap = i
                 if first_overlap is None:
                     first_overlap = i
@@ -107,7 +88,7 @@ class Spans:
         first_complete_overlap = last_complete_overlap = None
         for i,(s_start,s_length) in enumerate(self._spans):
             s_end = s_start + s_length
-            o = self._overlap(s_start, s_length, start, length)
+            o = overlap(s_start, s_length, start, length)
             if o:
                 o_start, o_length = o
                 o_end = o_start+o_length
@@ -202,13 +183,32 @@ class Spans:
 
     def __contains__(self, (start,length)):
         for span_start,span_length in self._spans:
-            o = self._overlap(start, length, span_start, span_length)
+            o = overlap(start, length, span_start, span_length)
             if o:
                 o_start,o_length = o
                 if o_start == start and o_length == length:
                     return True
         return False
 
+def overlap(start0, length0, start1, length1):
+    # return start2,length2 of the overlapping region, or None
+    #  00      00   000   0000  00  00 000  00   00  00      00
+    #     11    11   11    11   111 11 11  1111 111 11    11
+    left = max(start0, start1)
+    right = min(start0+length0, start1+length1)
+    # if there is overlap, 'left' will be its start, and right-1 will
+    # be the end'
+    if left < right:
+        return (left, right-left)
+    return None
+
+def adjacent(start0, length0, start1, length1):
+    if (start0 < start1) and start0+length0 == start1:
+        return True
+    elif (start1 < start0) and start1+length1 == start0:
+        return True
+    return False
+
 class SimpleDataSpans:
     """I represent portions of a large string. Equivalently, I can be said to
     maintain a large array of characters (with gaps of empty elements). I can

commit 55d9fa3ec085c0f26811ec1249cf01fe18563e2c
Author: Brian Warner <warner@lothar.com>
Date:   Tue Feb 2 08:32:57 2010 -0800

    more
---
 src/allmydata/immutable/download2.py |   96 ++++++++++++++++------------------
 1 files changed, 45 insertions(+), 51 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index dfc48b7..adf9675 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1,33 +1,30 @@
 
-# TODO: if server1 has all shares, and server2-10 have one each, make the
-# loop stall slightly before requesting all shares from the first server, to
-# give it a chance to learn about the other shares and get some diversity.
-# Or, don't bother, let the first block all come from one server, and take
-# comfort in the fact that we'll learn about the other servers by the time we
-# fetch the second block.
-
-# as a query gets later, we're more willing to duplicate work.
-
-# should change server read protocol to allow small shares to be fetched in a
-# single RTT. Instead of get_buckets-then-read, just use read(shnums, readv),
-# where shnums=[] means all shares, and the return value is a dict of
-# # shnum->ta (like with mutable files). The DYHB query should also fetch the
-# offset table, since everything else can be located once we have that.
-
+import binascii
 from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 
+import weakref
+class Inhibitor(service.Service):
+    def __init__(self):
+        service.Service.__init__(self)
+        self._clients = weakref.WeakKeyDictionary()
+    def register(self, c):
+        self._clients[c] = None
+    def stopService(self):
+        for c in self._clients:
+            c.stop()
+        return service.Service.stopService(self)
+
 class Share:
     # this is a specific implementation of IShare for tahoe's native storage
     # servers. A different backend would use a different class.
     """I represent a single instance of a single share (e.g. I reference the
     shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
-    I am associated with a Commonshare that remembers data that is held in
+    I am associated with a CommonShare that remembers data that is held in
     common among e.g. SI=abcde/shnum2 across all servers. I am also
     associated with a CiphertextFileNode for e.g. SI=abcde (all shares, all
     servers).
     """
-    implements(IShare)
 
     def __init__(self, rref, verifycap, commonshare, node):
         self._rref = rref
@@ -751,6 +748,14 @@ class CiphertextFileNode:
         if size is None:
             size = self._size - offset
         s = Segmentation(self, offset, size, consumer)
+        # this raises an interesting question: what segments to fetch? if
+        # offset=0, always fetch the first segment, and then allow
+        # Segmentation to be responsible for pulling the subsequent ones if
+        # the first wasn't large enough. If offset>0, we're going to need an
+        # extra roundtrip to get the UEB (and therefore the segment size)
+        # before we can figure out which segment to get. TODO: allow the
+        # offset-table-guessing code (which starts by guessing the segsize)
+        # to assist the offset>0 process.
         d = s.start()
         return d
 
@@ -953,20 +958,16 @@ class DecryptingConsumer:
 
     def __init__(self, consumer, readkey, offset):
         self._consumer = consumer
-        self._decryptor = AES(readkey)
-        if offset != 0:
-            # TODO: pycryptopp CTR-mode needs random-access operations: I
-            # want either a=AES(readkey, offset) or better yet both of:
-            #  a=AES(readkey, offset=0)
-            #  a.process(ciphertext, offset=xyz)
-            # For now, we fake it by decrypting a bunch of dummy data first.
-            dummysize = offset
-            BLOCKSIZE = 16*1024
-            while dummysize:
-                size = min(dummysize, BLOCKSIZE)
-                dummydata = "0" * size
-                self._decryptor.process(dummydata)
-                dummysize -= size
+        # TODO: pycryptopp CTR-mode needs random-access operations: I want
+        # either a=AES(readkey, offset) or better yet both of:
+        #  a=AES(readkey, offset=0)
+        #  a.process(ciphertext, offset=xyz)
+        # For now, we fake it with the existing iv= argument.
+        offset_big = offset // 16
+        offset_small = offset % 16
+        iv = binascii.unhexlify("%032x" % offset_big)
+        self._decryptor = AES(readkey, iv=iv)
+        self._decryptor.process("\x00"*offset_small)
 
     def registerProducer(self, producer):
         # this passes through, so the real consumer can flow-control the real
@@ -996,24 +997,17 @@ class ImmutableFileNode:
         return self._cnode.read(decryptor, offset, size)
 
 
-# download-process lifetime management
-#
-#  I like the idea of having each FileNode be a service child of the
-#  Downloader (or some filenode-parent), or at least having the downloader
-#  component of each node be parented to a service somewhere. That way, when
-#  the clientnode shuts down (as in unit tests), we can turn off all work in
-#  progress.
-#
-#  However, this complicates reference management. If a caller creates a
-#  filenode, uses it up, and then discards it, we need it to go away, without
-#  requiring the caller to explicitly drop it. That means the service-parent
-#  must hold a weakref instead.
-#
-#  Maybe create a special object to act as the parent of these filenodes. It
-#  will use a WeakSet (i.e. WeakKeyDictionary with value=None). In its
-#  stopService, it will call stopService (and wait) for all existing
-#  children. When
-
-# make downloaders register with an Inhibitor(Service), which maintains a
-# weakset, and tells them all to shutdown when it gets a stopService.
+# TODO: if server1 has all shares, and server2-10 have one each, make the
+# loop stall slightly before requesting all shares from the first server, to
+# give it a chance to learn about the other shares and get some diversity.
+# Or, don't bother, let the first block all come from one server, and take
+# comfort in the fact that we'll learn about the other servers by the time we
+# fetch the second block.
+
+# as a query gets later, we're more willing to duplicate work.
 
+# should change server read protocol to allow small shares to be fetched in a
+# single RTT. Instead of get_buckets-then-read, just use read(shnums, readv),
+# where shnums=[] means all shares, and the return value is a dict of
+# # shnum->ta (like with mutable files). The DYHB query should also fetch the
+# offset table, since everything else can be located once we have that.

commit e1561d11f4443f24067694dae766a9ba51933433
Author: Brian Warner <warner@lothar.com>
Date:   Mon Feb 1 22:09:57 2010 -0800

    more WIP
---
 src/allmydata/immutable/download2.py |  243 ++++++++++++++++++++++------------
 1 files changed, 158 insertions(+), 85 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index b00fda2..dfc48b7 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -193,12 +193,12 @@ class Share:
         hashdata = rdata.get(o["share_hashes"], hashlen)
         if not hashdata:
             return False
-        share_hashes = []
+        share_hashes = {}
         for i in range(0, hashlen, 2+HASH_SIZE):
             hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
             hashvalue = hashdata[i+2:i+2+HASH_SIZE]
-            share_hashes.append( (hashnum, hashvalue) )
-        ok = self._node.process_share_hashes(share_hashes) # XXX
+            share_hashes[hashnum] = hashvalue
+        ok = self._node.process_share_hashes(share_hashes)
         # adds to self._node.share_hash_tree
         if ok:
             rdata.remove(o["share_hashes"], hashlen)
@@ -379,13 +379,23 @@ class Share:
         return o
 
 class CommonShare:
+    """I hold data that is common across all instances of a single share,
+    like sh2 on both servers A and B. This is just the block hash tree.
+    """
+    def __init__(self, numsegs):
+        if numsegs is not None:
+            self._block_hash_tree = IncompleteHashTree(numsegs)
+
+    def got_numsegs(self, numsegs):
+        self._block_hash_tree = IncompleteHashTree(numsegs)
+
     def process_block_hashes(self, block_hashes):
-        self.block_hash_tree.add_hashes(block_hashes)
+        self._block_hash_tree.add_hashes(block_hashes)
         return True
     def check_block(self, segnum, block):
         h = hashutil.block_hash(block)
         try:
-            self.block_hash_tree.set_hashes(leaves={segnum: h})
+            self._block_hash_tree.set_hashes(leaves={segnum: h})
         except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
             LOG(...)
             return False
@@ -401,7 +411,8 @@ class CommonShare:
 
 class SegmentFetcher:
     """I am responsible for acquiring blocks for a single segment."""
-    def __init__(self, segnum):
+    def __init__(self, parent, segnum):
+        self.parent = parent
         self.segnum = segnum
         self.shares = {} # maps non-dead Share instance to a state, one of
                          # (UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT).
@@ -415,6 +426,10 @@ class SegmentFetcher:
         self.shnums = DictOfSets() # maps shnum to the shares that provide it
         self.blocks = {} # maps shnum to validated block data
         self._no_more_shares = False
+        self.running = True
+
+    def stop(self):
+        self.running = False
 
     def add_shares(self, shares):
         # called when ShareFinder locates a new share, and when a non-initial
@@ -439,10 +454,12 @@ class SegmentFetcher:
         self._no_more_shares = True
 
     def loop(self):
+        if not self.running:
+            return
         # are we done?
         if self.count_shnums(COMPLETE) >= self.k:
             # yay!
-            self.parent.got_blocks(self, self.blocks)
+            self.parent.process_blocks(self.segnum, self.blocks)
             return stop_running()
 
         # we may have exhausted everything
@@ -505,12 +522,13 @@ class SegmentFetcher:
             self.shnums[shnum].remove(share)
         eventually(self.loop)
 
+
 class ShareFinder(service.MultiService):
     def __init__(self, storage_broker, storage_index,
-                 consumer, max_outstanding_requests=10):
+                 share_consumer, max_outstanding_requests=10):
         service.MultiService.__init__(self)
         self._servers = storage_broker.get_servers_for_index(storage_index)
-        self.consumer = consumer
+        self.share_consumer = share_consumer
         self.max_outstanding = max_outstanding_requests
 
         self._hungry = False
@@ -519,6 +537,16 @@ class ShareFinder(service.MultiService):
         self.undelivered_shares = []
         self.pending_requests = set()
 
+        self._num_segments = None
+        d = share_consumer.get_num_segments()
+        d.addCallback(self._got_numsegs)
+        d.addErrback(log.err, ...) # XXX
+
+    def _got_numsegs(self, numsegs):
+        for cs in self._commonshares.values():
+            cs.got_numsegs(numsegs)
+        self._num_segments = numsegs
+
     # called by our parent CiphertextDownloader
     def hungry(self):
         self._hungry = True
@@ -532,7 +560,7 @@ class ShareFinder(service.MultiService):
             sh = self.undelivered_shares.pop(0)
             # they will call hungry() again if they want more
             self._hungry = False
-            eventually(self.consumer.got_shares, [sh])
+            eventually(self.share_consumer.got_shares, [sh])
             return
         if len(self.pending_requests) >= self.max_outstanding_requests:
             # cannot send more requests, must wait for some to retire
@@ -549,7 +577,7 @@ class ShareFinder(service.MultiService):
             # we've run out of servers (so we can't send any more requests),
             # and we have nothing in flight. No further progress can be made.
             # They are destined to remain hungry.
-            self.consumer.no_more_shares()
+            self.share_consumer.no_more_shares()
             self.stopService()
             # TODO: stop running
             return
@@ -565,61 +593,11 @@ class ShareFinder(service.MultiService):
         if good:
             rref = response # the BucketReader
             if shnum not in self._commonshares:
-                self._commonshares[shnum] = CommonShare(...)
+                self._commonshares[shnum] = CommonShare(self._num_segments)
             s = Share(rref, verifycap, self._commonshares[shnum], node)
             self.undelivered_shares.append(s)
             eventually(self.loop)
 
-class CiphertextDownloader(service.MultiService):
-    def __init__(self, node, storage_broker, secret_holder, history):
-        service.MultiService.__init__(self)
-        self._active_segment_fetcher = None
-        self._node = node
-        self._storage_broker = storage_broker
-        self._secret_holder = secret_holder
-        self._history = history
-        self._sharefinder = ShareFinder(storage_broker, storage_index, self)
-        self._sharefinder.setServiceParent(self)
-
-    # called by a Reader
-    def want_segment(..
-
-    # called by.. ??
-    def start_new_segment(self, segnum):
-        # one segment at a time
-        assert not self._active_segment_fetcher
-        sf = SegmentFetcher(segnum)
-        self._active_segment_fetcher = sf
-        sf.setServiceParent(self)
-        active_shares = [s for s in self._shares if s.not_dead()]
-        sf.add_shares(active_shares) # this triggers the loop
-
-    # called by our child SegmentFetcher
-    def got_blocks(self, sf, blocks):
-        assert sf is self._active_segment_fetcher
-        sf.disownServiceParent()
-        self._active_segment_fetcher = None
-        ... # decode, deliver
-
-    def fetch_failed(self, sf):
-        assert sf is self._active_segment_fetcher
-        sf.disownServiceParent()
-        self._active_segment_fetcher = None
-        ... # deliver error upwards
-
-    def want_more_shares(self):
-        self._sharefinder.hungry()
-
-    # called by our child ShareFinder
-    def got_shares(self, shares):
-        self._shares.update(shares)
-        if self._active_segment_fetcher:
-            self._active_segment_fetcher.add_shares(shares)
-    def no_more_shares(self):
-        self._no_more_shares = True
-        if self._active_segment_fetcher:
-            self._active_segment_fetcher.no_more_shares()
-
 
 
 class Segmentation:
@@ -718,16 +696,20 @@ class Cancel:
 class CiphertextFileNode:
     # Share._node points to me
     def __init__(self, verifycap, storage_broker, secret_holder,
-                 download_manager, history):
+                 inhibitor, history):
         assert isinstance(verifycap, CHKFileVerifierURI)
         self.u = verifycap
-        self._downloader = CiphertextDownloader(self, storage_broker,
-                                                secret_holder, history)
-        self._downloader.setServiceParent(download_manager)
+        storage_index = verifycap.storage_index
+        self._needed_shares = verifycap.needed_shares
+        self._total_shares = verifycap.total_shares
+        self.running = True
+        inhibitor.register(self) # calls self.stop() at stopService()
         # the rule is: only send network requests if you're active
         # (self.running is True). You can do eventual-sends any time. This
         # rule should mean that once stopService()+flushEventualQueue()
         # fires, everything will be done.
+        self._secret_holder = secret_holder
+        self._history = history
 
         self.share_hash_tree = IncompleteHashTree(self.u.total_shares)
 
@@ -744,7 +726,33 @@ class CiphertextFileNode:
 
         # things to track callers that want data
         self._segsize_observers = OneShotObserverList()
+        self._numsegs_observers = OneShotObserverList()
+        # _segment_requests can have duplicates
         self._segment_requests = [] # (segnum, d, cancel_handle)
+        self._active_segnum = None
+        self._active_fetcher = None
+
+        self._sharefinder = ShareFinder(storage_broker, storage_index, self)
+        self._shares = set()
+
+    def stop(self):
+        if self._active_fetcher:
+            self._active_fetcher.stop()
+        self._sharefinder.stop()
+        ...
+
+    # things called by our client, either a filenode user or an
+    # ImmutableFileNode wrapper
+
+    def read(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data."""
+        # tolerate concurrent operations: each gets its own Reader
+        if size is None:
+            size = self._size - offset
+        s = Segmentation(self, offset, size, consumer)
+        d = s.start()
+        return d
 
     # things called by the Segmentation object used to transform
     # arbitrary-sized read() calls into quantized segment fetches
@@ -753,6 +761,10 @@ class CiphertextFileNode:
         """I return a Deferred that fires with the segment_size used by this
         file."""
         return self._segsize_observers.when_fired()
+    def get_num_segments(self):
+        """I return a Deferred that fires with the number of segments used by
+        this file."""
+        return self._numsegs_observers.when_fired()
 
     def get_segment(self, segnum):
         """Begin downloading a segment. I return a tuple (d, c): 'd' is a
@@ -766,21 +778,43 @@ class CiphertextFileNode:
         d = defer.Deferred()
         c = Cancel(self)
         self._segment_requests.append( (segnum, d, c) )
+        self._start_new_segment()
         eventually(self._loop)
         return (d, c)
 
-    # things called by our client, either a filenode user or an
-    # ImmutableFileNode wrapper
+    def _start_new_segment(self):
+        if self._active_segnum is None and self._segment_requests:
+            self._active_segnum = self._segment_requests[0][0]
+            self._active_fetcher = sf = SegmentFetcher(self, segnum)
+            active_shares = [s for s in self._shares if s.not_dead()]
+            sf.add_shares(active_shares) # this triggers the loop
 
-    def read(self, consumer, offset=0, size=None):
-        """I am the main entry point, from which FileNode.read() can get
-        data."""
-        # tolerate concurrent operations: each gets its own Reader
-        if size is None:
-            size = self._size - offset
-        s = Segmentation(self, offset, size, consumer)
-        d = s.start()
-        return d
+
+    # called by our child ShareFinder
+    def got_shares(self, shares):
+        self._shares.update(shares)
+        if self._active_fetcher:
+            self._active_fetcher.add_shares(shares)
+    def no_more_shares(self):
+        self._no_more_shares = True
+        if self._active_fetcher:
+            self._active_fetcher.no_more_shares()
+
+    # called by our child SegmentFetcher
+    def got_blocks(self, sf, blocks):
+        assert sf is self._active_fetcher
+        sf.disownServiceParent()
+        self._active_fetcher = None
+        ... # decode, deliver
+
+    def fetch_failed(self, sf):
+        assert sf is self._active_fetcher
+        sf.disownServiceParent()
+        self._active_fetcher = None
+        ... # deliver error upwards
+
+    def want_more_shares(self):
+        self._sharefinder.hungry()
 
     # things called by our Share instances
 
@@ -794,6 +828,7 @@ class CiphertextFileNode:
             self._parse_UEB(self, d) # sets self._stuff, raise error
             self.have_UEB = True
             self._segsize_observers.fire(self.segment_size)
+            self._numsegs_observers.fire(self.num_segments)
             return True
         except (errors..):
             return False
@@ -801,7 +836,7 @@ class CiphertextFileNode:
 
     def _parse_UEB(self, d):
         self.share_size = mathutil.div_ceil(self._verifycap.size,
-                                            self._verifycap.needed_shares)
+                                            self._needed_shares)
 
         # First, things that we really need to learn from the UEB:
         # segment_size, crypttext_root_hash, and share_root_hash.
@@ -819,6 +854,14 @@ class CiphertextFileNode:
         self.tail_segment_size = mathutil.next_multiple(self.tail_data_size,
                                                         self._needed_shares)
 
+        # zfec.Decode() instantiation is fast, but still, let's use the same
+        # codec for anything we can. 3-of-10 takes 15us on my laptop,
+        # 25-of-100 is 900us, 25-of-255 is 2.5ms, 3-of-255 is 97us
+        self._codec = codec.CRSDecoder()
+        self._codec.set_params(self.segment_size,
+                               self._needed_shares, self._total_shares)
+
+
         # Ciphertext hash tree root is mandatory, so that there is at most
         # one ciphertext that matches this read-cap or verify-cap. The
         # integrity check on the shares is not sufficient to prevent the
@@ -845,25 +888,55 @@ class CiphertextFileNode:
 
 
     def process_share_hashes(self, share_hashes):
-        self.share_hash_tree.x(share_hashes) # XXX
+        try:
+            self.share_hash_tree.set_hashes(share_hashes)
+        except IndexError, hashtree.BadHashError, hashtree.NotEnoughHashesError:
+            STUFF # XXX
+            return False
         return True
 
-    # other stuff
-    def _loop(self):
-        pass
-
-    def _got_segment(self, segnum, segment):
+    def process_blocks(self, segnum, blocks):
+        codec = self._codec
+        if segnum == self.num_segments-1:
+            codec = codec.CRSDecoder()
+            k, N = self._needed_shares, self._total_shares
+            codec.set_params(self.tail_segment_size, k, N)
+
+        shares = []
+        shareids = []
+        for (shareid, share) in blocks.iteritems():
+            shareids.append(shareid)
+            shares.append(share)
+        del blocks
+        segment = codec.decode(shares, shareids)
+        del shares
+        self._process_segment(segnum, segment)
+
+    def _process_segment(self, segnum, segment):
+        h = hashutil.crypttext_hash(segment)
+        try:
+            self.ciphertext_hash_tree.set_hashes(leaves={segnum, h})
+        except SOMETHING:
+            SOMETHING
+        assert self._active_segnum == segnum
         retire = [(d,c) for (segnum0, d, c) in self._segment_requests
                   if segnum0 == segnum]
         self._segment_requests = [t for t in self._segment_requests
                                   if t[0] != segnum]
         for (d,c) in retire:
             eventually(self._deliver, d, c, segment)
+        self._active_segnum = None
+        self._start_new_segment()
 
     def _cancel(self, c):
         self._segment_requests = [t for t in self._segment_requests
                                   if t[2] != c]
-        # TODO: update active
+        segnums =  [segnum for (segnum,d,c) in self._segment_requests]
+        if self._active_segnum not in segnums:
+            self._active_fetcher.stop()
+            self._active_fetcher = None
+            self._active_segnum = None
+            self._start_new_segment()
 
     def _deliver(self, d, c, segment):
         # this method exists to handle cancel() that occurs between

commit 9b0b7aefb7711bb58c0d4d5935853b8097f67b62
Author: Brian Warner <warner@lothar.com>
Date:   Mon Feb 1 16:14:38 2010 -0800

    split out probably-unhelpful code from probably-helpful code
---
 src/allmydata/immutable/download2.py      | 1435 +++++++++++------------------
 src/allmydata/immutable/download2_off.py  |  647 +++++++++++++
 src/allmydata/immutable/download2_util.py |   33 +
 3 files changed, 1241 insertions(+), 874 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index 3480a9c..b00fda2 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -1,43 +1,4 @@
 
-# known (shnum,Server) pairs are sorted into a list according to
-# desireability. This sort is picking a winding path through a matrix of
-# [shnum][server]. The goal is to get diversity of both shnum and server.
-
-# The initial order is:
-#  find the lowest shnum on the first server, add it
-#  look at the next server, find the lowest shnum that we don't already have
-#   if any
-#  next server, etc, until all known servers are checked
-#  now look at servers that we skipped (because ...
-
-# Keep track of which block requests are outstanding by (shnum,Server). Don't
-# bother prioritizing "validated" shares: the overhead to pull the share hash
-# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
-# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
-# 832 bytes). Each time a block request is sent, also request any necessary
-# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
-# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
-
-# For each outstanding segread, walk the list and send requests (skipping
-# outstanding shnums) until requests for k distinct shnums are in flight. If
-# we can't do that, ask for more. If we get impatient on a request, find the
-# first non-outstanding
-
-# start with the first Share in the list, and send a request. Then look at
-# the next one. If we already have a pending request for the same shnum or
-# server, push that Share down onto the fallback list and try the next one,
-# etc. If we run out of non-fallback shares, use the fallback ones,
-# preferring shnums that we don't have outstanding requests for (i.e. assume
-# that all requests will complete). Do this by having a second fallback list.
-
-# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
-# per file, not thousands.
-
-# actually, don't bother sorting the initial list. Append Shares as the
-# responses come back, that will put the fastest servers at the front of the
-# list, and give a tiny preference to servers that are earlier in the
-# permuted order.
-
 # TODO: if server1 has all shares, and server2-10 have one each, make the
 # loop stall slightly before requesting all shares from the first server, to
 # give it a chance to learn about the other shares and get some diversity.
@@ -47,19 +8,6 @@
 
 # as a query gets later, we're more willing to duplicate work.
 
-# more ideas:
-#  sort shares by:
-#   1: number of roundtrips needed to get some data
-#   2: share number
-#   3: ms of RTT delay
-# maybe measure average time-to-completion of requests, compare completion
-# time against that, much larger indicates congestion on the server side
-# or the server's upstream speed is less than our downstream. Minimum
-# time-to-completion indicates min(our-downstream,their-upstream). Could
-# fetch shares one-at-a-time to measure that better.
-
-# when should we risk duplicate work and send a new request?
-
 # should change server read protocol to allow small shares to be fetched in a
 # single RTT. Instead of get_buckets-then-read, just use read(shnums, readv),
 # where shnums=[] means all shares, and the return value is a dict of
@@ -69,617 +17,39 @@
 from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
      NotEnoughHashesError
 
-class PendingRequests:
-    def add(self, s):
-        self.outstanding[s] = time.time() # turn this into a list for retries
-    def done(self, s):
-        self.outstanding.pop(s)
-    def nonlate(self):
-        pass
-
-def walk(self):
-    shares = sorted(list)
-    oldshares = copy(shares)
-    outstanding = list()
-    fallbacks = list()
-    second_fallbacks = list()
-    while len(outstanding.nonlate.shnums) < k: # need more requests
-        while oldshares:
-            s = shares.pop(0)
-            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
-                fallbacks.append(s)
-                continue
-            outstanding.append(s)
-            send_request(s)
-            break #'while need_more_requests'
-        # must use fallback list. Ask for more servers while we're at it.
-        ask_for_more_servers()
-        while fallbacks:
-            s = fallbacks.pop(0)
-            if s.shnum in outstanding.shnums:
-                # assume that the outstanding requests will complete, but
-                # send new requests for other shnums to existing servers
-                second_fallbacks.append(s)
-                continue
-            outstanding.append(s)
-            send_request(s)
-            break #'while need_more_requests'
-        # if we get here, we're being forced to send out multiple queries per
-        # share. We've already asked for more servers, which might help. If
-        # there are no late outstanding queries, then duplicate shares won't
-        # help. Don't send queries for duplicate shares until some of the
-        # queries are late.
-        if outstanding.late:
-            # we're allowed to try any non-outstanding share
-            while second_fallbacks:
-                pass
-    newshares = outstanding + fallbacks + second_fallbacks + oldshares
-        
-
-class Reader:
-    implements(IPushProducer)
-    def __init__(self, consumer, offset, size):
-        self._needed = []
-        self._consumer = consumer
-        self._hungry = False
-        self._offset = offset
-        self._size = size
-    def start(self):
-        self._alive = True
-        self._deferred = defer.Deferred()
-        return self._deferred
-
-    def _compute_segnums(self, segsize):
-        # now that we know the file's segsize, what segments (and which
-        # ranges of each) will we need?
-        while size:
-            assert size >= 0
-            this_seg_num = int(offset / self._segsize)
-            this_seg_offset = offset - (seg_num*self._segsize)
-            this_seg_size = min(size, self._segsize-seg_offset)
-            size -= this_seg_size
-            if size:
-                offset += this_seg_size
-            record = (this_seg_num, this_seg_offset, this_seg_size)
-            self._needed.append(record)
-
-    def get_needed_segments(self):
-        return set([segnum for (segnum, off, size) in self._needed])
-
-
-    def stopProducing(self):
-        self._hungry = False
-        self._alive = False
-    def pauseProducing(self):
-        self._hungry = False
-    def resumeProducing(self):
-        self._hungry = True
-    def add_segment(self, segnum, offset, size):
-        self._needed.append( (segnum, offset, size) )
-    def got_segment(self, segnum, segdata):
-        """Return True if this schedule has more to go, or False if it is
-        done."""
-        assert self._needed[0][segnum] == segnum
-        (_ign, offset, size) = self._needed.pop(0)
-        data = segdata[offset:offset+size]
-        self._consumer.write(data)
-        if not self._needed:
-            # we're done
-            self._alive = False
-            self._hungry = False
-            self._consumer.unregisterProducer()
-            self._deferred.callback(self._consumer)
-    def error(self, f):
-        self._alive = False
-        self._hungry = False
-        self._consumer.unregisterProducer()
-        self._deferred.errback(f)
-
-class Server:
-    """I represent an abstract Storage Server. One day, the StorageBroker
-    will return instances of me. For now, the StorageBroker returns (peerid,
-    RemoteReference) tuples, and this code wraps a Server instance around
-    them.
-    """
-    def __init__(self, peerid, ss):
-        self.peerid = peerid
-        self.remote = ss
-        self._remote_buckets = {} # maps shnum to RIBucketReader
-        # TODO: release the bucket references on shares that we no longer
-        # want. OTOH, why would we not want them? Corruption?
-
-    def send_query(self, storage_index):
-        """I return a Deferred that fires with a set of shnums. If the server
-        had shares available, I will retain the RemoteReferences to its
-        buckets, so that get_data(shnum, range) can be called later."""
-        d = self.remote.callRemote("get_buckets", self.storage_index)
-        d.addCallback(self._got_response)
-        return d
-
-    def _got_response(self, r):
-        self._remote_buckets = r
-        return set(r.keys())
-
-def incidentally(res, f, *args, **kwargs):
-    """Add me to a Deferred chain like this:
-     d.addBoth(incidentally, func, arg)
-    and I'll behave as if you'd added the following function:
-     def _(res):
-         func(arg)
-         return res
-    This is useful if you want to execute an expression when the Deferred
-    fires, but don't care about its value.
-    """
-    f(*args, **kwargs)
-    return res
-
-class DictOfSets:
-    def add(self, key, value): pass
-    def values(self): # return set that merges all value sets
-        r = set()
-        for key in self:
-            r.update(self[key])
-        return r
-
-class ShareOnAServer:
-    """I represent one instance of a share, known to live on a specific
-    server. I am created every time a server responds affirmatively to a
-    do-you-have-block query."""
-
-    def __init__(self, shnum, server):
-        self._shnum = shnum
-        self._server = server
-        self._block_hash_tree = None
-
-    def cost(self, segnum):
-        """I return a tuple of (roundtrips, bytes, rtt), indicating how
-        expensive I think it would be to fetch the given segment. Roundtrips
-        indicates how many roundtrips it is likely to take (one to get the
-        data and hashes, plus one to get the offset table and UEB if this is
-        the first segment we've ever fetched). 'bytes' is how many bytes we
-        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
-        seconds for a trivial request. The downloading algorithm will compare
-        costs to decide which shares should be used."""
-        # the most significant factor here is roundtrips: a Share for which
-        # we already have the offset table is better to than a brand new one
-
-    def max_bandwidth(self):
-        """Return a float, indicating the highest plausible bytes-per-second
-        that I've observed coming from this share. This will be based upon
-        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
-        can we used to estimate the server's upstream bandwidth. Clearly this
-        is only accurate if a share is retrieved with no contention for
-        either the upstream, downstream, or middle of the connection, but it
-        may still serve as a useful metric for deciding which servers to pull
-        from."""
-
-    def get_segment(self, segnum):
-        """I return a Deferred that will fire with the segment data, or
-        errback."""
-
-class NativeShareOnAServer(ShareOnAServer):
-    """For tahoe native (foolscap) servers, I contain a RemoteReference to
-    the RIBucketReader instance."""
-    def __init__(self, shnum, server, rref):
-        ShareOnAServer.__init__(self, shnum, server)
-        self._rref = rref # RIBucketReader
-
-class Share:
-    def __init__(self, shnum):
-        self._shnum = shnum
-        # _servers are the Server instances which appear to hold a copy of
-        # this share. It is populated when the ValidShare is first created,
-        # or when we receive a get_buckets() response for a shnum that
-        # already has a ValidShare instance. When we lose the connection to a
-        # server, we remove it.
-        self._servers = set()
-        # offsets, UEB, and share_hash_tree all live in the parent.
-        # block_hash_tree lives here.
-        self._block_hash_tree = None
-
-        self._want
-
-    def get_servers(self):
-        return self._servers
-
-
-    def get_block(self, segnum):
-        # read enough data to obtain a single validated block
-        if not self.have_offsets:
-            # we get the offsets in their own read, since they tell us where
-            # everything else lives. We must fetch offsets for each share
-            # separately, since they aren't directly covered by the UEB.
-            pass
-        if not self.parent.have_ueb:
-            # use _guessed_segsize to make a guess about the layout, so we
-            # can fetch both the offset table and the UEB in the same read.
-            # This also requires making a guess about the presence or absence
-            # of the plaintext_hash_tree. Oh, and also the version number. Oh
-            # well.
-            pass
-
-class CiphertextDownloader:
-    """I manage all downloads for a single file. I operate a state machine
-    with input events that are local read() requests, responses to my remote
-    'get_bucket' and 'read_bucket' messages, and connection establishment and
-    loss. My outbound events are connection establishment requests and bucket
-    read requests messages.
-    """
-    # eventually this will merge into the FileNode
-    ServerClass = Server # for tests to override
-
-    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
-                 shutdowner):
-        # values we get from the filecap
-        self._storage_index = si = storage_index
-        self._ueb_hash = ueb_hash
-        self._size = size
-        self._needed_shares = k
-        self._total_shares = N
-        self._share_hash_tree = IncompleteHashTree(self._total_shares)
-        # values we discover when we first fetch the UEB
-        self._ueb = None # is dict after UEB fetch+validate
-        self._segsize = None
-        self._numsegs = None
-        self._blocksize = None
-        self._tail_segsize = None
-        self._ciphertext_hash = None # optional
-        # structures we create when we fetch the UEB, then continue to fill
-        # as we download the file
-        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
-        self._ciphertext_hash_tree = None
-
-        # values we learn as we download the file
-        self._offsets = {} # (shnum,Server) to offset table (dict)
-        self._block_hash_tree = {} # shnum to IncompleteHashTree
-        # other things which help us
-        self._guessed_segsize = min(128*1024, size)
-        self._active_share_readers = {} # maps shnum to Reader instance
-        self._share_readers = [] # sorted by preference, best first
-        self._readers = set() # set of Reader instances
-        self._recent_horizon = 10 # seconds
-
-        # 'shutdowner' is a MultiService parent used to cancel all downloads
-        # when the node is shutting down, to let tests have a clean reactor.
-
-        self._init_available_servers()
-        self._init_find_enough_shares()
-
-    # _available_servers is an iterator that provides us with Server
-    # instances. Each time we pull out a Server, we immediately send it a
-    # query, so we don't need to keep track of who we've sent queries to.
-
-    def _init_available_servers(self):
-        self._available_servers = self._get_available_servers()
-        self._no_more_available_servers = False
-
-    def _get_available_servers(self):
-        """I am a generator of servers to use, sorted by the order in which
-        we should query them. I make sure there are no duplicates in this
-        list."""
-        # TODO: make StorageBroker responsible for this non-duplication, and
-        # replace this method with a simple iter(get_servers_for_index()),
-        # plus a self._no_more_available_servers=True
-        seen = set()
-        sb = self._storage_broker
-        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
-            if peerid not in seen:
-                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
-                seen.add(peerid)
-        self._no_more_available_servers = True
-
-    # this block of code is responsible for having enough non-problematic
-    # distinct shares/servers available and ready for download, and for
-    # limiting the number of queries that are outstanding. The idea is that
-    # we'll use the k fastest/best shares, and have the other ones in reserve
-    # in case those servers stop responding or respond too slowly. We keep
-    # track of all known shares, but we also keep track of problematic shares
-    # (ones with hash failures or lost connections), so we can put them at
-    # the bottom of the list.
-
-    def _init_find_enough_shares(self):
-        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
-        # where viable (but not yet validated) shares are located. Each
-        # get_bucket() response adds to this map, each act of validation
-        # removes from it.
-        self._sharemap = DictOfSets()
-
-        # _sharemap maps shnum to set of Servers, and remembers where viable
-        # shares are located. Each get_bucket() response adds to this map,
-        # each hash failure or disconnect removes from it. (TODO: if we
-        # disconnect but reconnect later, we should be allowed to re-query).
-        self._sharemap = DictOfSets()
-
-        # _problem_shares is a set of (shnum, Server) tuples, and
-
-        # _queries_in_flight maps a Server to a timestamp, which remembers
-        # which servers we've sent queries to (and when) but have not yet
-        # heard a response. This lets us put a limit on the number of
-        # outstanding queries, to limit the size of the work window (how much
-        # extra work we ask servers to do in the hopes of keeping our own
-        # pipeline filled). We remove a Server from _queries_in_flight when
-        # we get an answer/error or we finally give up. If we ever switch to
-        # a non-connection-oriented protocol (like UDP, or forwarded Chord
-        # queries), we can use this information to retransmit any query that
-        # has gone unanswered for too long.
-        self._queries_in_flight = dict()
-
-    def _count_recent_queries_in_flight(self):
-        now = time.time()
-        recent = now - self._recent_horizon
-        return len([s for (s,when) in self._queries_in_flight.items()
-                    if when > recent])
-
-    def _find_enough_shares(self):
-        # goal: have 2*k distinct not-invalid shares available for reading,
-        # from 2*k distinct servers. Do not have more than 4*k "recent"
-        # queries in flight at a time.
-        if (len(self._sharemap) >= 2*self._needed_shares
-            and len(self._sharemap.values) >= 2*self._needed_shares):
-            return
-        num = self._count_recent_queries_in_flight()
-        while num < 4*self._needed_shares:
-            try:
-                s = self._available_servers.next()
-            except StopIteration:
-                return # no more progress can be made
-            self._queries_in_flight[s] = time.time()
-            d = s.send_query(self._storage_index)
-            d.addBoth(incidentally, self._queries_in_flight.discard, s)
-            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
-                                           for shnum in shnums],
-                           lambda f: self._query_error(f, s))
-            d.addErrback(self._error)
-            d.addCallback(self._reschedule)
-            num += 1
-
-    def _query_error(self, f, s):
-        # a server returned an error, log it gently and ignore
-        level = log.WEIRD
-        if f.check(DeadReferenceError):
-            level = log.UNUSUAL
-        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
-                failure=f, level=level, umid="3uuBUQ")
-
-    # this block is responsible for turning known shares into usable shares,
-    # by fetching enough data to validate their contents.
-
-    # UEB (from any share)
-    # share hash chain, validated (from any share, for given shnum)
-    # block hash (any share, given shnum)
-
-    def _got_ueb(self, ueb_data, share):
-        if self._ueb is not None:
-            return
-        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
-            share.error("UEB hash does not match")
-            return
-        d = uri.unpack_extension(ueb_data)
-        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
-
-
-        # There are several kinds of things that can be found in a UEB.
-        # First, things that we really need to learn from the UEB in order to
-        # do this download. Next: things which are optional but not redundant
-        # -- if they are present in the UEB they will get used. Next, things
-        # that are optional and redundant. These things are required to be
-        # consistent: they don't have to be in the UEB, but if they are in
-        # the UEB then they will be checked for consistency with the
-        # already-known facts, and if they are inconsistent then an exception
-        # will be raised. These things aren't actually used -- they are just
-        # tested for consistency and ignored. Finally: things which are
-        # deprecated -- they ought not be in the UEB at all, and if they are
-        # present then a warning will be logged but they are otherwise
-        # ignored.
-
-        # First, things that we really need to learn from the UEB:
-        # segment_size, crypttext_root_hash, and share_root_hash.
-        self._segsize = d['segment_size']
-
-        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
-        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
-
-        self._tail_segsize = self._size % self._segsize
-        if self._tail_segsize == 0:
-            self._tail_segsize = self._segsize
-        # padding for erasure code
-        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
-                                                    self._needed_shares)
-
-        # Ciphertext hash tree root is mandatory, so that there is at most
-        # one ciphertext that matches this read-cap or verify-cap. The
-        # integrity check on the shares is not sufficient to prevent the
-        # original encoder from creating some shares of file A and other
-        # shares of file B.
-        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
-        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
-
-        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
-
-
-        # Next: things that are optional and not redundant: crypttext_hash
-        if 'crypttext_hash' in d:
-            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
-                self._ciphertext_hash = d['crypttext_hash']
-            else:
-                log.msg("ignoring bad-length UEB[crypttext_hash], "
-                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
-                                                   hashutil.CRYPTO_VAL_SIZE),
-                        umid="oZkGLA", level=log.WEIRD)
-
-        # we ignore all of the redundant fields when downloading. The
-        # Verifier uses a different code path which does not ignore them.
-
-        # finally, set self._ueb as a marker that we don't need to request it
-        # anymore
-        self._ueb = d
-
-    def _got_share_hashes(self, hashes, share):
-        assert isinstance(hashes, dict)
-        try:
-            self._share_hash_tree.set_hashes(hashes)
-        except (IndexError, BadHashError, NotEnoughHashesError), le:
-            share.error("Bad or missing hashes")
-            return
-
-    #def _got_block_hashes(
-
-    def _init_validate_enough_shares(self):
-        # _valid_shares maps shnum to ValidatedShare instances, and is
-        # populated once the block hash root has been fetched and validated
-        # (which requires any valid copy of the UEB, and a valid copy of the
-        # share hash chain for each shnum)
-        self._valid_shares = {}
-
-        # _target_shares is an ordered list of ReadyShare instances, each of
-        # which is a (shnum, server) tuple. It is sorted in order of
-        # preference: we expect to get the fastest response from the
-        # ReadyShares at the front of the list. It is also sorted to
-        # distribute the shnums, so that fetching shares from
-        # _target_shares[:k] is likely (but not guaranteed) to give us k
-        # distinct shares. The rule is that we skip over entries for blocks
-        # that we've already received, limit the number of recent queries for
-        # the same block, 
-        self._target_shares = []
-
-    def _validate_enough_shares(self):
-        # my goal is to have at least 2*k distinct validated shares from at
-        # least 2*k distinct servers
-        valid_share_servers = set()
-        for vs in self._valid_shares.values():
-            valid_share_servers.update(vs.get_servers())
-        if (len(self._valid_shares) >= 2*self._needed_shares
-            and len(self._valid_share_servers) >= 2*self._needed_shares):
-            return
-        #for 
-
-    def _reschedule(self, _ign):
-        # fire the loop again
-        if not self._scheduled:
-            self._scheduled = True
-            eventually(self._loop)
-
-    def _loop(self):
-        self._scheduled = False
-        # what do we need?
-
-        self._find_enough_shares()
-        self._validate_enough_shares()
-
-        if not self._ueb:
-            # we always need a copy of the UEB
-            pass
-
-    def _error(self, f):
-        # this is an unexpected error: a coding bug
-        log.err(f, level=log.UNUSUAL)
-
-    def read(self, consumer, offset=0, size=None):
-        """I am the main entry point, from which FileNode.read() can get
-        data."""
-        # tolerate concurrent operations: each gets its own Reader
-        if size is None:
-            size = self._size - offset
-        r = Reader(consumer, offset, size)
-        self._readers.add(r)
-        d = start(consumer)
-        self._loop()
-        return d
-            
-
-
-# using a single packed string (and an offset table) may be an artifact of
-# our native storage server: other backends might allow cheap multi-part
-# files (think S3, several buckets per share, one for each section).
-
-# find new names for:
-#  data_holder
-#  Share / Share2  (ShareInstance / Share? but the first is more useful)
-
-class IShare(Interface):
-    """I represent a single instance of a single share (e.g. I reference the
-    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
-    This interface is used by SegmentFetcher to retrieve validated blocks.
-    """
-    def get_block(segnum):
-        """Return an Observer2, which will be notified with the following
-        events:
-         state=COMPLETE, block=data (terminal): validated block data
-         state=OVERDUE (non-terminal): we have reason to believe that the
-                                       request might have stalled, or we
-                                       might just be impatient
-         state=CORRUPT (terminal): the data we received was corrupt
-         state=DEAD (terminal): the connection has failed
-        """
-
-
-# it'd be nice if we receive the hashes before the block, or just
-# afterwards, so we aren't stuck holding on to unvalidated blocks
-# that we can't process. If we guess the offsets right, we can
-# accomplish this by sending the block request after the metadata
-# requests (by keeping two separate requestlists), and have a one RTT
-# pipeline like:
-#  1a=metadata, 1b=block
-#  1b->process+deliver : one RTT
-
-# But if we guess wrong, and fetch the wrong part of the block, we'll
-# have a pipeline that looks like:
-#  1a=wrong metadata, 1b=wrong block
-#  1a->2a=right metadata,2b=right block
-#  2b->process+deliver
-# which means two RTT and buffering one block (which, since we'll
-# guess the segsize wrong for everything, means buffering one
-# segment)
-
-# if we start asking for multiple segments, we could get something
-# worse:
-#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
-#  1a->2a=right metadata,2b=right block0,2c=right block1, .
-#  2b->process+deliver
-
-# which means two RTT but fetching and buffering the whole file
-# before delivering anything. However, since we don't know when the
-# other shares are going to arrive, we need to avoid having more than
-# one block in the pipeline anyways. So we shouldn't be able to get
-# into this state.
-
-# it also means that, instead of handling all of
-# self._requested_blocks at once, we should only be handling one
-# block at a time: one of the requested block should be special
-# (probably FIFO). But retire all we can.
-
-class Share2:
-    def process_block_hashes(self, block_hashes):
-        self.block_hash_tree.add_hashes(block_hashes)
-        return True
-    def process_block(self, block):
-        check_hash
-        return True
-
 class Share:
     # this is a specific implementation of IShare for tahoe's native storage
     # servers. A different backend would use a different class.
     """I represent a single instance of a single share (e.g. I reference the
     shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
-    I am associated with a Share2 that remembers data that is held in common
-    among e.g. SI=abcde/shnum2 across all servers. I am also associated with
-    a File for e.g. SI=abcde (all shares, all servers).
+    I am associated with a Commonshare that remembers data that is held in
+    common among e.g. SI=abcde/shnum2 across all servers. I am also
+    associated with a CiphertextFileNode for e.g. SI=abcde (all shares, all
+    servers).
     """
     implements(IShare)
 
-    def __init__(self, rref, verifycap, share2, file):
+    def __init__(self, rref, verifycap, commonshare, node):
         self._rref = rref
         self._guess_offsets(filesize, filecap_parms, my_default_segsize)
         self.actual_offsets = None
         self._UEB_length = None
-        self._share2 = share2
-        self._file = file
+        self._commonshare = commonshare # holds block_hash_tree
+        self._node = node # holds share_hash_tree and UEB
         self._wanted = Spans() # desired metadata
         self._wanted_blocks = Spans() # desired block data
         self._requested = Spans() # we've sent a request for this
         self._received = Spans() # we've received a response for this
         self._received_data = DataSpans() # the response contents, still unused
         self._requested_blocks = [] # (segnum, [observer2s])
+        ver = rref.version["http://allmydata.org/tahoe/protocols/storage/v1"]
+        self._overrun_ok = ver["tolerates-immutable-read-overrun"]
+        # If _overrun_ok and we guess the offsets correctly, we can get
+        # everything in one RTT. If _overrun_ok and we guess wrong, we might
+        # need two RTT (but we could get lucky and do it in one). If overrun
+        # is *not* ok (tahoe-1.3.0 or earlier), we need four RTT: 1=version,
+        # 2=offset table, 3=UEB_length and everything else (hashes, block),
+        # 4=UEB.
 
     def _guess_offsets(self, filesize, filecap_parms, my_default_segsize):
         offsets = {}
@@ -690,36 +60,11 @@ class Share:
                                  'share_hashes',
                                  'uri_extension',
                                  ):
-            offsets[field] = i # bad guesses are easy :)
+            offsets[field] = i # bad guesses are easy :) # XXX stub
         self.guessed_offsets = offsets
         self._fieldsize = 4
         self._fieldstruct = ">L"
 
-    def loop(self):
-        # See what data we've received, construct anything we can from it.
-        # Use self._wanted to track what we want, then we'll send out
-        # requests as necessary to retrieve it. We only retrieve information
-        # for one segment at a time, to minimize alacrity.
-
-        #  offset table (guessed, confirmed, stashed locally)
-        #  UEB (probably in File)
-        #  share hash chain (partially in File)
-        #  block hash chain (stash pieces in Share2)
-        #  block data (ephemeral, given to client upon receipt)
-
-        # first process any information that we have the data for. Keep going
-        # until I can't get no satisfaction for the active segment.
-        oldsegnum = "always different than None"
-        newsegnum = self._active_segnum()
-        while newsegnum != oldsegnum:
-            oldsegnum = newsegnum
-            self._satisfaction() # might retire active segnum
-            newsegnum = self._active_segnum()
-        # then figure out what additional data we need
-        self._desire() # compute desire
-        # finally send out requests for whatever we need
-        self._request_needed() # express desire
-
     def _active_segnum(self):
         if self._requested_blocks:
             return self._requested_blocks[0]
@@ -732,101 +77,162 @@ class Share:
             return self._requested_blocks[0]
         return None, []
 
-    def _satisfaction(self):
-        segnum, observers = self._active_segnum_and_observers()
-        if not self.actual_offsets:
-            self._ask_for_offsets()
-        fsize = self._fieldsize
-        rdata = self._received_data
-        share2 = self._share2
+    def loop(self):
+        # we are (eventually) called after all state transitions:
+        #  new segments added to self._requested_blocks
+        #  new data received from servers (responses to our read() calls)
+        #  impatience timer fires (server appears slow)
+
+        # First, consume all of the information that we currently have, for
+        # all the segments people currently want.
+        while self._get_satisfaction():
+            pass
 
-        if not self.actual_offsets:
-            return
-        o = self.actual_offsets
+        # When we get no satisfaction, it's time to find out what we desire.
+        # The number of segments is finite, so I can't get no satisfaction
+        # forever.
+        self._desire()
 
-        UEB = self._file.UEB
-        if UEB is None:
-            UEB = self._satisfy_UEB()
-        if UEB is None:
-            return
+        # finally send out requests for whatever we need
+        self._request_needed() # express desire
+
+    def _get_satisfaction(self):
+        # return True if we retired a data block, and should therefore be
+        # called again. Return False if we don't retire a data block (even if
+        # we do retire some other data, like hash chains).
+
+        if self.actual_offsets is None:
+            if not self._satisfy_offsets():
+                # can't even look at anything without the offset table
+                return False
+
+        if self._node.UEB is None:
+            if not self._satisfy_UEB():
+                # can't check any hashes without the UEB
+                return False
+
+        if self._node.share_hash_tree.needed_hashes(self.shnum):
+            if not self._satisfy_share_hash_tree():
+                # can't check block_hash_tree without a root
+                return False
 
-        if self._file.share_hash_tree.needed_hashes(self.shnum):
-            self._satisfy_share_hash_tree()
+        segnum, observers = self._active_segnum_and_observers()
+        if segnum is None:
+            return False # we don't want any particular segment right now
+
+        # block_hash_tree
+        needed_hashes = self._commonshare.block_hash_tree.needed_hashes(segnum)
+        if needed_hashes:
+            if not self._satisfy_block_hash_tree(needed_hashes):
+                # can't check block without block_hash_tree
+                return False
 
-        if segnum is not None:
-            # block_hash_tree
-            needed_hashes = share2.block_hash_tree.needed_hashes(segnum)
-            if needed_hashes:
-                self._satisfy_block_hash_tree(needed_hashes)
+        # data blocks
+        return self._satisfy_data_block(segnum, observers)
 
-            # data blocks
-            needed_hashes = share2.block_hash_tree.needed_hashes(segnum)
-            if not needed_hashes:
-                self._satisfy_data_block(segnum, observers)
+    def _satisfy_offsets(self):
+        version_s = self._received_data.get(0, 4)
+        if version_s is None:
+            return False
+        (version,) = struct.unpack(">L", version_s)
+        if version == 1:
+            table_start = 0x0c
+            self._fieldsize = 0x4
+            self._fieldstruct = ">L"
+        else:
+            table_start = 0x14
+            self._fieldsize = 0x8
+            self._fieldstruct = ">Q"
+        offset_table_size = 6 * self._fieldsize
+        table_s = self._received_data.pop(table_start, offset_table_size)
+        if table_s is None:
+            return False
+        fields = struct.unpack(6*self._fieldstruct, table_s)
+        offsets = {}
+        for i,field in enumerate('data',
+                                 'plaintext_hash_tree', # UNUSED
+                                 'crypttext_hash_tree',
+                                 'block_hashes',
+                                 'share_hashes',
+                                 'uri_extension',
+                                 ):
+            offsets[field] = fields[i]
+        self.actual_offsets = offsets
+        self._received_data.remove(0, 4) # don't need this anymore
+        return True
 
     def _satisfy_UEB(self):
+        o = self.actual_offsets
         fsize = self._fieldsize
         rdata = self._received_data
         UEB_length_s = rdata.get(o["uri_extension"], fsize)
         if not UEB_length_s:
-            return None
+            return False
         UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
-        UEB_s = rdata.get(o["uri_extension"]+fsize, UEB_length)
+        UEB_s = rdata.pop(o["uri_extension"]+fsize, UEB_length)
         if not UEB_s:
-            return None
-        ok = self._file.validate_UEB(UEB_s)
+            return False
+        rdata.remove(o["uri_extension"], fsize)
+        ok = self._node.validate_UEB(UEB_s) # stores in self._node.UEB # XXX
         if not ok:
-            return None
+            return False
         # TODO: if this UEB was bad, we'll keep trying to
         # validate it over and over again. Only log.err on
         # the first one, or better yet skip all but the first
-        rdata.remove(o["uri_extension"]+fsize, UEB_length)
-        return self._file.UEB
+        return True
 
     def _satisfy_share_hash_tree(self):
         # the share hash chain is stored as (hashnum,hash) tuples, so you
         # can't fetch just the pieces you need, because you don't know
         # exactly where they are. So fetch everything, and parse the results
         # later.
+        o = self.actual_offsets
+        rdata = self._received_data
         hashlen = o["uri_extension"] - o["share_hashes"]
         assert hashlen % (2+HASH_SIZE) == 0
         hashdata = rdata.get(o["share_hashes"], hashlen)
         if not hashdata:
-            return
+            return False
         share_hashes = []
         for i in range(0, hashlen, 2+HASH_SIZE):
             hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
             hashvalue = hashdata[i+2:i+2+HASH_SIZE]
             share_hashes.append( (hashnum, hashvalue) )
-        ok = self._file.process_share_hashes(share_hashes)
-        # adds to self._file.share_hash_tree
+        ok = self._node.process_share_hashes(share_hashes) # XXX
+        # adds to self._node.share_hash_tree
         if ok:
             rdata.remove(o["share_hashes"], hashlen)
+            return True
+        return False
 
     def _satisfy_block_hash_tree(self, needed_hashes):
+        o = self.actual_offsets
+        rdata = self._received_data
         block_hashes = {}
         for hashnum in needed_hashes:
             hashdata = rdata.get(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
             if hashdata:
                 block_hashes[hashnum] = hashdata
             else:
-                return # missing some hashes
+                return False # missing some hashes
         # note that we don't submit any hashes to the block_hash_tree until
         # we've gotten them all, because the hash tree will throw an
         # exception if we only give it a partial set (which it therefore
         # cannot validate)
-        ok = share2.process_block_hashes(block_hashes)
+        ok = commonshare.process_block_hashes(block_hashes) # XXX
         if not ok:
-            return
+            return False
         for hashnum in needed_hashes:
             rdata.remove(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+        return True
 
     def _satisfy_data_block(self, segnum, observers):
-        segsize = self._file.UEB["segment_size"]
-        needed_shares = self._file.UEB["needed_shares"]
-        sharesize = mathutil.div_ceil(self._file.UEB["size"],
+        o = self.actual_offsets
+        segsize = self._node.UEB["segment_size"]
+        needed_shares = self._node.UEB["needed_shares"]
+        sharesize = mathutil.div_ceil(self._node.UEB["size"],
                                       needed_shares)
-        blocksize = mathutil.div_ceil(segsize, needed_shares)
+        blocksize = mathutil.div_ceil(segsize, needed_shares) # XXX
         blockstart = o["data"] + segnum * blocksize
         if blocknum < NUM_BLOCKS-1:
             blocklen = blocksize
@@ -836,93 +242,105 @@ class Share:
                 blocklen = blocksize
         block = rdata.pop(blockstart, blocklen)
         if not block:
-            return
-        ok = share2.process_block(block)
-        if not ok:
-            return
+            return False
+        # this block is being retired, either as COMPLETE or CORRUPT, since
+        # no further data reads will help
         assert self._requested_blocks[0][0] == segnum
+        ok = commonshare.check_block(segnum, block)
+        if ok:
+            state = COMPLETE
+        else:
+            state = CORRUPT
         for o in observers:
-            o.notify(block=block)
+            # goes to SegmentFetcher._block_request_activity
+            o.notify(state=state, block=block)
         self._requested_blocks.pop(0) # retired
+        return True # got satisfaction
 
-    def _desire(self, segnum):
+    def _desire(self):
         segnum, observers = self._active_segnum_and_observers()
         fsize = self._fieldsize
         rdata = self._received_data
-        share2 = self._share2
+        commonshare = self._commonshare
+
+        if not self.actual_offsets:
+            self._desire_offsets()
+
+        # we can use guessed offsets as long as this server tolerates overrun
+        if not self.actual_offsets and not self._overrun_ok:
+            return # must wait for the offsets to arrive
 
         o = self.actual_offsets or self.guessed_offsets
         segsize = self.actual_segsize or self.guessed_segsize
-        if self._file.UEB is None:
-            # UEB data is stored as (length,data). We pre-fetch 2kb,
-            # which should probably cover it. If not, we'll come back
-            # here later with self._UEB_length!=None and fetch the rest.
-            self._wanted.add(o["uri_extension"], 2048)
-            if self.actual_offsets:
-                UEB_length_s = rdata.get(o["uri_extension"], fsize)
-                if UEB_length_s:
-                    UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
-                    # we know the length, so make sure we grab everything
-                    self._wanted.add(o["uri_extension"]+fsize, UEB_length)
-                # TODO: tahoe<1.3.0 would error if you read past the end
-
-        if self._file.share_hash_tree.needed_hashes(self.shnum):
+        if self._node.UEB is None:
+            self._desire_UEB(o)
+
+        if self._node.share_hash_tree.needed_hashes(self.shnum):
             hashlen = o["uri_extension"] - o["share_hashes"]
             self._wanted.add(o["share_hashes"], hashlen)
 
+        if segnum is None:
+            return # only need block hashes or blocks for active segments
+
         # block hash chain
-        needed = set()
-        if segnum is not None:
-            n = share2.block_hash_tree.needed_hashes(segnum)
-            needed.update(n)
-        for hashnum in needed:
+        for hashnum in commonshare.block_hash_tree.needed_hashes(segnum):
             self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
 
         # data
-        if segnum is not None:
-            blockstart, blocklen = COMPUTE(segnum, segsize, etc)
-            self._wanted_blocks.add(blockstart, blocklen)
+        blockstart, blocklen = COMPUTE(segnum, segsize, etc) # XXX
+        self._wanted_blocks.add(blockstart, blocklen)
 
 
-    def _ask_for_offsets(self):
-        version_s = self._received_data.get(0, 4)
-        if version_s is None:
-            # includes version number, sizes, and offsets
+    def _desire_offsets(self):
+        if self._overrun_ok:
+            # easy! this includes version number, sizes, and offsets
             self._wanted.add(0,1024)
             return
+
+        # v1 has an offset table that lives [0x0,0x24). v2 lives [0x0,0x44).
+        # To be conservative, only request the data that we know lives there,
+        # even if that means more roundtrips.
+
+        self._wanted.add(0,4)  # version number, always safe
+        version_s = self._received_data.get(0, 4)
+        if not version_s:
+            return
         (version,) = struct.unpack(">L", version_s)
         if version == 1:
             table_start = 0x0c
-            self._fieldsize = 0x4
-            self._fieldstruct = ">L"
+            fieldsize = 0x4
         else:
             table_start = 0x14
-            self._fieldsize = 0x8
-            self._fieldstruct = ">Q"
-        offset_table_size = 6 * self._fieldsize
-        table_s = self._received_data.get(table_start, offset_table_size)
-        if table_s is None:
-            self._wanted.add(table_start, offset_table_size)
-            return
+            fieldsize = 0x8
+        offset_table_size = 6 * fieldsize
+        self._wanted.add(table_start, offset_table_size)
+
+    def _desire_UEB(self, o):
+        # UEB data is stored as (length,data).
+        if self._overrun_ok:
+            # We can pre-fetch 2kb, which should probably cover it. If it
+            # turns out to be larger, we'll come back here later with a known
+            # length and fetch the rest.
+            self._wanted.add(o["uri_extension"], 2048)
 
-        data = self._received_data[table_start,offset_table_size]
-        fields = struct.unpack(6*self._fieldstruct, data)
-        offsets = {}
-        for i,field in enumerate('data',
-                                 'plaintext_hash_tree', # UNUSED
-                                 'crypttext_hash_tree',
-                                 'block_hashes',
-                                 'share_hashes',
-                                 'uri_extension',
-                                 ):
-            offsets[field] = fields[i]
-        self.actual_offsets = offsets
+        self._wanted.add(o["uri_extension"], self._fieldsize)
+        # only use a length if we're sure it's correct, otherwise we'll
+        # probably fetch a huge number
+        if not self.actual_offsets:
+            return
+        UEB_length_s = rdata.get(o["uri_extension"], self._fieldsize)
+        if UEB_length_s:
+            UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
+            # we know the length, so make sure we grab everything
+            self._wanted.add(o["uri_extension"]+self._fieldsize, UEB_length)
 
     def _request_needed(self):
         # send requests for metadata first, to avoid hanging on to large data
-        # blocks for a long time
+        # blocks any longer than necessary.
         self._send_requests(self._wanted - self._received - self._requested)
-        # then send requests for data blocks
+        # then send requests for data blocks. All the hashes should arrive
+        # before the blocks, so the blocks can be consumed and released in a
+        # single turn.
         self._send_requests(self._wanted_blocks - self._received - self._requested
 
     def _send_requests(self, needed):
@@ -931,7 +349,7 @@ class Share:
             d = self._send_request(start, length)
             d.addCallback(self._got_data, start, length)
             d.addErrback(self._got_error)
-            d.addErrback(log.err, ...)
+            d.addErrback(log.err, ...) # XXX
 
     def _send_request(self, start, length):
         return self._rref.callRemote("read", start, length)
@@ -944,7 +362,7 @@ class Share:
         self._received_data.add(start, data)
         eventually(self.loop)
 
-    def _got_error(self, f):
+    def _got_error(self, f): # XXX
         ...
 
 
@@ -960,65 +378,18 @@ class Share:
         eventually(self.loop)
         return o
 
-class ShareFinder(service.MultiService):
-    def __init__(self, storage_broker, storage_index,
-                 consumer, max_outstanding_requests=10):
-        service.MultiService.__init__(self)
-        self._servers = storage_broker.get_servers_for_index(storage_index)
-        self.consumer = consumer
-        self.max_outstanding = max_outstanding_requests
-
-        self._hungry = False
-
-        self.undelivered_shares = []
-        self.pending_requests = set()
-
-    # called by our parent CiphertextDownloader
-    def hungry(self):
-        self._hungry = True
-        eventually(self.loop)
-
-    # internal methods
-    def loop(self):
-        if not self._hungry:
-            return
-        if self.undelivered_shares:
-            sh = self.undelivered_shares.pop(0)
-            # they will call hungry() again if they want more
-            self._hungry = False
-            eventually(self.consumer.got_shares, [sh])
-            return
-        if len(self.pending_requests) >= self.max_outstanding_requests:
-            # cannot send more requests, must wait for some to retire
-            return
-
-        server = None
+class CommonShare:
+    def process_block_hashes(self, block_hashes):
+        self.block_hash_tree.add_hashes(block_hashes)
+        return True
+    def check_block(self, segnum, block):
+        h = hashutil.block_hash(block)
         try:
-            if self._servers:
-                server = self._servers.next()
-        except StopIteration:
-            self._servers = None
-
-        if not server and not self.pending_requests:
-            # we've run out of servers (so we can't send any more requests),
-            # and we have nothing in flight. No further progress can be made.
-            # They are destined to remain hungry.
-            self.consumer.no_more_shares()
-            self.stopService()
-            # TODO: stop running
-            return
-
-        self.pending_requests.add(self.send_request(server))
-        return
-
-    def send_request(self, server):
-        ...
-
-    def got_response(self, response, req, server):
-        self.pending_requests.discard(req)
-        if good:
-            self.undelivered_shares.append(Share(..))
-            eventually(self.loop)
+            self.block_hash_tree.set_hashes(leaves={segnum: h})
+        except (hashtree.BadHashError, hashtree.NotEnoughHashesError), le:
+            LOG(...)
+            return False
+        return True
 
 # all classes are also Services, and the rule is that you don't initiate more
 # work unless self.running
@@ -1043,6 +414,7 @@ class SegmentFetcher:
                          # dict.
         self.shnums = DictOfSets() # maps shnum to the shares that provide it
         self.blocks = {} # maps shnum to validated block data
+        self._no_more_shares = False
 
     def add_shares(self, shares):
         # called when ShareFinder locates a new share, and when a non-initial
@@ -1116,16 +488,8 @@ class SegmentFetcher:
                 return True # we made progress!
         return False # no progress made
 
-    # called by Shares, in response to our s.send_request() calls
-
-    # this might be better with a Deferred, using COMPLETE as the success
-    # case and CORRUPT/DEAD in an errback, because that would let us hold the
-    # 'share' and 'shnum' arguments locally (instead of roundtripping them
-    # through Share.send_request). But that OVERDUE is not terminal. So I
-    # want a new sort of callback mechanism, with the extra-argument-passing
-    # aspects of Deferred, but without being so one-shot. Is this a job for
-    # Observer? No, it doesn't take extra arguments.
     def _block_request_activity(self, share, shnum, state, block=None):
+        # called by Shares, in response to our s.send_request() calls
         if state is COMPLETE:
             # 'block' is fully validated
             self.shares[share] = COMPLETE
@@ -1141,38 +505,90 @@ class SegmentFetcher:
             self.shnums[shnum].remove(share)
         eventually(self.loop)
 
-class Observer2:
-    """A simple class to distribute multiple events to a single subscriber.
-    It accepts arbitrary kwargs, but no posargs."""
-    def __init__(self):
-        self._watcher = None
-        self._undelivered_results = []
-
-    def subscribe(self, observer, **watcher_kwargs):
-        self._watcher = (observer, watcher_kwargs)
-        while self._undelivered_results:
-            self._notify(self._undelivered_results.pop(0))
-
-    def notify(self, **result_kwargs):
-        if self._watcher:
-            self._notify(result_kwargs)
-        else:
-            self._undelivered_results.append(result_kwargs)
+class ShareFinder(service.MultiService):
+    def __init__(self, storage_broker, storage_index,
+                 consumer, max_outstanding_requests=10):
+        service.MultiService.__init__(self)
+        self._servers = storage_broker.get_servers_for_index(storage_index)
+        self.consumer = consumer
+        self.max_outstanding = max_outstanding_requests
+
+        self._hungry = False
+
+        self._commonshares = {} # shnum to CommonShare instance
+        self.undelivered_shares = []
+        self.pending_requests = set()
+
+    # called by our parent CiphertextDownloader
+    def hungry(self):
+        self._hungry = True
+        eventually(self.loop)
+
+    # internal methods
+    def loop(self):
+        if not self._hungry:
+            return
+        if self.undelivered_shares:
+            sh = self.undelivered_shares.pop(0)
+            # they will call hungry() again if they want more
+            self._hungry = False
+            eventually(self.consumer.got_shares, [sh])
+            return
+        if len(self.pending_requests) >= self.max_outstanding_requests:
+            # cannot send more requests, must wait for some to retire
+            return
+
+        server = None
+        try:
+            if self._servers:
+                server = self._servers.next()
+        except StopIteration:
+            self._servers = None
+
+        if not server and not self.pending_requests:
+            # we've run out of servers (so we can't send any more requests),
+            # and we have nothing in flight. No further progress can be made.
+            # They are destined to remain hungry.
+            self.consumer.no_more_shares()
+            self.stopService()
+            # TODO: stop running
+            return
+
+        self.pending_requests.add(self.send_request(server))
+        return
+
+    def send_request(self, server):
+        ...
 
-    def _notify(self, result_kwargs):
-        o, watcher_kwargs = self._watcher
-        kwargs = dict(result_kwargs)
-        kwargs.update(watcher_kwargs)
-        eventually(o, **kwargs)
+    def got_response(self, response, req, server):
+        self.pending_requests.discard(req)
+        if good:
+            rref = response # the BucketReader
+            if shnum not in self._commonshares:
+                self._commonshares[shnum] = CommonShare(...)
+            s = Share(rref, verifycap, self._commonshares[shnum], node)
+            self.undelivered_shares.append(s)
+            eventually(self.loop)
 
 class CiphertextDownloader(service.MultiService):
-    def __init__(self):
+    def __init__(self, node, storage_broker, secret_holder, history):
         service.MultiService.__init__(self)
         self._active_segment_fetcher = None
+        self._node = node
+        self._storage_broker = storage_broker
+        self._secret_holder = secret_holder
+        self._history = history
+        self._sharefinder = ShareFinder(storage_broker, storage_index, self)
+        self._sharefinder.setServiceParent(self)
 
+    # called by a Reader
+    def want_segment(..
+
+    # called by.. ??
     def start_new_segment(self, segnum):
-        sf = SegmentFetcher(segnum)
+        # one segment at a time
         assert not self._active_segment_fetcher
+        sf = SegmentFetcher(segnum)
         self._active_segment_fetcher = sf
         sf.setServiceParent(self)
         active_shares = [s for s in self._shares if s.not_dead()]
@@ -1206,6 +622,255 @@ class CiphertextDownloader(service.MultiService):
 
 
 
+class Segmentation:
+    """I am responsible for a single offset+size read of the file. I handle
+    segmentation: I figure out which segments are necessary, request them
+    (from my CiphertextDownloader) in order, and trim the segments down to
+    match the offset+size span. I use the Producer/Consumer interface to only
+    request one segment at a time.
+    """
+    implements(IPushProducer)
+    def __init__(self, node, offset, size, consumer):
+        self._node = node
+        self._hungry = False
+        self._offset = offset
+        self._size = size
+        self._consumer = consumer
+
+    def start(self):
+        self._alive = True
+        self._deferred = defer.Deferred()
+        self._node.get_segment_size().addCallbacks(self._got_segment_size,
+                                                   self._got_segsize_error)
+        # the process doesn't actually start until we get the segment size
+        return self._deferred
+
+    def _got_segment_size(self, segment_size):
+        # now we really start
+        self._needed = self._compute_segnums(segment_size)
+        self._loop()
+
+    def _compute_segnums(self, segsize):
+        # now that we know the file's segsize, what segments (and which
+        # ranges of each) will we need?
+        size = self._size
+        offset = self._offset
+        while size:
+            assert size >= 0
+            this_seg_num = int(offset / segsize)
+            this_seg_offset = offset - (seg_num*segsize)
+            this_seg_size = min(size, segsize-seg_offset)
+            size -= this_seg_size
+            if size:
+                offset += this_seg_size
+            yield (this_seg_num, this_seg_offset, this_seg_size)
+
+    def _loop(self):
+        try:
+            (segnum, segoff, segsize) = self._needed.next()
+        except StopIteration:
+            # done!
+            self._alive = False
+            self._hungry = False
+            self._consumer.unregisterProducer()
+            self._deferred.callback(self._consumer)
+            return
+        (d,self._cancel) = self._node.get_segment(segnum)
+        d.addCallback(self._got_segment, segoff, segsize)
+        d.addErrback(self._got_segment_error)
+
+    def _got_segment(self, segment, segoff, segsize):
+        assert segoff+segsize < len(segment)
+        if segoff != 0 or segsize != len(segment):
+            segment = segment[segoff:segoff+segsize]
+        self._consumer.write(segment)
+        eventually(self._loop)
+
+    def _got_segment_error(self, f):
+        self.error(f)
+
+    def stopProducing(self):
+        self._hungry = False
+        self._alive = False
+        # cancel any outstanding segment request
+        if self._cancel:
+            self._cancel()
+            self._cancel = None
+    def pauseProducing(self):
+        self._hungry = False
+    def resumeProducing(self):
+        self._hungry = True
+
+    def error(self, f):
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
+
+class Cancel:
+    def __init__(self, parent):
+        self._parent = parent
+        self._cancelled = False
+    def cancel(self):
+        self._cancelled = True
+        self._parent._cancel(self)
+
+class CiphertextFileNode:
+    # Share._node points to me
+    def __init__(self, verifycap, storage_broker, secret_holder,
+                 download_manager, history):
+        assert isinstance(verifycap, CHKFileVerifierURI)
+        self.u = verifycap
+        self._downloader = CiphertextDownloader(self, storage_broker,
+                                                secret_holder, history)
+        self._downloader.setServiceParent(download_manager)
+        # the rule is: only send network requests if you're active
+        # (self.running is True). You can do eventual-sends any time. This
+        # rule should mean that once stopService()+flushEventualQueue()
+        # fires, everything will be done.
+
+        self.share_hash_tree = IncompleteHashTree(self.u.total_shares)
+
+        # filled in when we parse a valid UEB
+        self.have_UEB = False
+        self.num_segments = None
+        self.segment_size = None
+        self.tail_data_size = None
+        self.tail_segment_size = None
+        self.block_size = None
+        self.share_size = None
+        self.ciphertext_hash_tree = None # size depends on num_segments
+        self.ciphertext_hash = None # flat hash
+
+        # things to track callers that want data
+        self._segsize_observers = OneShotObserverList()
+        self._segment_requests = [] # (segnum, d, cancel_handle)
+
+    # things called by the Segmentation object used to transform
+    # arbitrary-sized read() calls into quantized segment fetches
+
+    def get_segment_size(self):
+        """I return a Deferred that fires with the segment_size used by this
+        file."""
+        return self._segsize_observers.when_fired()
+
+    def get_segment(self, segnum):
+        """Begin downloading a segment. I return a tuple (d, c): 'd' is a
+        Deferred that fires with the data when the desired segment is
+        available, and c is an object on which c.cancel() can be called to
+        disavow interest in the segment (after which 'd' will never fire).
+
+        You probably need to know the segment size before calling this,
+        unless you want the first few bytes of the file.
+        """
+        d = defer.Deferred()
+        c = Cancel(self)
+        self._segment_requests.append( (segnum, d, c) )
+        eventually(self._loop)
+        return (d, c)
+
+    # things called by our client, either a filenode user or an
+    # ImmutableFileNode wrapper
+
+    def read(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data."""
+        # tolerate concurrent operations: each gets its own Reader
+        if size is None:
+            size = self._size - offset
+        s = Segmentation(self, offset, size, consumer)
+        d = s.start()
+        return d
+
+    # things called by our Share instances
+
+    def validate_UEB(self, UEB_s):
+        h = hashutil.uri_extension_hash(UEB_s)
+        try:
+            if h != self._verifycap.uri_extension_hash:
+                ERR()
+                return False
+            d = uri.unpack_extension(data) # could raise parse error
+            self._parse_UEB(self, d) # sets self._stuff, raise error
+            self.have_UEB = True
+            self._segsize_observers.fire(self.segment_size)
+            return True
+        except (errors..):
+            return False
+
+
+    def _parse_UEB(self, d):
+        self.share_size = mathutil.div_ceil(self._verifycap.size,
+                                            self._verifycap.needed_shares)
+
+        # First, things that we really need to learn from the UEB:
+        # segment_size, crypttext_root_hash, and share_root_hash.
+        self.segment_size = d['segment_size']
+        for r in self._readers:
+            r.set_segment_size(self.segment_size)
+
+        self.block_size = mathutil.div_ceil(self._segsize, self._needed_shares)
+        self.num_segments = mathutil.div_ceil(self._size, self.segment_size)
+
+        self.tail_data_size = self._size % self.segment_size
+        if self.tail_data_size == 0:
+            self.tail_data_size = self.segment_size
+        # padding for erasure code
+        self.tail_segment_size = mathutil.next_multiple(self.tail_data_size,
+                                                        self._needed_shares)
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B.
+        self.ciphertext_hash_tree = IncompleteHashTree(self.num_segments)
+        self.ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
+
+        self.share_hash_tree.set_hashes({0: d['share_root_hash']})
+
+        # Next: things that are optional and not redundant: crypttext_hash.
+        # We only pull this from the first UEB that we see.
+        if 'crypttext_hash' in d:
+            if len(d["crypttext_hash"]) == hashutil.CRYPTO_VAL_SIZE:
+                self.ciphertext_hash = d['crypttext_hash']
+            else:
+                log.msg("ignoring bad-length UEB[crypttext_hash], "
+                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
+                                                   hashutil.CRYPTO_VAL_SIZE),
+                        umid="oZkGLA", level=log.WEIRD)
+
+        # we ignore all of the redundant fields when downloading. The
+        # Verifier uses a different code path which does not ignore them.
+
+
+    def process_share_hashes(self, share_hashes):
+        self.share_hash_tree.x(share_hashes) # XXX
+        return True
+
+    # other stuff
+    def _loop(self):
+        pass
+
+    def _got_segment(self, segnum, segment):
+        retire = [(d,c) for (segnum0, d, c) in self._segment_requests
+                  if segnum0 == segnum]
+        self._segment_requests = [t for t in self._segment_requests
+                                  if t[0] != segnum]
+        for (d,c) in retire:
+            eventually(self._deliver, d, c, segment)
+
+    def _cancel(self, c):
+        self._segment_requests = [t for t in self._segment_requests
+                                  if t[2] != c]
+        # TODO: update active
+
+    def _deliver(self, d, c, segment):
+        # this method exists to handle cancel() that occurs between
+        # _got_segment and _deliver
+        if not c._cancelled:
+            d.callback(segment)
+
 class DecryptingConsumer:
     """I sit between a CiphertextDownloader (which acts as a Producer) and
     the real Consumer, decrypting everything that passes by. The real
@@ -1242,18 +907,40 @@ class DecryptingConsumer:
         plaintext = self._decryptor.process(ciphertext)
         self._consumer.write(plaintext)
 
+class ImmutableFileNode:
+    # I wrap a CiphertextFileNode with a decryption key
+    def __init__(self, filecap, storage_broker, secret_holder, downloader,
+                 history):
+        assert isinstance(filecap, CHKFileURI)
+        verifycap = filecap.get_verify_cap()
+        self._cnode = CiphertextFileNode(verifycap, storage_broker,
+                                         secret_holder, downloader, history)
+        assert isinstance(filecap, CHKFileURI)
+        self.u = filecap
 
-class FileNode:
     def read(self, consumer, offset=0, size=None):
         decryptor = DecryptingConsumer(consumer, self._readkey, offset)
-        return self._downloader.read(decryptor, offset, size)
+        return self._cnode.read(decryptor, offset, size)
+
+
+# download-process lifetime management
+#
+#  I like the idea of having each FileNode be a service child of the
+#  Downloader (or some filenode-parent), or at least having the downloader
+#  component of each node be parented to a service somewhere. That way, when
+#  the clientnode shuts down (as in unit tests), we can turn off all work in
+#  progress.
+#
+#  However, this complicates reference management. If a caller creates a
+#  filenode, uses it up, and then discards it, we need it to go away, without
+#  requiring the caller to explicitly drop it. That means the service-parent
+#  must hold a weakref instead.
+#
+#  Maybe create a special object to act as the parent of these filenodes. It
+#  will use a WeakSet (i.e. WeakKeyDictionary with value=None). In its
+#  stopService, it will call stopService (and wait) for all existing
+#  children. When
+
+# make downloaders register with an Inhibitor(Service), which maintains a
+# weakset, and tells them all to shutdown when it gets a stopService.
 
-    # things called by our Share instances
-    def validate_UEB(self, UEB_s):
-        self.UEB = UEB
-        ...
-        return True
-
-    def process_share_hashes(self, share_hashes):
-        self.share_hash_tree = x
-        return True
diff --git a/src/allmydata/immutable/download2_off.py b/src/allmydata/immutable/download2_off.py
new file mode 100755
index 0000000..3871ec5
--- /dev/null
+++ b/src/allmydata/immutable/download2_off.py
@@ -0,0 +1,647 @@
+#! /usr/bin/python
+
+# known (shnum,Server) pairs are sorted into a list according to
+# desireability. This sort is picking a winding path through a matrix of
+# [shnum][server]. The goal is to get diversity of both shnum and server.
+
+# The initial order is:
+#  find the lowest shnum on the first server, add it
+#  look at the next server, find the lowest shnum that we don't already have
+#   if any
+#  next server, etc, until all known servers are checked
+#  now look at servers that we skipped (because ...
+
+# Keep track of which block requests are outstanding by (shnum,Server). Don't
+# bother prioritizing "validated" shares: the overhead to pull the share hash
+# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
+# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
+# 832 bytes). Each time a block request is sent, also request any necessary
+# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
+# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
+
+# For each outstanding segread, walk the list and send requests (skipping
+# outstanding shnums) until requests for k distinct shnums are in flight. If
+# we can't do that, ask for more. If we get impatient on a request, find the
+# first non-outstanding
+
+# start with the first Share in the list, and send a request. Then look at
+# the next one. If we already have a pending request for the same shnum or
+# server, push that Share down onto the fallback list and try the next one,
+# etc. If we run out of non-fallback shares, use the fallback ones,
+# preferring shnums that we don't have outstanding requests for (i.e. assume
+# that all requests will complete). Do this by having a second fallback list.
+
+# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
+# per file, not thousands.
+
+# actually, don't bother sorting the initial list. Append Shares as the
+# responses come back, that will put the fastest servers at the front of the
+# list, and give a tiny preference to servers that are earlier in the
+# permuted order.
+
+# more ideas:
+#  sort shares by:
+#   1: number of roundtrips needed to get some data
+#   2: share number
+#   3: ms of RTT delay
+# maybe measure average time-to-completion of requests, compare completion
+# time against that, much larger indicates congestion on the server side
+# or the server's upstream speed is less than our downstream. Minimum
+# time-to-completion indicates min(our-downstream,their-upstream). Could
+# fetch shares one-at-a-time to measure that better.
+
+# when should we risk duplicate work and send a new request?
+
+def walk(self):
+    shares = sorted(list)
+    oldshares = copy(shares)
+    outstanding = list()
+    fallbacks = list()
+    second_fallbacks = list()
+    while len(outstanding.nonlate.shnums) < k: # need more requests
+        while oldshares:
+            s = shares.pop(0)
+            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
+                fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # must use fallback list. Ask for more servers while we're at it.
+        ask_for_more_servers()
+        while fallbacks:
+            s = fallbacks.pop(0)
+            if s.shnum in outstanding.shnums:
+                # assume that the outstanding requests will complete, but
+                # send new requests for other shnums to existing servers
+                second_fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # if we get here, we're being forced to send out multiple queries per
+        # share. We've already asked for more servers, which might help. If
+        # there are no late outstanding queries, then duplicate shares won't
+        # help. Don't send queries for duplicate shares until some of the
+        # queries are late.
+        if outstanding.late:
+            # we're allowed to try any non-outstanding share
+            while second_fallbacks:
+                pass
+    newshares = outstanding + fallbacks + second_fallbacks + oldshares
+        
+
+class Server:
+    """I represent an abstract Storage Server. One day, the StorageBroker
+    will return instances of me. For now, the StorageBroker returns (peerid,
+    RemoteReference) tuples, and this code wraps a Server instance around
+    them.
+    """
+    def __init__(self, peerid, ss):
+        self.peerid = peerid
+        self.remote = ss
+        self._remote_buckets = {} # maps shnum to RIBucketReader
+        # TODO: release the bucket references on shares that we no longer
+        # want. OTOH, why would we not want them? Corruption?
+
+    def send_query(self, storage_index):
+        """I return a Deferred that fires with a set of shnums. If the server
+        had shares available, I will retain the RemoteReferences to its
+        buckets, so that get_data(shnum, range) can be called later."""
+        d = self.remote.callRemote("get_buckets", self.storage_index)
+        d.addCallback(self._got_response)
+        return d
+
+    def _got_response(self, r):
+        self._remote_buckets = r
+        return set(r.keys())
+
+def incidentally(res, f, *args, **kwargs):
+    """Add me to a Deferred chain like this:
+     d.addBoth(incidentally, func, arg)
+    and I'll behave as if you'd added the following function:
+     def _(res):
+         func(arg)
+         return res
+    This is useful if you want to execute an expression when the Deferred
+    fires, but don't care about its value.
+    """
+    f(*args, **kwargs)
+    return res
+
+class ShareOnAServer:
+    """I represent one instance of a share, known to live on a specific
+    server. I am created every time a server responds affirmatively to a
+    do-you-have-block query."""
+
+    def __init__(self, shnum, server):
+        self._shnum = shnum
+        self._server = server
+        self._block_hash_tree = None
+
+    def cost(self, segnum):
+        """I return a tuple of (roundtrips, bytes, rtt), indicating how
+        expensive I think it would be to fetch the given segment. Roundtrips
+        indicates how many roundtrips it is likely to take (one to get the
+        data and hashes, plus one to get the offset table and UEB if this is
+        the first segment we've ever fetched). 'bytes' is how many bytes we
+        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
+        seconds for a trivial request. The downloading algorithm will compare
+        costs to decide which shares should be used."""
+        # the most significant factor here is roundtrips: a Share for which
+        # we already have the offset table is better to than a brand new one
+
+    def max_bandwidth(self):
+        """Return a float, indicating the highest plausible bytes-per-second
+        that I've observed coming from this share. This will be based upon
+        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
+        can we used to estimate the server's upstream bandwidth. Clearly this
+        is only accurate if a share is retrieved with no contention for
+        either the upstream, downstream, or middle of the connection, but it
+        may still serve as a useful metric for deciding which servers to pull
+        from."""
+
+    def get_segment(self, segnum):
+        """I return a Deferred that will fire with the segment data, or
+        errback."""
+
+class NativeShareOnAServer(ShareOnAServer):
+    """For tahoe native (foolscap) servers, I contain a RemoteReference to
+    the RIBucketReader instance."""
+    def __init__(self, shnum, server, rref):
+        ShareOnAServer.__init__(self, shnum, server)
+        self._rref = rref # RIBucketReader
+
+class Share:
+    def __init__(self, shnum):
+        self._shnum = shnum
+        # _servers are the Server instances which appear to hold a copy of
+        # this share. It is populated when the ValidShare is first created,
+        # or when we receive a get_buckets() response for a shnum that
+        # already has a ValidShare instance. When we lose the connection to a
+        # server, we remove it.
+        self._servers = set()
+        # offsets, UEB, and share_hash_tree all live in the parent.
+        # block_hash_tree lives here.
+        self._block_hash_tree = None
+
+        self._want
+
+    def get_servers(self):
+        return self._servers
+
+
+    def get_block(self, segnum):
+        # read enough data to obtain a single validated block
+        if not self.have_offsets:
+            # we get the offsets in their own read, since they tell us where
+            # everything else lives. We must fetch offsets for each share
+            # separately, since they aren't directly covered by the UEB.
+            pass
+        if not self.parent.have_ueb:
+            # use _guessed_segsize to make a guess about the layout, so we
+            # can fetch both the offset table and the UEB in the same read.
+            # This also requires making a guess about the presence or absence
+            # of the plaintext_hash_tree. Oh, and also the version number. Oh
+            # well.
+            pass
+
+class CiphertextDownloader:
+    """I manage all downloads for a single file. I operate a state machine
+    with input events that are local read() requests, responses to my remote
+    'get_bucket' and 'read_bucket' messages, and connection establishment and
+    loss. My outbound events are connection establishment requests and bucket
+    read requests messages.
+    """
+    # eventually this will merge into the FileNode
+    ServerClass = Server # for tests to override
+
+    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
+                 shutdowner):
+        # values we get from the filecap
+        self._storage_index = si = storage_index
+        self._ueb_hash = ueb_hash
+        self._size = size
+        self._needed_shares = k
+        self._total_shares = N
+        self._share_hash_tree = IncompleteHashTree(self._total_shares)
+        # values we discover when we first fetch the UEB
+        self._ueb = None # is dict after UEB fetch+validate
+        self._segsize = None
+        self._numsegs = None
+        self._blocksize = None
+        self._tail_segsize = None
+        self._ciphertext_hash = None # optional
+        # structures we create when we fetch the UEB, then continue to fill
+        # as we download the file
+        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
+        self._ciphertext_hash_tree = None
+
+        # values we learn as we download the file
+        self._offsets = {} # (shnum,Server) to offset table (dict)
+        self._block_hash_tree = {} # shnum to IncompleteHashTree
+        # other things which help us
+        self._guessed_segsize = min(128*1024, size)
+        self._active_share_readers = {} # maps shnum to Reader instance
+        self._share_readers = [] # sorted by preference, best first
+        self._readers = set() # set of Reader instances
+        self._recent_horizon = 10 # seconds
+
+        # 'shutdowner' is a MultiService parent used to cancel all downloads
+        # when the node is shutting down, to let tests have a clean reactor.
+
+        self._init_available_servers()
+        self._init_find_enough_shares()
+
+    # _available_servers is an iterator that provides us with Server
+    # instances. Each time we pull out a Server, we immediately send it a
+    # query, so we don't need to keep track of who we've sent queries to.
+
+    def _init_available_servers(self):
+        self._available_servers = self._get_available_servers()
+        self._no_more_available_servers = False
+
+    def _get_available_servers(self):
+        """I am a generator of servers to use, sorted by the order in which
+        we should query them. I make sure there are no duplicates in this
+        list."""
+        # TODO: make StorageBroker responsible for this non-duplication, and
+        # replace this method with a simple iter(get_servers_for_index()),
+        # plus a self._no_more_available_servers=True
+        seen = set()
+        sb = self._storage_broker
+        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
+            if peerid not in seen:
+                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
+                seen.add(peerid)
+        self._no_more_available_servers = True
+
+    # this block of code is responsible for having enough non-problematic
+    # distinct shares/servers available and ready for download, and for
+    # limiting the number of queries that are outstanding. The idea is that
+    # we'll use the k fastest/best shares, and have the other ones in reserve
+    # in case those servers stop responding or respond too slowly. We keep
+    # track of all known shares, but we also keep track of problematic shares
+    # (ones with hash failures or lost connections), so we can put them at
+    # the bottom of the list.
+
+    def _init_find_enough_shares(self):
+        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
+        # where viable (but not yet validated) shares are located. Each
+        # get_bucket() response adds to this map, each act of validation
+        # removes from it.
+        self._sharemap = DictOfSets()
+
+        # _sharemap maps shnum to set of Servers, and remembers where viable
+        # shares are located. Each get_bucket() response adds to this map,
+        # each hash failure or disconnect removes from it. (TODO: if we
+        # disconnect but reconnect later, we should be allowed to re-query).
+        self._sharemap = DictOfSets()
+
+        # _problem_shares is a set of (shnum, Server) tuples, and
+
+        # _queries_in_flight maps a Server to a timestamp, which remembers
+        # which servers we've sent queries to (and when) but have not yet
+        # heard a response. This lets us put a limit on the number of
+        # outstanding queries, to limit the size of the work window (how much
+        # extra work we ask servers to do in the hopes of keeping our own
+        # pipeline filled). We remove a Server from _queries_in_flight when
+        # we get an answer/error or we finally give up. If we ever switch to
+        # a non-connection-oriented protocol (like UDP, or forwarded Chord
+        # queries), we can use this information to retransmit any query that
+        # has gone unanswered for too long.
+        self._queries_in_flight = dict()
+
+    def _count_recent_queries_in_flight(self):
+        now = time.time()
+        recent = now - self._recent_horizon
+        return len([s for (s,when) in self._queries_in_flight.items()
+                    if when > recent])
+
+    def _find_enough_shares(self):
+        # goal: have 2*k distinct not-invalid shares available for reading,
+        # from 2*k distinct servers. Do not have more than 4*k "recent"
+        # queries in flight at a time.
+        if (len(self._sharemap) >= 2*self._needed_shares
+            and len(self._sharemap.values) >= 2*self._needed_shares):
+            return
+        num = self._count_recent_queries_in_flight()
+        while num < 4*self._needed_shares:
+            try:
+                s = self._available_servers.next()
+            except StopIteration:
+                return # no more progress can be made
+            self._queries_in_flight[s] = time.time()
+            d = s.send_query(self._storage_index)
+            d.addBoth(incidentally, self._queries_in_flight.discard, s)
+            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
+                                           for shnum in shnums],
+                           lambda f: self._query_error(f, s))
+            d.addErrback(self._error)
+            d.addCallback(self._reschedule)
+            num += 1
+
+    def _query_error(self, f, s):
+        # a server returned an error, log it gently and ignore
+        level = log.WEIRD
+        if f.check(DeadReferenceError):
+            level = log.UNUSUAL
+        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
+                failure=f, level=level, umid="3uuBUQ")
+
+    # this block is responsible for turning known shares into usable shares,
+    # by fetching enough data to validate their contents.
+
+    # UEB (from any share)
+    # share hash chain, validated (from any share, for given shnum)
+    # block hash (any share, given shnum)
+
+    def _got_ueb(self, ueb_data, share):
+        if self._ueb is not None:
+            return
+        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
+            share.error("UEB hash does not match")
+            return
+        d = uri.unpack_extension(ueb_data)
+        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
+
+
+        # There are several kinds of things that can be found in a UEB.
+        # First, things that we really need to learn from the UEB in order to
+        # do this download. Next: things which are optional but not redundant
+        # -- if they are present in the UEB they will get used. Next, things
+        # that are optional and redundant. These things are required to be
+        # consistent: they don't have to be in the UEB, but if they are in
+        # the UEB then they will be checked for consistency with the
+        # already-known facts, and if they are inconsistent then an exception
+        # will be raised. These things aren't actually used -- they are just
+        # tested for consistency and ignored. Finally: things which are
+        # deprecated -- they ought not be in the UEB at all, and if they are
+        # present then a warning will be logged but they are otherwise
+        # ignored.
+
+        # First, things that we really need to learn from the UEB:
+        # segment_size, crypttext_root_hash, and share_root_hash.
+        self._segsize = d['segment_size']
+
+        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
+        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
+
+        self._tail_segsize = self._size % self._segsize
+        if self._tail_segsize == 0:
+            self._tail_segsize = self._segsize
+        # padding for erasure code
+        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
+                                                    self._needed_shares)
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B.
+        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
+        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
+
+        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
+
+
+        # Next: things that are optional and not redundant: crypttext_hash
+        if 'crypttext_hash' in d:
+            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
+                self._ciphertext_hash = d['crypttext_hash']
+            else:
+                log.msg("ignoring bad-length UEB[crypttext_hash], "
+                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
+                                                   hashutil.CRYPTO_VAL_SIZE),
+                        umid="oZkGLA", level=log.WEIRD)
+
+        # we ignore all of the redundant fields when downloading. The
+        # Verifier uses a different code path which does not ignore them.
+
+        # finally, set self._ueb as a marker that we don't need to request it
+        # anymore
+        self._ueb = d
+
+    def _got_share_hashes(self, hashes, share):
+        assert isinstance(hashes, dict)
+        try:
+            self._share_hash_tree.set_hashes(hashes)
+        except (IndexError, BadHashError, NotEnoughHashesError), le:
+            share.error("Bad or missing hashes")
+            return
+
+    #def _got_block_hashes(
+
+    def _init_validate_enough_shares(self):
+        # _valid_shares maps shnum to ValidatedShare instances, and is
+        # populated once the block hash root has been fetched and validated
+        # (which requires any valid copy of the UEB, and a valid copy of the
+        # share hash chain for each shnum)
+        self._valid_shares = {}
+
+        # _target_shares is an ordered list of ReadyShare instances, each of
+        # which is a (shnum, server) tuple. It is sorted in order of
+        # preference: we expect to get the fastest response from the
+        # ReadyShares at the front of the list. It is also sorted to
+        # distribute the shnums, so that fetching shares from
+        # _target_shares[:k] is likely (but not guaranteed) to give us k
+        # distinct shares. The rule is that we skip over entries for blocks
+        # that we've already received, limit the number of recent queries for
+        # the same block, 
+        self._target_shares = []
+
+    def _validate_enough_shares(self):
+        # my goal is to have at least 2*k distinct validated shares from at
+        # least 2*k distinct servers
+        valid_share_servers = set()
+        for vs in self._valid_shares.values():
+            valid_share_servers.update(vs.get_servers())
+        if (len(self._valid_shares) >= 2*self._needed_shares
+            and len(self._valid_share_servers) >= 2*self._needed_shares):
+            return
+        #for 
+
+    def _reschedule(self, _ign):
+        # fire the loop again
+        if not self._scheduled:
+            self._scheduled = True
+            eventually(self._loop)
+
+    def _loop(self):
+        self._scheduled = False
+        # what do we need?
+
+        self._find_enough_shares()
+        self._validate_enough_shares()
+
+        if not self._ueb:
+            # we always need a copy of the UEB
+            pass
+
+    def _error(self, f):
+        # this is an unexpected error: a coding bug
+        log.err(f, level=log.UNUSUAL)
+            
+
+
+# using a single packed string (and an offset table) may be an artifact of
+# our native storage server: other backends might allow cheap multi-part
+# files (think S3, several buckets per share, one for each section).
+
+# find new names for:
+#  data_holder
+#  Share / Share2  (ShareInstance / Share? but the first is more useful)
+
+class IShare(Interface):
+    """I represent a single instance of a single share (e.g. I reference the
+    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
+    This interface is used by SegmentFetcher to retrieve validated blocks.
+    """
+    def get_block(segnum):
+        """Return an Observer2, which will be notified with the following
+        events:
+         state=COMPLETE, block=data (terminal): validated block data
+         state=OVERDUE (non-terminal): we have reason to believe that the
+                                       request might have stalled, or we
+                                       might just be impatient
+         state=CORRUPT (terminal): the data we received was corrupt
+         state=DEAD (terminal): the connection has failed
+        """
+
+
+# it'd be nice if we receive the hashes before the block, or just
+# afterwards, so we aren't stuck holding on to unvalidated blocks
+# that we can't process. If we guess the offsets right, we can
+# accomplish this by sending the block request after the metadata
+# requests (by keeping two separate requestlists), and have a one RTT
+# pipeline like:
+#  1a=metadata, 1b=block
+#  1b->process+deliver : one RTT
+
+# But if we guess wrong, and fetch the wrong part of the block, we'll
+# have a pipeline that looks like:
+#  1a=wrong metadata, 1b=wrong block
+#  1a->2a=right metadata,2b=right block
+#  2b->process+deliver
+# which means two RTT and buffering one block (which, since we'll
+# guess the segsize wrong for everything, means buffering one
+# segment)
+
+# if we start asking for multiple segments, we could get something
+# worse:
+#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
+#  1a->2a=right metadata,2b=right block0,2c=right block1, .
+#  2b->process+deliver
+
+# which means two RTT but fetching and buffering the whole file
+# before delivering anything. However, since we don't know when the
+# other shares are going to arrive, we need to avoid having more than
+# one block in the pipeline anyways. So we shouldn't be able to get
+# into this state.
+
+# it also means that, instead of handling all of
+# self._requested_blocks at once, we should only be handling one
+# block at a time: one of the requested block should be special
+# (probably FIFO). But retire all we can.
+
+    # this might be better with a Deferred, using COMPLETE as the success
+    # case and CORRUPT/DEAD in an errback, because that would let us hold the
+    # 'share' and 'shnum' arguments locally (instead of roundtripping them
+    # through Share.send_request). But that OVERDUE is not terminal. So I
+    # want a new sort of callback mechanism, with the extra-argument-passing
+    # aspects of Deferred, but without being so one-shot. Is this a job for
+    # Observer? No, it doesn't take extra arguments. So this uses Observer2.
+
+
+class Reader:
+    """I am responsible for a single offset+size read of the file. I handle
+    segmentation: I figure out which segments are necessary, request them
+    (from my CiphertextDownloader) in order, and trim the segments down to
+    match the offset+size span. I use the Producer/Consumer interface to only
+    request one segment at a time.
+    """
+    implements(IPushProducer)
+    def __init__(self, consumer, offset, size):
+        self._needed = []
+        self._consumer = consumer
+        self._hungry = False
+        self._offset = offset
+        self._size = size
+        self._segsize = None
+    def start(self):
+        self._alive = True
+        self._deferred = defer.Deferred()
+        # the process doesn't actually start until set_segment_size()
+        return self._deferred
+
+    def set_segment_size(self, segsize):
+        if self._segsize is not None:
+            return
+        self._segsize = segsize
+        self._compute_segnums()
+
+    def _compute_segnums(self, segsize):
+        # now that we know the file's segsize, what segments (and which
+        # ranges of each) will we need?
+        size = self._size
+        offset = self._offset
+        while size:
+            assert size >= 0
+            this_seg_num = int(offset / self._segsize)
+            this_seg_offset = offset - (seg_num*self._segsize)
+            this_seg_size = min(size, self._segsize-seg_offset)
+            size -= this_seg_size
+            if size:
+                offset += this_seg_size
+            yield (this_seg_num, this_seg_offset, this_seg_size)
+
+    def get_needed_segments(self):
+        return set([segnum for (segnum, off, size) in self._needed])
+
+
+    def stopProducing(self):
+        self._hungry = False
+        self._alive = False
+        # TODO: cancel the segment requests
+    def pauseProducing(self):
+        self._hungry = False
+    def resumeProducing(self):
+        self._hungry = True
+    def add_segment(self, segnum, offset, size):
+        self._needed.append( (segnum, offset, size) )
+    def got_segment(self, segnum, segdata):
+        """Return True if this schedule has more to go, or False if it is
+        done."""
+        assert self._needed[0][segnum] == segnum
+        (_ign, offset, size) = self._needed.pop(0)
+        data = segdata[offset:offset+size]
+        self._consumer.write(data)
+        if not self._needed:
+            # we're done
+            self._alive = False
+            self._hungry = False
+            self._consumer.unregisterProducer()
+            self._deferred.callback(self._consumer)
+    def error(self, f):
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
+
+
+
+class x:
+    def OFFread(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data."""
+        # tolerate concurrent operations: each gets its own Reader
+        if size is None:
+            size = self._size - offset
+        r = Reader(consumer, offset, size)
+        self._readers.add(r)
+        d = r.start()
+        if self.segment_size is not None:
+            r.set_segment_size(self.segment_size)
+            # TODO: if we can't find any segments, and thus never get a
+            # segsize, tell the Readers to give up
+        return d
diff --git a/src/allmydata/immutable/download2_util.py b/src/allmydata/immutable/download2_util.py
new file mode 100755
index 0000000..9786662
--- /dev/null
+++ b/src/allmydata/immutable/download2_util.py
@@ -0,0 +1,33 @@
+#! /usr/bin/python
+
+class Observer2:
+    """A simple class to distribute multiple events to a single subscriber.
+    It accepts arbitrary kwargs, but no posargs."""
+    def __init__(self):
+        self._watcher = None
+        self._undelivered_results = []
+
+    def subscribe(self, observer, **watcher_kwargs):
+        self._watcher = (observer, watcher_kwargs)
+        while self._undelivered_results:
+            self._notify(self._undelivered_results.pop(0))
+
+    def notify(self, **result_kwargs):
+        if self._watcher:
+            self._notify(result_kwargs)
+        else:
+            self._undelivered_results.append(result_kwargs)
+
+    def _notify(self, result_kwargs):
+        o, watcher_kwargs = self._watcher
+        kwargs = dict(result_kwargs)
+        kwargs.update(watcher_kwargs)
+        eventually(o, **kwargs)
+
+class DictOfSets:
+    def add(self, key, value): pass
+    def values(self): # return set that merges all value sets
+        r = set()
+        for key in self:
+            r.update(self[key])
+        return r

commit b08c4b8d048a925ac0f699aedc3c61fd9522ccee
Author: Brian Warner <warner@lothar.com>
Date:   Sat Jan 30 23:42:11 2010 -0800

    WIP, split out satisfy() and desire() methods
---
 src/allmydata/immutable/download2.py |  231 ++++++++++++++++++++++------------
 1 files changed, 149 insertions(+), 82 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
index c4d7254..3480a9c 100644
--- a/src/allmydata/immutable/download2.py
+++ b/src/allmydata/immutable/download2.py
@@ -648,6 +648,14 @@ class IShare(Interface):
 # block at a time: one of the requested block should be special
 # (probably FIFO). But retire all we can.
 
+class Share2:
+    def process_block_hashes(self, block_hashes):
+        self.block_hash_tree.add_hashes(block_hashes)
+        return True
+    def process_block(self, block):
+        check_hash
+        return True
+
 class Share:
     # this is a specific implementation of IShare for tahoe's native storage
     # servers. A different backend would use a different class.
@@ -699,97 +707,149 @@ class Share:
         #  block hash chain (stash pieces in Share2)
         #  block data (ephemeral, given to client upon receipt)
 
-        segnum = None
+        # first process any information that we have the data for. Keep going
+        # until I can't get no satisfaction for the active segment.
+        oldsegnum = "always different than None"
+        newsegnum = self._active_segnum()
+        while newsegnum != oldsegnum:
+            oldsegnum = newsegnum
+            self._satisfaction() # might retire active segnum
+            newsegnum = self._active_segnum()
+        # then figure out what additional data we need
+        self._desire() # compute desire
+        # finally send out requests for whatever we need
+        self._request_needed() # express desire
+
+    def _active_segnum(self):
         if self._requested_blocks:
-            segnum, observers = self._requested_blocks[0]
+            return self._requested_blocks[0]
+        return None
 
-        # first part: process any information that we need and have the data
-        # for
+    def _active_segnum_and_observers(self):
+        if self._requested_blocks:
+            # we only retrieve information for one segment at a time, to
+            # minimize alacrity (first come, first served)
+            return self._requested_blocks[0]
+        return None, []
 
+    def _satisfaction(self):
+        segnum, observers = self._active_segnum_and_observers()
         if not self.actual_offsets:
             self._ask_for_offsets()
         fsize = self._fieldsize
         rdata = self._received_data
         share2 = self._share2
 
-        if self.actual_offsets:
-            o = self.actual_offsets
+        if not self.actual_offsets:
+            return
+        o = self.actual_offsets
 
-            # UEB
-            if self._file.UEB is None:
-                UEB_length_s = rdata.get(o["uri_extension"], fsize)
-                if UEB_length_s:
-                    UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
-                    UEB_s = rdata.get(o["uri_extension"]+fsize, UEB_length)
-                    if UEB_s:
-                        UEB = self._validate_UEB(UEB_s)
-                        if UEB is not None:
-                            self._file.UEB = UEB
-                            rdata.remove(o["uri_extension"]+fsize, UEB_length)
-                        # TODO: if this UEB was bad, we'll keep trying to
-                        # validate it over and over again. Only log.err on
-                        # the first one, or better yet skip all but the first
-
-            # share hash chain
-            if self._file.share_hash_tree.needed_hashes(self.shnum):
-                # the share hash chain is stored as (hashnum,hash) tuples, so
-                # you can't fetch just the pieces you need, because you don't
-                # know exactly where they are. So fetch everything, and parse
-                # the results later.
-                hashlen = o["uri_extension"] - o["share_hashes"]
-                assert hashlen % (2+HASH_SIZE) == 0
-                hashdata = rdata.get(o["share_hashes"], hashlen)
-                if hashdata:
-                    share_hashes = []
-                    for i in range(0, hashlen, 2+HASH_SIZE):
-                        hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
-                        hashvalue = hashdata[i+2:i+2+HASH_SIZE]
-                        share_hashes.append( (hashnum, hashvalue) )
-                    if self._process_share_hashes(share_hashes):
-                        # adds to self._file.share_hash_tree
-                        rdata.remove(o["share_hashes"], hashlen)
-
-            # block hash chain
-            if segnum is not None:
-                block_hashes = {}
-                for hashnum in share2.block_hash_tree.needed_hashes(segnum):
-                    hashdata = rdata.get(o["block_hashes"]+hashnum*HASH_SIZE,
-                                         HASH_SIZE)
-                    if hashdata:
-                        block_hashes[hashnum] = hashdata
-                # TODO: if we get the offsets wrong, we'll grab some of the
-                # desired hashes but not all. I don't think that
-                # IncompleteHashTree.set_hashes() is prepared to handle this:
-                # it wants all or nothing. To deal with it, I think we'd need
-                # an API that accepts partial hashes: we'd then use 'not
-                # needed_hashes()' as below to tell whether the hashtree is
-                # good or not.
-                share2.block_hash_tree.ADDSTUFF(block_hashes)
+        UEB = self._file.UEB
+        if UEB is None:
+            UEB = self._satisfy_UEB()
+        if UEB is None:
+            return
+
+        if self._file.share_hash_tree.needed_hashes(self.shnum):
+            self._satisfy_share_hash_tree()
+
+        if segnum is not None:
+            # block_hash_tree
+            needed_hashes = share2.block_hash_tree.needed_hashes(segnum)
+            if needed_hashes:
+                self._satisfy_block_hash_tree(needed_hashes)
 
             # data blocks
-            if (segnum is not None and
-                not share2.block_hash_tree.needed_hashes(segnum)):
-                segsize = self._file.UEB["segment_size"]
-                needed_shares = self._file.UEB["needed_shares"]
-                sharesize = mathutil.div_ceil(self._file.UEB["size"],
-                                              needed_shares)
-                blocksize = mathutil.div_ceil(segsize, needed_shares)
-                blockstart = o["data"] + segnum * blocksize
-                if blocknum < NUM_BLOCKS-1:
-                    blocklen = blocksize
-                else:
-                    blocklen = sharesize % blocksize
-                    if blocklen == 0:
-                        blocklen = blocksize
-                block = rdata.pop(blockstart, blocklen)
-                if block:
-                    PROCESS_BLOCK(block) # check hash
-                    assert self._requested_blocks[0][0] == segnum
-                    for o in observers:
-                        o.notify(block=block)
-                    self._requested_blocks.pop(0) # retired
-
-        # second part: figure out what we need data for
+            needed_hashes = share2.block_hash_tree.needed_hashes(segnum)
+            if not needed_hashes:
+                self._satisfy_data_block(segnum, observers)
+
+    def _satisfy_UEB(self):
+        fsize = self._fieldsize
+        rdata = self._received_data
+        UEB_length_s = rdata.get(o["uri_extension"], fsize)
+        if not UEB_length_s:
+            return None
+        UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
+        UEB_s = rdata.get(o["uri_extension"]+fsize, UEB_length)
+        if not UEB_s:
+            return None
+        ok = self._file.validate_UEB(UEB_s)
+        if not ok:
+            return None
+        # TODO: if this UEB was bad, we'll keep trying to
+        # validate it over and over again. Only log.err on
+        # the first one, or better yet skip all but the first
+        rdata.remove(o["uri_extension"]+fsize, UEB_length)
+        return self._file.UEB
+
+    def _satisfy_share_hash_tree(self):
+        # the share hash chain is stored as (hashnum,hash) tuples, so you
+        # can't fetch just the pieces you need, because you don't know
+        # exactly where they are. So fetch everything, and parse the results
+        # later.
+        hashlen = o["uri_extension"] - o["share_hashes"]
+        assert hashlen % (2+HASH_SIZE) == 0
+        hashdata = rdata.get(o["share_hashes"], hashlen)
+        if not hashdata:
+            return
+        share_hashes = []
+        for i in range(0, hashlen, 2+HASH_SIZE):
+            hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
+            hashvalue = hashdata[i+2:i+2+HASH_SIZE]
+            share_hashes.append( (hashnum, hashvalue) )
+        ok = self._file.process_share_hashes(share_hashes)
+        # adds to self._file.share_hash_tree
+        if ok:
+            rdata.remove(o["share_hashes"], hashlen)
+
+    def _satisfy_block_hash_tree(self, needed_hashes):
+        block_hashes = {}
+        for hashnum in needed_hashes:
+            hashdata = rdata.get(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+            if hashdata:
+                block_hashes[hashnum] = hashdata
+            else:
+                return # missing some hashes
+        # note that we don't submit any hashes to the block_hash_tree until
+        # we've gotten them all, because the hash tree will throw an
+        # exception if we only give it a partial set (which it therefore
+        # cannot validate)
+        ok = share2.process_block_hashes(block_hashes)
+        if not ok:
+            return
+        for hashnum in needed_hashes:
+            rdata.remove(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+
+    def _satisfy_data_block(self, segnum, observers):
+        segsize = self._file.UEB["segment_size"]
+        needed_shares = self._file.UEB["needed_shares"]
+        sharesize = mathutil.div_ceil(self._file.UEB["size"],
+                                      needed_shares)
+        blocksize = mathutil.div_ceil(segsize, needed_shares)
+        blockstart = o["data"] + segnum * blocksize
+        if blocknum < NUM_BLOCKS-1:
+            blocklen = blocksize
+        else:
+            blocklen = sharesize % blocksize
+            if blocklen == 0:
+                blocklen = blocksize
+        block = rdata.pop(blockstart, blocklen)
+        if not block:
+            return
+        ok = share2.process_block(block)
+        if not ok:
+            return
+        assert self._requested_blocks[0][0] == segnum
+        for o in observers:
+            o.notify(block=block)
+        self._requested_blocks.pop(0) # retired
+
+    def _desire(self, segnum):
+        segnum, observers = self._active_segnum_and_observers()
+        fsize = self._fieldsize
+        rdata = self._received_data
+        share2 = self._share2
 
         o = self.actual_offsets or self.guessed_offsets
         segsize = self.actual_segsize or self.guessed_segsize
@@ -824,9 +884,6 @@ class Share:
             self._wanted_blocks.add(blockstart, blocklen)
 
 
-        self._request_needed()
-
-
     def _ask_for_offsets(self):
         version_s = self._received_data.get(0, 4)
         if version_s is None:
@@ -1190,3 +1247,13 @@ class FileNode:
     def read(self, consumer, offset=0, size=None):
         decryptor = DecryptingConsumer(consumer, self._readkey, offset)
         return self._downloader.read(decryptor, offset, size)
+
+    # things called by our Share instances
+    def validate_UEB(self, UEB_s):
+        self.UEB = UEB
+        ...
+        return True
+
+    def process_share_hashes(self, share_hashes):
+        self.share_hash_tree = x
+        return True

commit b601ead3036ccca16646ed3d89a4d5fb92d86012
Author: Brian Warner <warner@lothar.com>
Date:   Sat Jan 30 22:59:27 2010 -0800

    spans.py: update docs
---
 src/allmydata/util/spans.py |   27 +++++++++++++++++----------
 1 files changed, 17 insertions(+), 10 deletions(-)

diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
index 49c56d7..41b2215 100755
--- a/src/allmydata/util/spans.py
+++ b/src/allmydata/util/spans.py
@@ -1,15 +1,22 @@
 
 class Spans:
-    """I maintain a set of integers, optimized for operations on spans like
-    'add range to set' and 'is range in set'. Equivalently, I can be said to
-    maintain a large array of booleans. Typically the integers represent
-    offsets into a large string, such as a distant share. In such a context,
-    I can help track which portions of the share are have been requested or
-    received.
-
-    I behave similarly to perl's Set::IntSpan module. This module is commonly
-    used for representing .newsrc files (in which the integers represent
-    message numbers).
+    """I represent a compressed list of booleans, one per index (an integer).
+    Typically, each index represents an offset into a large string, pointing
+    to a specific byte of a share. In this context, True means that byte has
+    been received, or has been requested.
+
+    Another way to look at this is maintaining a set of integers, optimized
+    for operations on spans like 'add range to set' and 'is range in set?'.
+
+    This is a python equivalent of perl's Set::IntSpan module, frequently
+    used to represent .newsrc contents.
+
+    Rather than storing an actual (large) list or dictionary, I represent my
+    internal state as a sorted list of spans, each with a start and a length.
+    My API is presented in terms of start+length pairs. I provide set
+    arithmetic operators, to efficiently answer questions like 'I want bytes
+    XYZ, I already requested bytes ABC, and I've already received bytes DEF:
+    what bytes should I request now?'.
 
     The new downloader will use it to keep track of which bytes we've requested
     or received already.

commit 2d5467bbe4db36f1e573d7e44b83a1bd02746fcd
Author: Brian Warner <warner@lothar.com>
Date:   Sat Jan 30 22:59:20 2010 -0800

    download2.py: WIP
---
 src/allmydata/immutable/download2.py | 1192 ++++++++++++++++++++++++++++++++++
 1 files changed, 1192 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/immutable/download2.py b/src/allmydata/immutable/download2.py
new file mode 100644
index 0000000..c4d7254
--- /dev/null
+++ b/src/allmydata/immutable/download2.py
@@ -0,0 +1,1192 @@
+
+# known (shnum,Server) pairs are sorted into a list according to
+# desireability. This sort is picking a winding path through a matrix of
+# [shnum][server]. The goal is to get diversity of both shnum and server.
+
+# The initial order is:
+#  find the lowest shnum on the first server, add it
+#  look at the next server, find the lowest shnum that we don't already have
+#   if any
+#  next server, etc, until all known servers are checked
+#  now look at servers that we skipped (because ...
+
+# Keep track of which block requests are outstanding by (shnum,Server). Don't
+# bother prioritizing "validated" shares: the overhead to pull the share hash
+# chain is tiny (4 hashes = 128 bytes), and the overhead to pull a new block
+# hash chain is also tiny (1GB file, 8192 segments of 128KiB each, 13 hashes,
+# 832 bytes). Each time a block request is sent, also request any necessary
+# hashes. Don't bother with a "ValidatedShare" class (as distinct from some
+# other sort of Share). Don't bother avoiding duplicate hash-chain requests.
+
+# For each outstanding segread, walk the list and send requests (skipping
+# outstanding shnums) until requests for k distinct shnums are in flight. If
+# we can't do that, ask for more. If we get impatient on a request, find the
+# first non-outstanding
+
+# start with the first Share in the list, and send a request. Then look at
+# the next one. If we already have a pending request for the same shnum or
+# server, push that Share down onto the fallback list and try the next one,
+# etc. If we run out of non-fallback shares, use the fallback ones,
+# preferring shnums that we don't have outstanding requests for (i.e. assume
+# that all requests will complete). Do this by having a second fallback list.
+
+# hell, I'm reviving the Herder. But remember, we're still talking 3 objects
+# per file, not thousands.
+
+# actually, don't bother sorting the initial list. Append Shares as the
+# responses come back, that will put the fastest servers at the front of the
+# list, and give a tiny preference to servers that are earlier in the
+# permuted order.
+
+# TODO: if server1 has all shares, and server2-10 have one each, make the
+# loop stall slightly before requesting all shares from the first server, to
+# give it a chance to learn about the other shares and get some diversity.
+# Or, don't bother, let the first block all come from one server, and take
+# comfort in the fact that we'll learn about the other servers by the time we
+# fetch the second block.
+
+# as a query gets later, we're more willing to duplicate work.
+
+# more ideas:
+#  sort shares by:
+#   1: number of roundtrips needed to get some data
+#   2: share number
+#   3: ms of RTT delay
+# maybe measure average time-to-completion of requests, compare completion
+# time against that, much larger indicates congestion on the server side
+# or the server's upstream speed is less than our downstream. Minimum
+# time-to-completion indicates min(our-downstream,their-upstream). Could
+# fetch shares one-at-a-time to measure that better.
+
+# when should we risk duplicate work and send a new request?
+
+# should change server read protocol to allow small shares to be fetched in a
+# single RTT. Instead of get_buckets-then-read, just use read(shnums, readv),
+# where shnums=[] means all shares, and the return value is a dict of
+# # shnum->ta (like with mutable files). The DYHB query should also fetch the
+# offset table, since everything else can be located once we have that.
+
+from allmydata.util.hashtree import IncompleteHashTree, BadHashError, \
+     NotEnoughHashesError
+
+class PendingRequests:
+    def add(self, s):
+        self.outstanding[s] = time.time() # turn this into a list for retries
+    def done(self, s):
+        self.outstanding.pop(s)
+    def nonlate(self):
+        pass
+
+def walk(self):
+    shares = sorted(list)
+    oldshares = copy(shares)
+    outstanding = list()
+    fallbacks = list()
+    second_fallbacks = list()
+    while len(outstanding.nonlate.shnums) < k: # need more requests
+        while oldshares:
+            s = shares.pop(0)
+            if s.server in outstanding.servers or s.shnum in outstanding.shnums:
+                fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # must use fallback list. Ask for more servers while we're at it.
+        ask_for_more_servers()
+        while fallbacks:
+            s = fallbacks.pop(0)
+            if s.shnum in outstanding.shnums:
+                # assume that the outstanding requests will complete, but
+                # send new requests for other shnums to existing servers
+                second_fallbacks.append(s)
+                continue
+            outstanding.append(s)
+            send_request(s)
+            break #'while need_more_requests'
+        # if we get here, we're being forced to send out multiple queries per
+        # share. We've already asked for more servers, which might help. If
+        # there are no late outstanding queries, then duplicate shares won't
+        # help. Don't send queries for duplicate shares until some of the
+        # queries are late.
+        if outstanding.late:
+            # we're allowed to try any non-outstanding share
+            while second_fallbacks:
+                pass
+    newshares = outstanding + fallbacks + second_fallbacks + oldshares
+        
+
+class Reader:
+    implements(IPushProducer)
+    def __init__(self, consumer, offset, size):
+        self._needed = []
+        self._consumer = consumer
+        self._hungry = False
+        self._offset = offset
+        self._size = size
+    def start(self):
+        self._alive = True
+        self._deferred = defer.Deferred()
+        return self._deferred
+
+    def _compute_segnums(self, segsize):
+        # now that we know the file's segsize, what segments (and which
+        # ranges of each) will we need?
+        while size:
+            assert size >= 0
+            this_seg_num = int(offset / self._segsize)
+            this_seg_offset = offset - (seg_num*self._segsize)
+            this_seg_size = min(size, self._segsize-seg_offset)
+            size -= this_seg_size
+            if size:
+                offset += this_seg_size
+            record = (this_seg_num, this_seg_offset, this_seg_size)
+            self._needed.append(record)
+
+    def get_needed_segments(self):
+        return set([segnum for (segnum, off, size) in self._needed])
+
+
+    def stopProducing(self):
+        self._hungry = False
+        self._alive = False
+    def pauseProducing(self):
+        self._hungry = False
+    def resumeProducing(self):
+        self._hungry = True
+    def add_segment(self, segnum, offset, size):
+        self._needed.append( (segnum, offset, size) )
+    def got_segment(self, segnum, segdata):
+        """Return True if this schedule has more to go, or False if it is
+        done."""
+        assert self._needed[0][segnum] == segnum
+        (_ign, offset, size) = self._needed.pop(0)
+        data = segdata[offset:offset+size]
+        self._consumer.write(data)
+        if not self._needed:
+            # we're done
+            self._alive = False
+            self._hungry = False
+            self._consumer.unregisterProducer()
+            self._deferred.callback(self._consumer)
+    def error(self, f):
+        self._alive = False
+        self._hungry = False
+        self._consumer.unregisterProducer()
+        self._deferred.errback(f)
+
+class Server:
+    """I represent an abstract Storage Server. One day, the StorageBroker
+    will return instances of me. For now, the StorageBroker returns (peerid,
+    RemoteReference) tuples, and this code wraps a Server instance around
+    them.
+    """
+    def __init__(self, peerid, ss):
+        self.peerid = peerid
+        self.remote = ss
+        self._remote_buckets = {} # maps shnum to RIBucketReader
+        # TODO: release the bucket references on shares that we no longer
+        # want. OTOH, why would we not want them? Corruption?
+
+    def send_query(self, storage_index):
+        """I return a Deferred that fires with a set of shnums. If the server
+        had shares available, I will retain the RemoteReferences to its
+        buckets, so that get_data(shnum, range) can be called later."""
+        d = self.remote.callRemote("get_buckets", self.storage_index)
+        d.addCallback(self._got_response)
+        return d
+
+    def _got_response(self, r):
+        self._remote_buckets = r
+        return set(r.keys())
+
+def incidentally(res, f, *args, **kwargs):
+    """Add me to a Deferred chain like this:
+     d.addBoth(incidentally, func, arg)
+    and I'll behave as if you'd added the following function:
+     def _(res):
+         func(arg)
+         return res
+    This is useful if you want to execute an expression when the Deferred
+    fires, but don't care about its value.
+    """
+    f(*args, **kwargs)
+    return res
+
+class DictOfSets:
+    def add(self, key, value): pass
+    def values(self): # return set that merges all value sets
+        r = set()
+        for key in self:
+            r.update(self[key])
+        return r
+
+class ShareOnAServer:
+    """I represent one instance of a share, known to live on a specific
+    server. I am created every time a server responds affirmatively to a
+    do-you-have-block query."""
+
+    def __init__(self, shnum, server):
+        self._shnum = shnum
+        self._server = server
+        self._block_hash_tree = None
+
+    def cost(self, segnum):
+        """I return a tuple of (roundtrips, bytes, rtt), indicating how
+        expensive I think it would be to fetch the given segment. Roundtrips
+        indicates how many roundtrips it is likely to take (one to get the
+        data and hashes, plus one to get the offset table and UEB if this is
+        the first segment we've ever fetched). 'bytes' is how many bytes we
+        must fetch (estimated). 'rtt' is estimated round-trip time (float) in
+        seconds for a trivial request. The downloading algorithm will compare
+        costs to decide which shares should be used."""
+        # the most significant factor here is roundtrips: a Share for which
+        # we already have the offset table is better to than a brand new one
+
+    def max_bandwidth(self):
+        """Return a float, indicating the highest plausible bytes-per-second
+        that I've observed coming from this share. This will be based upon
+        the minimum (bytes-per-fetch / time-per-fetch) ever observed. This
+        can we used to estimate the server's upstream bandwidth. Clearly this
+        is only accurate if a share is retrieved with no contention for
+        either the upstream, downstream, or middle of the connection, but it
+        may still serve as a useful metric for deciding which servers to pull
+        from."""
+
+    def get_segment(self, segnum):
+        """I return a Deferred that will fire with the segment data, or
+        errback."""
+
+class NativeShareOnAServer(ShareOnAServer):
+    """For tahoe native (foolscap) servers, I contain a RemoteReference to
+    the RIBucketReader instance."""
+    def __init__(self, shnum, server, rref):
+        ShareOnAServer.__init__(self, shnum, server)
+        self._rref = rref # RIBucketReader
+
+class Share:
+    def __init__(self, shnum):
+        self._shnum = shnum
+        # _servers are the Server instances which appear to hold a copy of
+        # this share. It is populated when the ValidShare is first created,
+        # or when we receive a get_buckets() response for a shnum that
+        # already has a ValidShare instance. When we lose the connection to a
+        # server, we remove it.
+        self._servers = set()
+        # offsets, UEB, and share_hash_tree all live in the parent.
+        # block_hash_tree lives here.
+        self._block_hash_tree = None
+
+        self._want
+
+    def get_servers(self):
+        return self._servers
+
+
+    def get_block(self, segnum):
+        # read enough data to obtain a single validated block
+        if not self.have_offsets:
+            # we get the offsets in their own read, since they tell us where
+            # everything else lives. We must fetch offsets for each share
+            # separately, since they aren't directly covered by the UEB.
+            pass
+        if not self.parent.have_ueb:
+            # use _guessed_segsize to make a guess about the layout, so we
+            # can fetch both the offset table and the UEB in the same read.
+            # This also requires making a guess about the presence or absence
+            # of the plaintext_hash_tree. Oh, and also the version number. Oh
+            # well.
+            pass
+
+class CiphertextDownloader:
+    """I manage all downloads for a single file. I operate a state machine
+    with input events that are local read() requests, responses to my remote
+    'get_bucket' and 'read_bucket' messages, and connection establishment and
+    loss. My outbound events are connection establishment requests and bucket
+    read requests messages.
+    """
+    # eventually this will merge into the FileNode
+    ServerClass = Server # for tests to override
+
+    def __init__(self, storage_index, ueb_hash, size, k, N, storage_broker,
+                 shutdowner):
+        # values we get from the filecap
+        self._storage_index = si = storage_index
+        self._ueb_hash = ueb_hash
+        self._size = size
+        self._needed_shares = k
+        self._total_shares = N
+        self._share_hash_tree = IncompleteHashTree(self._total_shares)
+        # values we discover when we first fetch the UEB
+        self._ueb = None # is dict after UEB fetch+validate
+        self._segsize = None
+        self._numsegs = None
+        self._blocksize = None
+        self._tail_segsize = None
+        self._ciphertext_hash = None # optional
+        # structures we create when we fetch the UEB, then continue to fill
+        # as we download the file
+        self._share_hash_tree = None # is IncompleteHashTree after UEB fetch
+        self._ciphertext_hash_tree = None
+
+        # values we learn as we download the file
+        self._offsets = {} # (shnum,Server) to offset table (dict)
+        self._block_hash_tree = {} # shnum to IncompleteHashTree
+        # other things which help us
+        self._guessed_segsize = min(128*1024, size)
+        self._active_share_readers = {} # maps shnum to Reader instance
+        self._share_readers = [] # sorted by preference, best first
+        self._readers = set() # set of Reader instances
+        self._recent_horizon = 10 # seconds
+
+        # 'shutdowner' is a MultiService parent used to cancel all downloads
+        # when the node is shutting down, to let tests have a clean reactor.
+
+        self._init_available_servers()
+        self._init_find_enough_shares()
+
+    # _available_servers is an iterator that provides us with Server
+    # instances. Each time we pull out a Server, we immediately send it a
+    # query, so we don't need to keep track of who we've sent queries to.
+
+    def _init_available_servers(self):
+        self._available_servers = self._get_available_servers()
+        self._no_more_available_servers = False
+
+    def _get_available_servers(self):
+        """I am a generator of servers to use, sorted by the order in which
+        we should query them. I make sure there are no duplicates in this
+        list."""
+        # TODO: make StorageBroker responsible for this non-duplication, and
+        # replace this method with a simple iter(get_servers_for_index()),
+        # plus a self._no_more_available_servers=True
+        seen = set()
+        sb = self._storage_broker
+        for (peerid, ss) in sb.get_servers_for_index(self._storage_index):
+            if peerid not in seen:
+                yield self.ServerClass(peerid, ss) # Server(peerid, ss)
+                seen.add(peerid)
+        self._no_more_available_servers = True
+
+    # this block of code is responsible for having enough non-problematic
+    # distinct shares/servers available and ready for download, and for
+    # limiting the number of queries that are outstanding. The idea is that
+    # we'll use the k fastest/best shares, and have the other ones in reserve
+    # in case those servers stop responding or respond too slowly. We keep
+    # track of all known shares, but we also keep track of problematic shares
+    # (ones with hash failures or lost connections), so we can put them at
+    # the bottom of the list.
+
+    def _init_find_enough_shares(self):
+        # _unvalidated_sharemap maps shnum to set of Servers, and remembers
+        # where viable (but not yet validated) shares are located. Each
+        # get_bucket() response adds to this map, each act of validation
+        # removes from it.
+        self._sharemap = DictOfSets()
+
+        # _sharemap maps shnum to set of Servers, and remembers where viable
+        # shares are located. Each get_bucket() response adds to this map,
+        # each hash failure or disconnect removes from it. (TODO: if we
+        # disconnect but reconnect later, we should be allowed to re-query).
+        self._sharemap = DictOfSets()
+
+        # _problem_shares is a set of (shnum, Server) tuples, and
+
+        # _queries_in_flight maps a Server to a timestamp, which remembers
+        # which servers we've sent queries to (and when) but have not yet
+        # heard a response. This lets us put a limit on the number of
+        # outstanding queries, to limit the size of the work window (how much
+        # extra work we ask servers to do in the hopes of keeping our own
+        # pipeline filled). We remove a Server from _queries_in_flight when
+        # we get an answer/error or we finally give up. If we ever switch to
+        # a non-connection-oriented protocol (like UDP, or forwarded Chord
+        # queries), we can use this information to retransmit any query that
+        # has gone unanswered for too long.
+        self._queries_in_flight = dict()
+
+    def _count_recent_queries_in_flight(self):
+        now = time.time()
+        recent = now - self._recent_horizon
+        return len([s for (s,when) in self._queries_in_flight.items()
+                    if when > recent])
+
+    def _find_enough_shares(self):
+        # goal: have 2*k distinct not-invalid shares available for reading,
+        # from 2*k distinct servers. Do not have more than 4*k "recent"
+        # queries in flight at a time.
+        if (len(self._sharemap) >= 2*self._needed_shares
+            and len(self._sharemap.values) >= 2*self._needed_shares):
+            return
+        num = self._count_recent_queries_in_flight()
+        while num < 4*self._needed_shares:
+            try:
+                s = self._available_servers.next()
+            except StopIteration:
+                return # no more progress can be made
+            self._queries_in_flight[s] = time.time()
+            d = s.send_query(self._storage_index)
+            d.addBoth(incidentally, self._queries_in_flight.discard, s)
+            d.addCallbacks(lambda shnums: [self._sharemap.add(shnum, s)
+                                           for shnum in shnums],
+                           lambda f: self._query_error(f, s))
+            d.addErrback(self._error)
+            d.addCallback(self._reschedule)
+            num += 1
+
+    def _query_error(self, f, s):
+        # a server returned an error, log it gently and ignore
+        level = log.WEIRD
+        if f.check(DeadReferenceError):
+            level = log.UNUSUAL
+        log.msg("Error during get_buckets to server=%(server)s", server=str(s),
+                failure=f, level=level, umid="3uuBUQ")
+
+    # this block is responsible for turning known shares into usable shares,
+    # by fetching enough data to validate their contents.
+
+    # UEB (from any share)
+    # share hash chain, validated (from any share, for given shnum)
+    # block hash (any share, given shnum)
+
+    def _got_ueb(self, ueb_data, share):
+        if self._ueb is not None:
+            return
+        if hashutil.uri_extension_hash(ueb_data) != self._ueb_hash:
+            share.error("UEB hash does not match")
+            return
+        d = uri.unpack_extension(ueb_data)
+        self.share_size = mathutil.div_ceil(self._size, self._needed_shares)
+
+
+        # There are several kinds of things that can be found in a UEB.
+        # First, things that we really need to learn from the UEB in order to
+        # do this download. Next: things which are optional but not redundant
+        # -- if they are present in the UEB they will get used. Next, things
+        # that are optional and redundant. These things are required to be
+        # consistent: they don't have to be in the UEB, but if they are in
+        # the UEB then they will be checked for consistency with the
+        # already-known facts, and if they are inconsistent then an exception
+        # will be raised. These things aren't actually used -- they are just
+        # tested for consistency and ignored. Finally: things which are
+        # deprecated -- they ought not be in the UEB at all, and if they are
+        # present then a warning will be logged but they are otherwise
+        # ignored.
+
+        # First, things that we really need to learn from the UEB:
+        # segment_size, crypttext_root_hash, and share_root_hash.
+        self._segsize = d['segment_size']
+
+        self._blocksize = mathutil.div_ceil(self._segsize, self._needed_shares)
+        self._numsegs = mathutil.div_ceil(self._size, self._segsize)
+
+        self._tail_segsize = self._size % self._segsize
+        if self._tail_segsize == 0:
+            self._tail_segsize = self._segsize
+        # padding for erasure code
+        self._tail_segsize = mathutil.next_multiple(self._tail_segsize,
+                                                    self._needed_shares)
+
+        # Ciphertext hash tree root is mandatory, so that there is at most
+        # one ciphertext that matches this read-cap or verify-cap. The
+        # integrity check on the shares is not sufficient to prevent the
+        # original encoder from creating some shares of file A and other
+        # shares of file B.
+        self._ciphertext_hash_tree = IncompleteHashTree(self._numsegs)
+        self._ciphertext_hash_tree.set_hashes({0: d['crypttext_root_hash']})
+
+        self._share_hash_tree.set_hashes({0: d['share_root_hash']})
+
+
+        # Next: things that are optional and not redundant: crypttext_hash
+        if 'crypttext_hash' in d:
+            if len(self._ciphertext_hash) == hashutil.CRYPTO_VAL_SIZE:
+                self._ciphertext_hash = d['crypttext_hash']
+            else:
+                log.msg("ignoring bad-length UEB[crypttext_hash], "
+                        "got %d bytes, want %d" % (len(d['crypttext_hash']),
+                                                   hashutil.CRYPTO_VAL_SIZE),
+                        umid="oZkGLA", level=log.WEIRD)
+
+        # we ignore all of the redundant fields when downloading. The
+        # Verifier uses a different code path which does not ignore them.
+
+        # finally, set self._ueb as a marker that we don't need to request it
+        # anymore
+        self._ueb = d
+
+    def _got_share_hashes(self, hashes, share):
+        assert isinstance(hashes, dict)
+        try:
+            self._share_hash_tree.set_hashes(hashes)
+        except (IndexError, BadHashError, NotEnoughHashesError), le:
+            share.error("Bad or missing hashes")
+            return
+
+    #def _got_block_hashes(
+
+    def _init_validate_enough_shares(self):
+        # _valid_shares maps shnum to ValidatedShare instances, and is
+        # populated once the block hash root has been fetched and validated
+        # (which requires any valid copy of the UEB, and a valid copy of the
+        # share hash chain for each shnum)
+        self._valid_shares = {}
+
+        # _target_shares is an ordered list of ReadyShare instances, each of
+        # which is a (shnum, server) tuple. It is sorted in order of
+        # preference: we expect to get the fastest response from the
+        # ReadyShares at the front of the list. It is also sorted to
+        # distribute the shnums, so that fetching shares from
+        # _target_shares[:k] is likely (but not guaranteed) to give us k
+        # distinct shares. The rule is that we skip over entries for blocks
+        # that we've already received, limit the number of recent queries for
+        # the same block, 
+        self._target_shares = []
+
+    def _validate_enough_shares(self):
+        # my goal is to have at least 2*k distinct validated shares from at
+        # least 2*k distinct servers
+        valid_share_servers = set()
+        for vs in self._valid_shares.values():
+            valid_share_servers.update(vs.get_servers())
+        if (len(self._valid_shares) >= 2*self._needed_shares
+            and len(self._valid_share_servers) >= 2*self._needed_shares):
+            return
+        #for 
+
+    def _reschedule(self, _ign):
+        # fire the loop again
+        if not self._scheduled:
+            self._scheduled = True
+            eventually(self._loop)
+
+    def _loop(self):
+        self._scheduled = False
+        # what do we need?
+
+        self._find_enough_shares()
+        self._validate_enough_shares()
+
+        if not self._ueb:
+            # we always need a copy of the UEB
+            pass
+
+    def _error(self, f):
+        # this is an unexpected error: a coding bug
+        log.err(f, level=log.UNUSUAL)
+
+    def read(self, consumer, offset=0, size=None):
+        """I am the main entry point, from which FileNode.read() can get
+        data."""
+        # tolerate concurrent operations: each gets its own Reader
+        if size is None:
+            size = self._size - offset
+        r = Reader(consumer, offset, size)
+        self._readers.add(r)
+        d = start(consumer)
+        self._loop()
+        return d
+            
+
+
+# using a single packed string (and an offset table) may be an artifact of
+# our native storage server: other backends might allow cheap multi-part
+# files (think S3, several buckets per share, one for each section).
+
+# find new names for:
+#  data_holder
+#  Share / Share2  (ShareInstance / Share? but the first is more useful)
+
+class IShare(Interface):
+    """I represent a single instance of a single share (e.g. I reference the
+    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
+    This interface is used by SegmentFetcher to retrieve validated blocks.
+    """
+    def get_block(segnum):
+        """Return an Observer2, which will be notified with the following
+        events:
+         state=COMPLETE, block=data (terminal): validated block data
+         state=OVERDUE (non-terminal): we have reason to believe that the
+                                       request might have stalled, or we
+                                       might just be impatient
+         state=CORRUPT (terminal): the data we received was corrupt
+         state=DEAD (terminal): the connection has failed
+        """
+
+
+# it'd be nice if we receive the hashes before the block, or just
+# afterwards, so we aren't stuck holding on to unvalidated blocks
+# that we can't process. If we guess the offsets right, we can
+# accomplish this by sending the block request after the metadata
+# requests (by keeping two separate requestlists), and have a one RTT
+# pipeline like:
+#  1a=metadata, 1b=block
+#  1b->process+deliver : one RTT
+
+# But if we guess wrong, and fetch the wrong part of the block, we'll
+# have a pipeline that looks like:
+#  1a=wrong metadata, 1b=wrong block
+#  1a->2a=right metadata,2b=right block
+#  2b->process+deliver
+# which means two RTT and buffering one block (which, since we'll
+# guess the segsize wrong for everything, means buffering one
+# segment)
+
+# if we start asking for multiple segments, we could get something
+# worse:
+#  1a=wrong metadata, 1b=wrong block0, 1c=wrong block1, ..
+#  1a->2a=right metadata,2b=right block0,2c=right block1, .
+#  2b->process+deliver
+
+# which means two RTT but fetching and buffering the whole file
+# before delivering anything. However, since we don't know when the
+# other shares are going to arrive, we need to avoid having more than
+# one block in the pipeline anyways. So we shouldn't be able to get
+# into this state.
+
+# it also means that, instead of handling all of
+# self._requested_blocks at once, we should only be handling one
+# block at a time: one of the requested block should be special
+# (probably FIFO). But retire all we can.
+
+class Share:
+    # this is a specific implementation of IShare for tahoe's native storage
+    # servers. A different backend would use a different class.
+    """I represent a single instance of a single share (e.g. I reference the
+    shnum2 for share SI=abcde on server xy12t, not the one on server ab45q).
+    I am associated with a Share2 that remembers data that is held in common
+    among e.g. SI=abcde/shnum2 across all servers. I am also associated with
+    a File for e.g. SI=abcde (all shares, all servers).
+    """
+    implements(IShare)
+
+    def __init__(self, rref, verifycap, share2, file):
+        self._rref = rref
+        self._guess_offsets(filesize, filecap_parms, my_default_segsize)
+        self.actual_offsets = None
+        self._UEB_length = None
+        self._share2 = share2
+        self._file = file
+        self._wanted = Spans() # desired metadata
+        self._wanted_blocks = Spans() # desired block data
+        self._requested = Spans() # we've sent a request for this
+        self._received = Spans() # we've received a response for this
+        self._received_data = DataSpans() # the response contents, still unused
+        self._requested_blocks = [] # (segnum, [observer2s])
+
+    def _guess_offsets(self, filesize, filecap_parms, my_default_segsize):
+        offsets = {}
+        for i,field in enumerate('data',
+                                 'plaintext_hash_tree', # UNUSED
+                                 'crypttext_hash_tree',
+                                 'block_hashes',
+                                 'share_hashes',
+                                 'uri_extension',
+                                 ):
+            offsets[field] = i # bad guesses are easy :)
+        self.guessed_offsets = offsets
+        self._fieldsize = 4
+        self._fieldstruct = ">L"
+
+    def loop(self):
+        # See what data we've received, construct anything we can from it.
+        # Use self._wanted to track what we want, then we'll send out
+        # requests as necessary to retrieve it. We only retrieve information
+        # for one segment at a time, to minimize alacrity.
+
+        #  offset table (guessed, confirmed, stashed locally)
+        #  UEB (probably in File)
+        #  share hash chain (partially in File)
+        #  block hash chain (stash pieces in Share2)
+        #  block data (ephemeral, given to client upon receipt)
+
+        segnum = None
+        if self._requested_blocks:
+            segnum, observers = self._requested_blocks[0]
+
+        # first part: process any information that we need and have the data
+        # for
+
+        if not self.actual_offsets:
+            self._ask_for_offsets()
+        fsize = self._fieldsize
+        rdata = self._received_data
+        share2 = self._share2
+
+        if self.actual_offsets:
+            o = self.actual_offsets
+
+            # UEB
+            if self._file.UEB is None:
+                UEB_length_s = rdata.get(o["uri_extension"], fsize)
+                if UEB_length_s:
+                    UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
+                    UEB_s = rdata.get(o["uri_extension"]+fsize, UEB_length)
+                    if UEB_s:
+                        UEB = self._validate_UEB(UEB_s)
+                        if UEB is not None:
+                            self._file.UEB = UEB
+                            rdata.remove(o["uri_extension"]+fsize, UEB_length)
+                        # TODO: if this UEB was bad, we'll keep trying to
+                        # validate it over and over again. Only log.err on
+                        # the first one, or better yet skip all but the first
+
+            # share hash chain
+            if self._file.share_hash_tree.needed_hashes(self.shnum):
+                # the share hash chain is stored as (hashnum,hash) tuples, so
+                # you can't fetch just the pieces you need, because you don't
+                # know exactly where they are. So fetch everything, and parse
+                # the results later.
+                hashlen = o["uri_extension"] - o["share_hashes"]
+                assert hashlen % (2+HASH_SIZE) == 0
+                hashdata = rdata.get(o["share_hashes"], hashlen)
+                if hashdata:
+                    share_hashes = []
+                    for i in range(0, hashlen, 2+HASH_SIZE):
+                        hashnum = struct.unpack(">H", hashdata[i:i+2])[0]
+                        hashvalue = hashdata[i+2:i+2+HASH_SIZE]
+                        share_hashes.append( (hashnum, hashvalue) )
+                    if self._process_share_hashes(share_hashes):
+                        # adds to self._file.share_hash_tree
+                        rdata.remove(o["share_hashes"], hashlen)
+
+            # block hash chain
+            if segnum is not None:
+                block_hashes = {}
+                for hashnum in share2.block_hash_tree.needed_hashes(segnum):
+                    hashdata = rdata.get(o["block_hashes"]+hashnum*HASH_SIZE,
+                                         HASH_SIZE)
+                    if hashdata:
+                        block_hashes[hashnum] = hashdata
+                # TODO: if we get the offsets wrong, we'll grab some of the
+                # desired hashes but not all. I don't think that
+                # IncompleteHashTree.set_hashes() is prepared to handle this:
+                # it wants all or nothing. To deal with it, I think we'd need
+                # an API that accepts partial hashes: we'd then use 'not
+                # needed_hashes()' as below to tell whether the hashtree is
+                # good or not.
+                share2.block_hash_tree.ADDSTUFF(block_hashes)
+
+            # data blocks
+            if (segnum is not None and
+                not share2.block_hash_tree.needed_hashes(segnum)):
+                segsize = self._file.UEB["segment_size"]
+                needed_shares = self._file.UEB["needed_shares"]
+                sharesize = mathutil.div_ceil(self._file.UEB["size"],
+                                              needed_shares)
+                blocksize = mathutil.div_ceil(segsize, needed_shares)
+                blockstart = o["data"] + segnum * blocksize
+                if blocknum < NUM_BLOCKS-1:
+                    blocklen = blocksize
+                else:
+                    blocklen = sharesize % blocksize
+                    if blocklen == 0:
+                        blocklen = blocksize
+                block = rdata.pop(blockstart, blocklen)
+                if block:
+                    PROCESS_BLOCK(block) # check hash
+                    assert self._requested_blocks[0][0] == segnum
+                    for o in observers:
+                        o.notify(block=block)
+                    self._requested_blocks.pop(0) # retired
+
+        # second part: figure out what we need data for
+
+        o = self.actual_offsets or self.guessed_offsets
+        segsize = self.actual_segsize or self.guessed_segsize
+        if self._file.UEB is None:
+            # UEB data is stored as (length,data). We pre-fetch 2kb,
+            # which should probably cover it. If not, we'll come back
+            # here later with self._UEB_length!=None and fetch the rest.
+            self._wanted.add(o["uri_extension"], 2048)
+            if self.actual_offsets:
+                UEB_length_s = rdata.get(o["uri_extension"], fsize)
+                if UEB_length_s:
+                    UEB_length = struct.unpack(UEB_length_s, self._fieldstruct)
+                    # we know the length, so make sure we grab everything
+                    self._wanted.add(o["uri_extension"]+fsize, UEB_length)
+                # TODO: tahoe<1.3.0 would error if you read past the end
+
+        if self._file.share_hash_tree.needed_hashes(self.shnum):
+            hashlen = o["uri_extension"] - o["share_hashes"]
+            self._wanted.add(o["share_hashes"], hashlen)
+
+        # block hash chain
+        needed = set()
+        if segnum is not None:
+            n = share2.block_hash_tree.needed_hashes(segnum)
+            needed.update(n)
+        for hashnum in needed:
+            self._wanted.add(o["block_hashes"]+hashnum*HASH_SIZE, HASH_SIZE)
+
+        # data
+        if segnum is not None:
+            blockstart, blocklen = COMPUTE(segnum, segsize, etc)
+            self._wanted_blocks.add(blockstart, blocklen)
+
+
+        self._request_needed()
+
+
+    def _ask_for_offsets(self):
+        version_s = self._received_data.get(0, 4)
+        if version_s is None:
+            # includes version number, sizes, and offsets
+            self._wanted.add(0,1024)
+            return
+        (version,) = struct.unpack(">L", version_s)
+        if version == 1:
+            table_start = 0x0c
+            self._fieldsize = 0x4
+            self._fieldstruct = ">L"
+        else:
+            table_start = 0x14
+            self._fieldsize = 0x8
+            self._fieldstruct = ">Q"
+        offset_table_size = 6 * self._fieldsize
+        table_s = self._received_data.get(table_start, offset_table_size)
+        if table_s is None:
+            self._wanted.add(table_start, offset_table_size)
+            return
+
+        data = self._received_data[table_start,offset_table_size]
+        fields = struct.unpack(6*self._fieldstruct, data)
+        offsets = {}
+        for i,field in enumerate('data',
+                                 'plaintext_hash_tree', # UNUSED
+                                 'crypttext_hash_tree',
+                                 'block_hashes',
+                                 'share_hashes',
+                                 'uri_extension',
+                                 ):
+            offsets[field] = fields[i]
+        self.actual_offsets = offsets
+
+    def _request_needed(self):
+        # send requests for metadata first, to avoid hanging on to large data
+        # blocks for a long time
+        self._send_requests(self._wanted - self._received - self._requested)
+        # then send requests for data blocks
+        self._send_requests(self._wanted_blocks - self._received - self._requested
+
+    def _send_requests(self, needed):
+        for (start, length) in needed:
+            self._requested.add(start, length)
+            d = self._send_request(start, length)
+            d.addCallback(self._got_data, start, length)
+            d.addErrback(self._got_error)
+            d.addErrback(log.err, ...)
+
+    def _send_request(self, start, length):
+        return self._rref.callRemote("read", start, length)
+
+    def _got_data(self, data, start, length):
+        span = (start, length)
+        assert span in self._requested
+        self._requested.remove(start, length)
+        self._received.add(start, length)
+        self._received_data.add(start, data)
+        eventually(self.loop)
+
+    def _got_error(self, f):
+        ...
+
+
+    # called by our client, the SegmentFetcher
+    def get_block(self, segnum):
+        o = Observer2()
+        for i,(segnum0,observers) in enumerate(self._requested_blocks):
+            if segnum0 == segnum:
+                observers.append(o)
+                break
+        else:
+            self._requested_blocks.append(segnum, [o])
+        eventually(self.loop)
+        return o
+
+class ShareFinder(service.MultiService):
+    def __init__(self, storage_broker, storage_index,
+                 consumer, max_outstanding_requests=10):
+        service.MultiService.__init__(self)
+        self._servers = storage_broker.get_servers_for_index(storage_index)
+        self.consumer = consumer
+        self.max_outstanding = max_outstanding_requests
+
+        self._hungry = False
+
+        self.undelivered_shares = []
+        self.pending_requests = set()
+
+    # called by our parent CiphertextDownloader
+    def hungry(self):
+        self._hungry = True
+        eventually(self.loop)
+
+    # internal methods
+    def loop(self):
+        if not self._hungry:
+            return
+        if self.undelivered_shares:
+            sh = self.undelivered_shares.pop(0)
+            # they will call hungry() again if they want more
+            self._hungry = False
+            eventually(self.consumer.got_shares, [sh])
+            return
+        if len(self.pending_requests) >= self.max_outstanding_requests:
+            # cannot send more requests, must wait for some to retire
+            return
+
+        server = None
+        try:
+            if self._servers:
+                server = self._servers.next()
+        except StopIteration:
+            self._servers = None
+
+        if not server and not self.pending_requests:
+            # we've run out of servers (so we can't send any more requests),
+            # and we have nothing in flight. No further progress can be made.
+            # They are destined to remain hungry.
+            self.consumer.no_more_shares()
+            self.stopService()
+            # TODO: stop running
+            return
+
+        self.pending_requests.add(self.send_request(server))
+        return
+
+    def send_request(self, server):
+        ...
+
+    def got_response(self, response, req, server):
+        self.pending_requests.discard(req)
+        if good:
+            self.undelivered_shares.append(Share(..))
+            eventually(self.loop)
+
+# all classes are also Services, and the rule is that you don't initiate more
+# work unless self.running
+
+# GC: decide whether each service is restartable or not. For non-restartable
+# services, stopService() should delete a lot of attributes to kill reference
+# cycles. The primary goal is to decref remote storage BucketReaders when a
+# download is complete.
+
+class SegmentFetcher:
+    """I am responsible for acquiring blocks for a single segment."""
+    def __init__(self, segnum):
+        self.segnum = segnum
+        self.shares = {} # maps non-dead Share instance to a state, one of
+                         # (UNUSED, PENDING, OVERDUE, COMPLETE, CORRUPT).
+                         # State transition map is:
+                         #  UNUSED -(send-read)-> PENDING
+                         #  PENDING -(timer)-> OVERDUE
+                         #  PENDING -(rx)-> COMPLETE, CORRUPT, DEAD
+                         #  OVERDUE -(rx)-> COMPLETE, CORRUPT, DEAD
+                         # If a share becomes DEAD, it is removed from the
+                         # dict.
+        self.shnums = DictOfSets() # maps shnum to the shares that provide it
+        self.blocks = {} # maps shnum to validated block data
+
+    def add_shares(self, shares):
+        # called when ShareFinder locates a new share, and when a non-initial
+        # segment fetch is started and we already know about shares from the
+        # previous segment
+        for s in shares:
+            self.shares[s] = UNUSED
+            self.shnums[s.shnum].add(s)
+        eventually(self.loop)
+
+    def count_shnums(self, *states):
+        """shnums for which at least one state is in the following list"""
+        shnums = []
+        for shnum,shares in self.shnums.iteritems():
+            matches = [s for s in shares if s.state in states]
+            if matches:
+                shnums.append(shnum)
+        return len(shnums)
+
+    def no_more_shares(self):
+        # ShareFinder tells us it's reached the end of its list
+        self._no_more_shares = True
+
+    def loop(self):
+        # are we done?
+        if self.count_shnums(COMPLETE) >= self.k:
+            # yay!
+            self.parent.got_blocks(self, self.blocks)
+            return stop_running()
+
+        # we may have exhausted everything
+        if (self._no_more_shares and
+            self.count_shnums(UNUSED, PENDING, OVERDUE, COMPLETE) < k):
+            # no more new shares are coming, and the remaining hopeful shares
+            # aren't going to be enough. boo!
+            self.parent.fetch_failed(self)
+            return stop_running()
+
+        # nope, not done. Are we "block-hungry" (i.e. do we want to send out
+        # more read requests, or do we think we have enough in flight
+        # already)
+        while self.count_shnums(PENDING, COMPLETE) < k:
+            # we're hungry.. are there any unused shares?
+            progress = self.send_some_requests()
+            if not progress:
+                break
+
+        # ok, now are we "share-hungry" (i.e. do we have enough known shares
+        # to make us happy, or should we ask the ShareFinder to get us more?)
+        if self.count_shnums(UNUSED, PENDING, COMPLETE) < k:
+            # we're hungry for more shares
+            self.parent.want_more_shares()
+            # that will trigger the ShareFinder to keep looking
+
+    def send_some_requests(self):
+        for shnum,shares in self.shnums.iteritems():
+            states = [s.state for s in shares]
+            if COMPLETE in states or PENDING in states:
+                continue
+            if UNUSED in states:
+                # here's a candidate. Send a request.
+                s = find_one(shares, UNUSED)
+                self.shares[s] = PENDING
+                o = s.get_block(segnum)
+                o.subscribe(self._block_request_activity, share=s, shnum=shnum)
+                # TODO: build up a list of candidates, then walk through
+                # the list, sending requests to the most desireable
+                # servers, re-checking our block-hunger each time. For
+                # non-initial segment fetches, this would let us stick
+                # with faster servers.
+                return True # we made progress!
+        return False # no progress made
+
+    # called by Shares, in response to our s.send_request() calls
+
+    # this might be better with a Deferred, using COMPLETE as the success
+    # case and CORRUPT/DEAD in an errback, because that would let us hold the
+    # 'share' and 'shnum' arguments locally (instead of roundtripping them
+    # through Share.send_request). But that OVERDUE is not terminal. So I
+    # want a new sort of callback mechanism, with the extra-argument-passing
+    # aspects of Deferred, but without being so one-shot. Is this a job for
+    # Observer? No, it doesn't take extra arguments.
+    def _block_request_activity(self, share, shnum, state, block=None):
+        if state is COMPLETE:
+            # 'block' is fully validated
+            self.shares[share] = COMPLETE
+            self.blocks[shnum] = block
+        elif state is OVERDUE:
+            self.shares[share] = OVERDUE
+            # OVERDUE is not terminal: it will eventually transition to
+            # COMPLETE, CORRUPT, or DEAD.
+        elif state is CORRUPT:
+            self.shares[share] = CORRUPT
+        elif state is DEAD:
+            del self.shares[share]
+            self.shnums[shnum].remove(share)
+        eventually(self.loop)
+
+class Observer2:
+    """A simple class to distribute multiple events to a single subscriber.
+    It accepts arbitrary kwargs, but no posargs."""
+    def __init__(self):
+        self._watcher = None
+        self._undelivered_results = []
+
+    def subscribe(self, observer, **watcher_kwargs):
+        self._watcher = (observer, watcher_kwargs)
+        while self._undelivered_results:
+            self._notify(self._undelivered_results.pop(0))
+
+    def notify(self, **result_kwargs):
+        if self._watcher:
+            self._notify(result_kwargs)
+        else:
+            self._undelivered_results.append(result_kwargs)
+
+    def _notify(self, result_kwargs):
+        o, watcher_kwargs = self._watcher
+        kwargs = dict(result_kwargs)
+        kwargs.update(watcher_kwargs)
+        eventually(o, **kwargs)
+
+class CiphertextDownloader(service.MultiService):
+    def __init__(self):
+        service.MultiService.__init__(self)
+        self._active_segment_fetcher = None
+
+    def start_new_segment(self, segnum):
+        sf = SegmentFetcher(segnum)
+        assert not self._active_segment_fetcher
+        self._active_segment_fetcher = sf
+        sf.setServiceParent(self)
+        active_shares = [s for s in self._shares if s.not_dead()]
+        sf.add_shares(active_shares) # this triggers the loop
+
+    # called by our child SegmentFetcher
+    def got_blocks(self, sf, blocks):
+        assert sf is self._active_segment_fetcher
+        sf.disownServiceParent()
+        self._active_segment_fetcher = None
+        ... # decode, deliver
+
+    def fetch_failed(self, sf):
+        assert sf is self._active_segment_fetcher
+        sf.disownServiceParent()
+        self._active_segment_fetcher = None
+        ... # deliver error upwards
+
+    def want_more_shares(self):
+        self._sharefinder.hungry()
+
+    # called by our child ShareFinder
+    def got_shares(self, shares):
+        self._shares.update(shares)
+        if self._active_segment_fetcher:
+            self._active_segment_fetcher.add_shares(shares)
+    def no_more_shares(self):
+        self._no_more_shares = True
+        if self._active_segment_fetcher:
+            self._active_segment_fetcher.no_more_shares()
+
+
+
+class DecryptingConsumer:
+    """I sit between a CiphertextDownloader (which acts as a Producer) and
+    the real Consumer, decrypting everything that passes by. The real
+    Consumer sees the real Producer, but the Producer sees us instead of the
+    real consumer."""
+    implements(IConsumer)
+
+    def __init__(self, consumer, readkey, offset):
+        self._consumer = consumer
+        self._decryptor = AES(readkey)
+        if offset != 0:
+            # TODO: pycryptopp CTR-mode needs random-access operations: I
+            # want either a=AES(readkey, offset) or better yet both of:
+            #  a=AES(readkey, offset=0)
+            #  a.process(ciphertext, offset=xyz)
+            # For now, we fake it by decrypting a bunch of dummy data first.
+            dummysize = offset
+            BLOCKSIZE = 16*1024
+            while dummysize:
+                size = min(dummysize, BLOCKSIZE)
+                dummydata = "0" * size
+                self._decryptor.process(dummydata)
+                dummysize -= size
+
+    def registerProducer(self, producer):
+        # this passes through, so the real consumer can flow-control the real
+        # producer. Therefore we don't need to provide any IPushProducer
+        # methods. We implement all the IConsumer methods as pass-throughs,
+        # and only intercept write() to perform decryption.
+        self._consumer.registerProducer(producer)
+    def unregisterProducer(self):
+        self._consumer.unregisterProducer()
+    def write(self, ciphertext):
+        plaintext = self._decryptor.process(ciphertext)
+        self._consumer.write(plaintext)
+
+
+class FileNode:
+    def read(self, consumer, offset=0, size=None):
+        decryptor = DecryptingConsumer(consumer, self._readkey, offset)
+        return self._downloader.read(decryptor, offset, size)

commit 54028e5596a4eb3a3815ecc74595ea49f9b255fe
Author: Brian Warner <warner@lothar.com>
Date:   Sat Jan 30 22:49:43 2010 -0800

    implement fast version of bytespans, update tests, move reference into test
    file
---
 src/allmydata/test/test_util.py |  189 +++++++++++++++++++++++++++-------
 src/allmydata/util/spans.py     |  217 ++++++++++++++++++++++++++++++++++-----
 2 files changed, 342 insertions(+), 64 deletions(-)

diff --git a/src/allmydata/test/test_util.py b/src/allmydata/test/test_util.py
index ee0f722..5d59e5b 100644
--- a/src/allmydata/test/test_util.py
+++ b/src/allmydata/test/test_util.py
@@ -14,7 +14,7 @@ from allmydata.util import assertutil, fileutil, deferredutil, abbreviate
 from allmydata.util import limiter, time_format, pollmixin, cachedir
 from allmydata.util import statistics, dictutil, pipeline
 from allmydata.util import log as tahoe_log
-from allmydata.util.spans import SimpleSpans
+from allmydata.util.spans import Spans
 
 class Base32(unittest.TestCase):
     def test_b2a_matches_Pythons(self):
@@ -1541,34 +1541,107 @@ class Log(unittest.TestCase):
         self.flushLoggedErrors(SampleError)
 
 
+class SimpleSpans:
+    # this is a simple+inefficient form of util.spans.Spans . We compare the
+    # behavior of this reference model against the real (efficient) form.
+
+    def __init__(self, _span_or_start=None, length=None):
+        self._have = set()
+        if length is not None:
+            for i in range(_span_or_start, _span_or_start+length):
+                self._have.add(i)
+        elif _span_or_start:
+            for (start,length) in _span_or_start:
+                self.add(start, length)
+
+    def add(self, start, length):
+        for i in range(start, start+length):
+            self._have.add(i)
+        return self
+
+    def remove(self, start, length):
+        for i in range(start, start+length):
+            self._have.discard(i)
+        return self
+
+    def each(self):
+        return sorted(self._have)
+
+    def __iter__(self):
+        items = sorted(self._have)
+        prevstart = None
+        prevend = None
+        for i in items:
+            if prevstart is None:
+                prevstart = prevend = i
+                continue
+            if i == prevend+1:
+                prevend = i
+                continue
+            yield (prevstart, prevend-prevstart+1)
+            prevstart = prevend = i
+        if prevstart is not None:
+            yield (prevstart, prevend-prevstart+1)
+
+    def __len__(self):
+        # this also gets us bool(s)
+        return len(self._have)
+
+    def __add__(self, other):
+        s = self.__class__(self)
+        for (start, length) in other:
+            s.add(start, length)
+        return s
+
+    def __sub__(self, other):
+        s = self.__class__(self)
+        for (start, length) in other:
+            s.remove(start, length)
+        return s
+
+    def __iadd__(self, other):
+        for (start, length) in other:
+            self.add(start, length)
+        return self
+
+    def __isub__(self, other):
+        for (start, length) in other:
+            self.remove(start, length)
+        return self
+
+    def __contains__(self, (start,length)):
+        for i in range(start, start+length):
+            if i not in self._have:
+                return False
+        return True
+
 class ByteSpans(unittest.TestCase):
     def test_basic(self):
-        s = SimpleSpans()
+        s = Spans()
         self.failUnlessEqual(list(s), [])
         self.failIf(s)
         self.failIf((0,1) in s)
-        self.failUnless((0,0) in s)
         self.failUnlessEqual(len(s), 0)
 
-        s1 = SimpleSpans(3, 4) # 3,4,5,6
+        s1 = Spans(3, 4) # 3,4,5,6
         self._check1(s1)
 
-        s2 = SimpleSpans(s1)
+        s2 = Spans(s1)
         self._check1(s2)
 
         s2.add(10,2) # 10,11
         self._check1(s1)
         self.failUnless((10,1) in s2)
         self.failIf((10,1) in s1)
-        self.failUnlessEqual(s2.each(), [3,4,5,6,10,11])
+        self.failUnlessEqual(list(s2.each()), [3,4,5,6,10,11])
         self.failUnlessEqual(len(s2), 6)
 
         s2.add(15,2).add(20,2)
-        self.failUnlessEqual(s2.each(), [3,4,5,6,10,11,15,16,20,21])
+        self.failUnlessEqual(list(s2.each()), [3,4,5,6,10,11,15,16,20,21])
         self.failUnlessEqual(len(s2), 10)
 
         s2.remove(4,3).remove(15,1)
-        self.failUnlessEqual(s2.each(), [3,10,11,16,20,21])
+        self.failUnlessEqual(list(s2.each()), [3,10,11,16,20,21])
         self.failUnlessEqual(len(s2), 6)
 
     def _check1(self, s):
@@ -1576,53 +1649,52 @@ class ByteSpans(unittest.TestCase):
         self.failUnless(s)
         self.failUnlessEqual(len(s), 4)
         self.failIf((0,1) in s)
-        self.failUnless((0,0) in s)
         self.failUnless((3,4) in s)
         self.failUnless((3,1) in s)
         self.failUnless((5,2) in s)
         self.failUnless((6,1) in s)
         self.failIf((6,2) in s)
         self.failIf((7,1) in s)
-        self.failUnlessEqual(s.each(), [3,4,5,6])
+        self.failUnlessEqual(list(s.each()), [3,4,5,6])
 
     def test_math(self):
-        s1 = SimpleSpans(0, 10) # 0,1,2,3,4,5,6,7,8,9
-        s2 = SimpleSpans(5, 3) # 5,6,7
-        s3 = SimpleSpans(8, 4) # 8,9,10,11
+        s1 = Spans(0, 10) # 0,1,2,3,4,5,6,7,8,9
+        s2 = Spans(5, 3) # 5,6,7
+        s3 = Spans(8, 4) # 8,9,10,11
 
         s = s1 - s2
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,8,9])
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,8,9])
         s = s1 - s3
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7])
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7])
         s = s2 - s3
-        self.failUnlessEqual(s.each(), [5,6,7])
+        self.failUnlessEqual(list(s.each()), [5,6,7])
 
         s = s1 + s2
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9])
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7,8,9])
         s = s1 + s3
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9,10,11])
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7,8,9,10,11])
         s = s2 + s3
-        self.failUnlessEqual(s.each(), [5,6,7,8,9,10,11])
+        self.failUnlessEqual(list(s.each()), [5,6,7,8,9,10,11])
 
-        s = SimpleSpans(s1)
+        s = Spans(s1)
         s -= s2
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,8,9])
-        s = SimpleSpans(s1)
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,8,9])
+        s = Spans(s1)
         s -= s3
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7])
-        s = SimpleSpans(s2)
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7])
+        s = Spans(s2)
         s -= s3
-        self.failUnlessEqual(s.each(), [5,6,7])
+        self.failUnlessEqual(list(s.each()), [5,6,7])
 
-        s = SimpleSpans(s1)
+        s = Spans(s1)
         s += s2
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9])
-        s = SimpleSpans(s1)
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7,8,9])
+        s = Spans(s1)
         s += s3
-        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9,10,11])
-        s = SimpleSpans(s2)
+        self.failUnlessEqual(list(s.each()), [0,1,2,3,4,5,6,7,8,9,10,11])
+        s = Spans(s2)
         s += s3
-        self.failUnlessEqual(s.each(), [5,6,7,8,9,10,11])
+        self.failUnlessEqual(list(s.each()), [5,6,7,8,9,10,11])
 
     def test_random(self):
         # attempt to increase coverage of corner cases by comparing behavior
@@ -1630,7 +1702,7 @@ class ByteSpans(unittest.TestCase):
         # complex-but-fast actual implementation, in a large number of random
         # operations
         S1 = SimpleSpans
-        S2 = SimpleSpans
+        S2 = Spans
         s1 = S1(); s2 = S2()
         seed = ""
         def _create(subseed):
@@ -1639,16 +1711,18 @@ class ByteSpans(unittest.TestCase):
                 what = md5(subseed+str(i)).hexdigest()
                 op = what[0]
                 start = int(what[2:4], 16)
-                length = int(what[5:6], 16)
+                length = max(1,int(what[5:6], 16))
                 ns1.add(start, length); ns2.add(start, length)
             return ns1, ns2
 
+        #print
         for i in range(1000):
             what = md5(seed+str(i)).hexdigest()
             op = what[0]
             subop = what[1]
             start = int(what[2:4], 16)
-            length = int(what[5:6], 16)
+            length = max(1,int(what[5:6], 16))
+            #print what
             if op in "0":
                 if subop in "01234":
                     s1 = S1(); s2 = S2()
@@ -1656,30 +1730,38 @@ class ByteSpans(unittest.TestCase):
                     s1 = S1(start, length); s2 = S2(start, length)
                 else:
                     s1 = S1(s1); s2 = S2(s2)
+                #print "s2 = %s" % s2.dump()
             elif op in "123":
+                #print "s2.add(%d,%d)" % (start, length)
                 s1.add(start, length); s2.add(start, length)
             elif op in "456":
+                #print "s2.remove(%d,%d)" % (start, length)
                 s1.remove(start, length); s2.remove(start, length)
             elif op in "78":
                 ns1, ns2 = _create(what[7:11])
+                #print "s2 + %s" % ns2.dump()
                 s1 = s1 + ns1; s2 = s2 + ns2
             elif op in "9a":
                 ns1, ns2 = _create(what[7:11])
+                #print "%s - %s" % (s2.dump(), ns2.dump())
                 s1 = s1 - ns1; s2 = s2 - ns2
             elif op in "bc":
                 ns1, ns2 = _create(what[7:11])
+                #print "s2 += %s" % ns2.dump()
                 s1 += ns1; s2 += ns2
             else:
                 ns1, ns2 = _create(what[7:11])
+                #print "%s -= %s" % (s2.dump(), ns2.dump())
                 s1 -= ns1; s2 -= ns2
-            self.failUnlessEqual(s1.each(), s2.each())
+            #print "s2 now %s" % s2.dump()
+            self.failUnlessEqual(list(s1.each()), list(s2.each()))
             self.failUnlessEqual(len(s1), len(s2))
             self.failUnlessEqual(bool(s1), bool(s2))
             self.failUnlessEqual(list(s1), list(s2))
             for j in range(10):
                 what = md5(what[12:14]+str(j)).hexdigest()
                 start = int(what[2:4], 16)
-                length = int(what[5:6], 16)
+                length = max(1, int(what[5:6], 16))
                 span = (start, length)
                 self.failUnlessEqual(bool(span in s1), bool(span in s2))
 
@@ -1696,3 +1778,38 @@ class ByteSpans(unittest.TestCase):
     # len(s) -> number of bytes, for testing, bool(), and accounting/limiting
     # bool(s)  (__len__)
     # s = s1+s2, s1-s2, +=s1, -=s1
+
+class ByteSpans_Overlap(unittest.TestCase):
+    def test_overlap(self):
+        for a in range(20):
+            for b in range(10):
+                for c in range(20):
+                    for d in range(10):
+                        self._test(a,b,c,d)
+
+    def _test(self, a, b, c, d):
+        s1 = set(range(a,a+b))
+        s2 = set(range(c,c+d))
+        #print "---"
+        #self._show(s1, "1")
+        #self._show(s2, "2")
+        o = Spans()._overlap(a,b,c,d)
+        expected = s1.intersection(s2)
+        if not expected:
+            self.failUnlessEqual(o, None)
+        else:
+            start,length = o
+            so = set(range(start,start+length))
+            #self._show(so, "o")
+            self.failUnlessEqual(so, expected)
+
+    def _show(self, s, c):
+        import sys
+        out = sys.stdout
+        if s:
+            for i in range(max(s)):
+                if i in s:
+                    out.write(c)
+                else:
+                    out.write(" ")
+        out.write("\n")
diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
index bb099bb..49c56d7 100755
--- a/src/allmydata/util/spans.py
+++ b/src/allmydata/util/spans.py
@@ -1,46 +1,175 @@
 
-class SimpleSpans:
+class Spans:
+    """I maintain a set of integers, optimized for operations on spans like
+    'add range to set' and 'is range in set'. Equivalently, I can be said to
+    maintain a large array of booleans. Typically the integers represent
+    offsets into a large string, such as a distant share. In such a context,
+    I can help track which portions of the share are have been requested or
+    received.
+
+    I behave similarly to perl's Set::IntSpan module. This module is commonly
+    used for representing .newsrc files (in which the integers represent
+    message numbers).
+
+    The new downloader will use it to keep track of which bytes we've requested
+    or received already.
+    """
+
     def __init__(self, _span_or_start=None, length=None):
-        self._have = set()
+        self._spans = list()
         if length is not None:
-            for i in range(_span_or_start, _span_or_start+length):
-                self._have.add(i)
+            self._spans.append( (_span_or_start, length) )
         elif _span_or_start:
             for (start,length) in _span_or_start:
                 self.add(start, length)
+        self._check()
+
+    def _check(self):
+        assert sorted(self._spans) == self._spans
+        prev_start = prev_end = None
+        try:
+            for (start,length) in self._spans:
+                if prev_end is not None:
+                    assert start > prev_end
+                prev_start = start
+                prev_end = start+length
+        except AssertionError:
+            print "BAD:", self.dump()
+            raise
+
+    def _overlap(self, start0, length0, start1, length1):
+        # return start2,length2 of the overlapping region, or None
+        #  00      00   000   0000  00  00 000  00   00  00      00
+        #     11    11   11    11   111 11 11  1111 111 11    11
+        left = max(start0, start1)
+        right = min(start0+length0, start1+length1)
+        # if there is overlap, 'left' will be its start, and right-1 will
+        # be the end'
+        if left < right:
+            return (left, right-left)
+        return None
+
+    def _adjacent(self, start0, length0, start1, length1):
+        if (start0 < start1) and start0+length0 == start1:
+            return True
+        elif (start1 < start0) and start1+length1 == start0:
+            return True
+        return False
 
     def add(self, start, length):
-        for i in range(start, start+length):
-            self._have.add(i)
+        assert start >= 0
+        assert length > 0
+        #print " ADD [%d+%d -%d) to %s" % (start, length, start+length, self.dump())
+        first_overlap = last_overlap = None
+        for i,(s_start,s_length) in enumerate(self._spans):
+            #print "  (%d+%d)-> overlap=%s adjacent=%s" % (s_start,s_length, self._overlap(s_start, s_length, start, length), self._adjacent(s_start, s_length, start, length))
+            if (self._overlap(s_start, s_length, start, length)
+                or self._adjacent(s_start, s_length, start, length)):
+                last_overlap = i
+                if first_overlap is None:
+                    first_overlap = i
+                continue
+            # no overlap
+            if first_overlap is not None:
+                break
+        #print "  first_overlap", first_overlap, last_overlap
+        if first_overlap is None:
+            # no overlap, so just insert the span and sort by starting
+            # position.
+            self._spans.insert(0, (start,length))
+            self._spans.sort()
+        else:
+            # everything from [first_overlap] to [last_overlap] overlapped
+            first_start,first_length = self._spans[first_overlap]
+            last_start,last_length = self._spans[last_overlap]
+            newspan_start = min(start, first_start)
+            newspan_end = max(start+length, last_start+last_length)
+            newspan_length = newspan_end - newspan_start
+            newspan = (newspan_start, newspan_length)
+            self._spans[first_overlap:last_overlap+1] = [newspan]
+        #print "  ADD done: %s" % self.dump()
+        self._check()
+
         return self
 
     def remove(self, start, length):
-        for i in range(start, start+length):
-            self._have.discard(i)
+        assert start >= 0
+        assert length > 0
+        #print " REMOVE [%d+%d -%d) from %s" % (start, length, start+length, self.dump())
+        end = start+length
+        first_complete_overlap = last_complete_overlap = None
+        for i,(s_start,s_length) in enumerate(self._spans):
+            s_end = s_start + s_length
+            o = self._overlap(s_start, s_length, start, length)
+            if o:
+                o_start, o_length = o
+                o_end = o_start+o_length
+                if o_start == s_start and o_end == s_end:
+                    # delete this span altogether
+                    if first_complete_overlap is None:
+                        first_complete_overlap = i
+                    last_complete_overlap = i
+                elif o_start == s_start:
+                    # we only overlap the left side, so trim the start
+                    #    1111
+                    #  rrrr
+                    #    oo
+                    # ->   11
+                    new_start = o_end
+                    new_end = s_end
+                    assert new_start > s_start
+                    new_length = new_end - new_start
+                    self._spans[i] = (new_start, new_length)
+                elif o_end == s_end:
+                    # we only overlap the right side
+                    #    1111
+                    #      rrrr
+                    #      oo
+                    # -> 11
+                    new_start = s_start
+                    new_end = o_start
+                    assert new_end < s_end
+                    new_length = new_end - new_start
+                    self._spans[i] = (new_start, new_length)
+                else:
+                    # we overlap the middle, so create a new span. No need to
+                    # examine any other spans.
+                    #    111111
+                    #      rr
+                    #    LL  RR
+                    left_start = s_start
+                    left_end = o_start
+                    left_length = left_end - left_start
+                    right_start = o_end
+                    right_end = s_end
+                    right_length = right_end - right_start
+                    self._spans[i] = (left_start, left_length)
+                    self._spans.append( (right_start, right_length) )
+                    self._spans.sort()
+                    break
+        if first_complete_overlap is not None:
+            del self._spans[first_complete_overlap:last_complete_overlap+1]
+        #print "  REMOVE done: %s" % self.dump()
+        self._check()
         return self
 
+    def dump(self):
+        return "len=%d: %s" % (len(self),
+                               ",".join(["[%d-%d]" % (start,start+l-1)
+                                         for (start,l) in self._spans]) )
+
     def each(self):
-        return sorted(self._have)
+        for start, length in self._spans:
+            for i in range(start, start+length):
+                yield i
 
     def __iter__(self):
-        items = sorted(self._have)
-        prevstart = None
-        prevend = None
-        for i in items:
-            if prevstart is None:
-                prevstart = prevend = i
-                continue
-            if i == prevend+1:
-                prevend = i
-                continue
-            yield (prevstart, prevend-prevstart+1)
-            prevstart = prevend = i
-        if prevstart is not None:
-            yield (prevstart, prevend-prevstart+1)
+        for s in self._spans:
+            yield s
 
     def __len__(self):
         # this also gets us bool(s)
-        return len(self._have)
+        return sum([length for start,length in self._spans])
 
     def __add__(self, other):
         s = self.__class__(self)
@@ -65,7 +194,39 @@ class SimpleSpans:
         return self
 
     def __contains__(self, (start,length)):
-        for i in range(start, start+length):
-            if i not in self._have:
-                return False
-        return True
+        for span_start,span_length in self._spans:
+            o = self._overlap(start, length, span_start, span_length)
+            if o:
+                o_start,o_length = o
+                if o_start == start and o_length == length:
+                    return True
+        return False
+
+class SimpleDataSpans:
+    """I represent portions of a large string. Equivalently, I can be said to
+    maintain a large array of characters (with gaps of empty elements). I can
+    be used to manage access to a remote share, where some pieces have been
+    retrieved, some have been requested, and others have not been read.
+    """
+
+    def __init__(self):
+        pass
+
+    def get(self, start, length):
+        # returns a string of LENGTH, or None
+        pass
+
+    def add(self, start, data):
+        pass
+
+    def remove(self, start, length):
+        pass
+
+    def pop(self, start, length):
+        data = self.get(start, length)
+        if data:
+            self.remove(start, length)
+        return data
+
+    def get_what(self, wanted):
+        pass

commit 023e134feac1c607af8ab215e3df2028888d7ed2
Author: Brian Warner <warner@lothar.com>
Date:   Wed Jan 27 12:04:49 2010 -0800

    add a byte-spans utility class, like perl's Set::IntSpan for .newsrc files.
    
    The new downloader will use it to keep track of which bytes we've requested
    or received already.
---
 src/allmydata/test/test_util.py |  159 +++++++++++++++++++++++++++++++++++++++
 src/allmydata/util/spans.py     |   71 +++++++++++++++++
 2 files changed, 230 insertions(+), 0 deletions(-)

diff --git a/src/allmydata/test/test_util.py b/src/allmydata/test/test_util.py
index 0a326b3..ee0f722 100644
--- a/src/allmydata/test/test_util.py
+++ b/src/allmydata/test/test_util.py
@@ -7,12 +7,14 @@ from twisted.trial import unittest
 from twisted.internet import defer, reactor
 from twisted.python.failure import Failure
 from twisted.python import log
+from hashlib import md5
 
 from allmydata.util import base32, idlib, humanreadable, mathutil, hashutil
 from allmydata.util import assertutil, fileutil, deferredutil, abbreviate
 from allmydata.util import limiter, time_format, pollmixin, cachedir
 from allmydata.util import statistics, dictutil, pipeline
 from allmydata.util import log as tahoe_log
+from allmydata.util.spans import SimpleSpans
 
 class Base32(unittest.TestCase):
     def test_b2a_matches_Pythons(self):
@@ -1537,3 +1539,160 @@ class Log(unittest.TestCase):
         tahoe_log.err(format="intentional sample error",
                       failure=f, level=tahoe_log.OPERATIONAL, umid="wO9UoQ")
         self.flushLoggedErrors(SampleError)
+
+
+class ByteSpans(unittest.TestCase):
+    def test_basic(self):
+        s = SimpleSpans()
+        self.failUnlessEqual(list(s), [])
+        self.failIf(s)
+        self.failIf((0,1) in s)
+        self.failUnless((0,0) in s)
+        self.failUnlessEqual(len(s), 0)
+
+        s1 = SimpleSpans(3, 4) # 3,4,5,6
+        self._check1(s1)
+
+        s2 = SimpleSpans(s1)
+        self._check1(s2)
+
+        s2.add(10,2) # 10,11
+        self._check1(s1)
+        self.failUnless((10,1) in s2)
+        self.failIf((10,1) in s1)
+        self.failUnlessEqual(s2.each(), [3,4,5,6,10,11])
+        self.failUnlessEqual(len(s2), 6)
+
+        s2.add(15,2).add(20,2)
+        self.failUnlessEqual(s2.each(), [3,4,5,6,10,11,15,16,20,21])
+        self.failUnlessEqual(len(s2), 10)
+
+        s2.remove(4,3).remove(15,1)
+        self.failUnlessEqual(s2.each(), [3,10,11,16,20,21])
+        self.failUnlessEqual(len(s2), 6)
+
+    def _check1(self, s):
+        self.failUnlessEqual(list(s), [(3,4)])
+        self.failUnless(s)
+        self.failUnlessEqual(len(s), 4)
+        self.failIf((0,1) in s)
+        self.failUnless((0,0) in s)
+        self.failUnless((3,4) in s)
+        self.failUnless((3,1) in s)
+        self.failUnless((5,2) in s)
+        self.failUnless((6,1) in s)
+        self.failIf((6,2) in s)
+        self.failIf((7,1) in s)
+        self.failUnlessEqual(s.each(), [3,4,5,6])
+
+    def test_math(self):
+        s1 = SimpleSpans(0, 10) # 0,1,2,3,4,5,6,7,8,9
+        s2 = SimpleSpans(5, 3) # 5,6,7
+        s3 = SimpleSpans(8, 4) # 8,9,10,11
+
+        s = s1 - s2
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,8,9])
+        s = s1 - s3
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7])
+        s = s2 - s3
+        self.failUnlessEqual(s.each(), [5,6,7])
+
+        s = s1 + s2
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9])
+        s = s1 + s3
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9,10,11])
+        s = s2 + s3
+        self.failUnlessEqual(s.each(), [5,6,7,8,9,10,11])
+
+        s = SimpleSpans(s1)
+        s -= s2
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,8,9])
+        s = SimpleSpans(s1)
+        s -= s3
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7])
+        s = SimpleSpans(s2)
+        s -= s3
+        self.failUnlessEqual(s.each(), [5,6,7])
+
+        s = SimpleSpans(s1)
+        s += s2
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9])
+        s = SimpleSpans(s1)
+        s += s3
+        self.failUnlessEqual(s.each(), [0,1,2,3,4,5,6,7,8,9,10,11])
+        s = SimpleSpans(s2)
+        s += s3
+        self.failUnlessEqual(s.each(), [5,6,7,8,9,10,11])
+
+    def test_random(self):
+        # attempt to increase coverage of corner cases by comparing behavior
+        # of a simple-but-slow model implementation against the
+        # complex-but-fast actual implementation, in a large number of random
+        # operations
+        S1 = SimpleSpans
+        S2 = SimpleSpans
+        s1 = S1(); s2 = S2()
+        seed = ""
+        def _create(subseed):
+            ns1 = S1(); ns2 = S2()
+            for i in range(10):
+                what = md5(subseed+str(i)).hexdigest()
+                op = what[0]
+                start = int(what[2:4], 16)
+                length = int(what[5:6], 16)
+                ns1.add(start, length); ns2.add(start, length)
+            return ns1, ns2
+
+        for i in range(1000):
+            what = md5(seed+str(i)).hexdigest()
+            op = what[0]
+            subop = what[1]
+            start = int(what[2:4], 16)
+            length = int(what[5:6], 16)
+            if op in "0":
+                if subop in "01234":
+                    s1 = S1(); s2 = S2()
+                elif subop in "5678":
+                    s1 = S1(start, length); s2 = S2(start, length)
+                else:
+                    s1 = S1(s1); s2 = S2(s2)
+            elif op in "123":
+                s1.add(start, length); s2.add(start, length)
+            elif op in "456":
+                s1.remove(start, length); s2.remove(start, length)
+            elif op in "78":
+                ns1, ns2 = _create(what[7:11])
+                s1 = s1 + ns1; s2 = s2 + ns2
+            elif op in "9a":
+                ns1, ns2 = _create(what[7:11])
+                s1 = s1 - ns1; s2 = s2 - ns2
+            elif op in "bc":
+                ns1, ns2 = _create(what[7:11])
+                s1 += ns1; s2 += ns2
+            else:
+                ns1, ns2 = _create(what[7:11])
+                s1 -= ns1; s2 -= ns2
+            self.failUnlessEqual(s1.each(), s2.each())
+            self.failUnlessEqual(len(s1), len(s2))
+            self.failUnlessEqual(bool(s1), bool(s2))
+            self.failUnlessEqual(list(s1), list(s2))
+            for j in range(10):
+                what = md5(what[12:14]+str(j)).hexdigest()
+                start = int(what[2:4], 16)
+                length = int(what[5:6], 16)
+                span = (start, length)
+                self.failUnlessEqual(bool(span in s1), bool(span in s2))
+
+
+    # s()
+    # s(start,length)
+    # s(s0)
+    # s.add(start,length) : returns s
+    # s.remove(start,length)
+    # s.each() -> list of byte offsets, mostly for testing
+    # list(s) -> list of (start,length) tuples, one per span
+    # (start,length) in s -> True if (start..start+length-1) are all members
+    #  NOT equivalent to x in list(s)
+    # len(s) -> number of bytes, for testing, bool(), and accounting/limiting
+    # bool(s)  (__len__)
+    # s = s1+s2, s1-s2, +=s1, -=s1
diff --git a/src/allmydata/util/spans.py b/src/allmydata/util/spans.py
new file mode 100755
index 0000000..bb099bb
--- /dev/null
+++ b/src/allmydata/util/spans.py
@@ -0,0 +1,71 @@
+
+class SimpleSpans:
+    def __init__(self, _span_or_start=None, length=None):
+        self._have = set()
+        if length is not None:
+            for i in range(_span_or_start, _span_or_start+length):
+                self._have.add(i)
+        elif _span_or_start:
+            for (start,length) in _span_or_start:
+                self.add(start, length)
+
+    def add(self, start, length):
+        for i in range(start, start+length):
+            self._have.add(i)
+        return self
+
+    def remove(self, start, length):
+        for i in range(start, start+length):
+            self._have.discard(i)
+        return self
+
+    def each(self):
+        return sorted(self._have)
+
+    def __iter__(self):
+        items = sorted(self._have)
+        prevstart = None
+        prevend = None
+        for i in items:
+            if prevstart is None:
+                prevstart = prevend = i
+                continue
+            if i == prevend+1:
+                prevend = i
+                continue
+            yield (prevstart, prevend-prevstart+1)
+            prevstart = prevend = i
+        if prevstart is not None:
+            yield (prevstart, prevend-prevstart+1)
+
+    def __len__(self):
+        # this also gets us bool(s)
+        return len(self._have)
+
+    def __add__(self, other):
+        s = self.__class__(self)
+        for (start, length) in other:
+            s.add(start, length)
+        return s
+
+    def __sub__(self, other):
+        s = self.__class__(self)
+        for (start, length) in other:
+            s.remove(start, length)
+        return s
+
+    def __iadd__(self, other):
+        for (start, length) in other:
+            self.add(start, length)
+        return self
+
+    def __isub__(self, other):
+        for (start, length) in other:
+            self.remove(start, length)
+        return self
+
+    def __contains__(self, (start,length)):
+        for i in range(start, start+length):
+            if i not in self._have:
+                return False
+        return True
